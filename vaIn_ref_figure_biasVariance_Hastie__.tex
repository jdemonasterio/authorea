In \ref{figure-biasVariance} Hastie et al. fit a regression model on data to illustrate on the interaction between bias and variance. The model increases complexity when using a greater number of features, more variables need to be fit during the optimization procedure. Thus the model increases in complexity (measured in degrees of freedom). 

At first the bias of the model is high both for the training and testing sets. The interpretation is that the model has high bias. Then the prediction error decreases as complexity increases. This behavior is expected because the model learns to better fit the data. At first, the expected prediction error, estimated by averaging over the test set' prediction error's, also decreases when the model's complexity is increased. However when the model starts to overfit the data, the expected prediction error starts to rise. A common heuristic to select the best model is to stop increasing the model's complexity once the $EPE$ stops decreasing.


prediction 
show different curves of prediction error for different models built and tested on samples of the training and testing set respectively in blue and red colors. As the model increases co

An opposite scneario scenario occurs whenb

rise Typical combinations
The four possible combinations of high 


\textbf{Overfitting}: 
 


\subsection{Brief comment on the convergence of the Training Error and the Vapnik–Chervonenkis Dimension}
\textbf{Vapnik–Chervonenkis Dimension}
\cite{vapnik-nature2013}
\cite{cherkassky-learning2007}

Vapnik and Chervonenkis (VC) have worked on establishing a theoretical framework for expected predictive error estimation. Using only finite samples from the underlying distribution they propose a nonparametric way to estimate this difference, given a class of learners. A modern introduction to this work is written in \cite{cherkassky-learning2007}, establishing a common ground for multiple machine learning methods. The principles and issues explained in the aforementioned work can be expanded to any probabilistic method that intends to predict an output from inputs. The book extends over various topics but here we will focus on a specific result and the ideas around it. In no way does this correspond to a complete introduction to VC or risk minimization theory.

To present the main result we must first introduce two notions: shattering and the VC dimension.

\begin{definition}{Shattering}

Let $\mathcal {A}= \{A_1,A_{2},\dots \}$ be a set family and $T$ a finite set. Let $t \subseteq T$, it is said that $\mathcal {A}$ picks out $t$ if there exists $A' \subseteq \mathcal {A} $ such that $ T \cap A' = t$. $T$ is said to be shattered by $\mathcal {A}$ if it picks out all of its subsets.

\end{definition}
 
The n-th shattering coefficient $\Delta_n$ of a class $\mathcal {A}$ is defined to be the maximum number of subsets of $n$ elements picked out by the class. 

In a supervised learning setting, we would have $\mathrm{T} = (\textbf{X},\textbf{Y})$ as the training set and $Y = \{0,1 \}$. Now let $\mathcal {F}$ be a class of classifiers where $f: X \rightarrow Y \, \forall f \ \ \in \mathcal {F}$. $t$ is said to be picked out by $\mathcal {F}$ if there exists a classifier $f \in \mathcal {F}$ such that $T = f^{-1}(\{1\})$. The classifiers in $\mathcal {F}$ define a unique mapping to the class of sets where each classifier is positive. It is said that $\mathcal {F}$ shatters a set $A$ if all of its subsets are picked out by the class of functions.

\begin{definition}{Vapnik–Chervonenkis (VC) Dimension}
 	
The Vapnik–Chervonenkis Dimension (VC) of a class of binary functions is the cardinality of the largest set which is shattered by $\mathcal {F}$.
\end{definition}\footnote{The VC dimension is briefly introduced here for the purpose of giving a theoretical approach to error estimation in machine learning methods. For a complete explanation on this topic refer to \cite{vapnik-nature2013}}

The VC dimension gives a certain criteria for measuring the complexity of a class of binary functions by measuring its expressiveness. This is because it gives the maximum number of samples for which all of their binary labeling possibilities can be selected by this class of functions.
Note that the VC dimension need not be finite. Refer to \cite{cherkassky-learning2007} Pg. 113 for examples of different VC classes, finite and infinite.

Having a class of functions that has a finite VC dimension conceptually means it is not that \textit{rich}. Meaning that this class won't be as flexible to adapt to different forms of underlying distributions. This tradeoff in simplicity vs. generalization is very important in risk estimation. It will provide us with enough necessary and sufficient conditions in the consistency and rate of convergence of the estimated risk. This is because we are giving convergence estimations for a \texit{class} of functions rather than for a single learner. Concrete bounds can be given for a wide array of methods, as long as the VC dimension is finite.

It can be proven that if a class of binary classifiers is of finite VC dimension, then the n-th shattering coefficient is bounded by a polynomial of order equal to the dimension 
i.e. $\Delta_n(\mathcal {F}) \leq O(n^{VC})$ \footnote{$O(\cdot)$ corresponds to Big-O notation.} where $VC$ stands for the VC dimension of the class.

These concepts show that for

It can be proven that for classes which have a finite VC dimension, then

