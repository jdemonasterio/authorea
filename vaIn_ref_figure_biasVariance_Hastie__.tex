In \ref{figure-biasVariance} Hastie et al. fit a regression model on data to illustrate on the interaction between bias and variance. The model increases complexity when using a greater number of features, more variables need to be fit during the optimization procedure. Thus the model increases in complexity (measured in degrees of freedom). 

At first the bias of the model is high both for the training and testing sets. The interpretation is that the model has high bias. Then the prediction error decreases as complexity increases. This behavior is expected because the model learns to better fit the data. At first, the expected prediction error, estimated by averaging over the test set' prediction error's, also decreases when the model's complexity is increased. However when the model starts to overfit the data, the expected prediction error starts to rise. A common heuristic to select the best model is to stop increasing the model's complexity once the $EPE$ stops decreasing.


prediction 
show different curves of prediction error for different models built and tested on samples of the training and testing set respectively in blue and red colors. As the model increases co

An opposite scneario scenario occurs when

rise Typical combinations
The four possible combinations of high 


\textbf{Overfitting}: 
 




\subsection{Brief comment on the convergence of the Training Error and the Vapnik–Chervonenkis Dimension}
\textbf{Vapnik–Chervonenkis Dimension}
\cite{vapnik-nature2013}
\cite{cherkassky-learning2007}

Related to estimating the true expected prediction error of the underlying distribution with finite samples.

VC dimension true error rate minimization. 
Shattering
\begin{definition}{Shattering}

Let $\mathcal {A}= \{A_1,A_{2},\dots \}$ be a set family and $T$  another set, then $\mathcal {A}$ shatters $T$ if for every subset $t \subseteq T$ there exists a set $A' \subseteq \mathcal {A} $ such that $ T \cap A' = t$. The VC dimension of $\mathcal {A}$ is the biggest cardinality of a set shattered by $\mathcal {A}$. Note that by definition this means \textit{any} possible set shattered by $\mathcal {A}$. Finally
 \end{definition}

In a supervised learning setting, let  $\mathrm{T} = (\textbf{X},\textbf{Y})$ be the training set and $Y = \{0,1 \}$. Let $\mathcal {F}$ be a class of classifiers where $f: X \rightarrow Y \, \forall f \in \mathcal {F}$. $t$ is said to be picked out by $\mathcal {F}$ if there exists a classifier $f \in \mathcal {F}$ such that $T = f^{-1}(\{1\})$. 
A set $T$ is said to be shattered by $\mathcal {F}$ if all of its subsets are picked out by the class of functions.

\begin{definition}{Vapnik–Chervonenkis (VC) Dimension}
 	
 	The Vapnik–Chervonenkis Dimension (VC) of a class of binary functions is the cardinality of the largest set which is shattered by $\mathcal {F}$.
\end{definition}\footnote{The VC dimension is briefly introduced here for support on the methods  \cite{cherkassky-learning2007} }
 
Given that all functions can be 
 
