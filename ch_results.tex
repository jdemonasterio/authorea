%!TEX root = main.tex

%===============================================================================
%     File: ch4_evaluation_results.tex
%    Author: Juan de Monasterio
%    Created: 15 Feb 2017
%  Description: Chapter: Evaluation and Results
%===============================================================================


% Problem1: We want to infer which users lived in the endemic region in the past.
% Problem 2: We want to infer which users lived in $E_Z$ in the past and then migrated.
% Problem 3: Users that migrated to a different region (any direction movement)
% Problem 4: Users that migrated from the endemic region, but conditional to users which are currently not endemic i.e. the base of users wich 



\chapter{Summary of Results}\label{ch:results}

\todo{Review whole results chapter, fix typos grammar mistakes, etc.}

% handling data, models and features with acumen
% After extensive research of machine learning literature and CDR usage in health problems.

The idea of this chapter is to present a combined overview of the work done and the results encountered in
\cref{ch:descr-risk,ch:machineLearning,ch:modelSelection,ch:ensembleMethods}.
All in all, we intend to showcase most of the relevant information from the preceding sections.
The overall picture of this work shows that there is evidence to confirm the hypothesis which stipulates that cellphone usage logs are rich in social interactions and in turn, these interactions are informative enough on long-term migrations and human mobility.

% A summary of the work done we can say we have discussed issues on the most of the following items:

In \cref{ch:descr-risk} we presented the CDR dataset and how it was relevant to the problem.
We showed how the data contained dynamic georeferrenced data at the user level.

With a minuscule sample of the CDRs we showed that the dataset is rich in social interactions between users.
This gave us an idea of how this type of data could be leveraged to the general problem.
We then framed our problem in the context of long-term human migrations and gave an idea of the importance of them, in relation to the epidemic nature of the disease.

We noted that the dataset posed a very unbalanced classification problem.
As expected, most telco users did not show any migration pattern to or from the endemic region, in the time of analysis.
This also implied that we had a very correlated problem, where most users did not change their past and present endemic conditions.
This information, combined with the local calling nature, meant that we had correlations with the target feature.
As users living in the endemic region will most probably have lived there in the past as well.

% in which there are few positive cases over all of the possible.
 % an exploratory We compared the relative sample of users

In this stage we set to explore data transformations and  visualizations to explore further the information contained in the data.
As a starting point we transformed the user-level data into a dataset at the antennas level.
This step aggregated information of call patterns to and from the endemic region.
It allowed us to present geolocalized visualizations of these social interactions to the vulnerable areas.
As an intermediate step in this process we had to introduce the definitions that we later used for the rest of this work.
We technically set what we meant was a vulnerable interaction and what we find is a user\'s home antenna.

% For the latter we base

The antenna level information was finally aggregated into heat maps.
In these, georeferenced call interactions and colored cellphone antennas according to their level of vulnerable interactions, as defined previously.
% We did this with the help of

The visualizations exposed a `temperature' descent from the core regions outwards.
As expected, the heat was noted to be concentrated in the ecoregion.
We also found out that the level of vulnerable interactions per antenna gradually descends as we move further away.
We say this is expected behavior because it is consistent with findings in the literature in which most calls are done to other local antennas. %in general of a local nature.
Given this period and TelCo of analysis, we saw that ninety percent of the users limited usage to at most four Telco antennas.

We also discovered some unexpected findings that were higlighted by the risk maps.
Interactions from non-endemic antennas towards those in the endemic region were seen to be non-homogeneous in some areas.
As an example, \cref{fig:amba_map} outlines various antennas with higher vulnerability.
We suspect this non-uniformity in the vulnerable interactions can help detect communities with higher probability of disease prevalence.

Health experts agree that these anomalies can be a great starting point to start.
Where potential communities atypical in their neighboring region could be worthy of a latent endemic foci.
These antennas stood out for their strong communication ties with the regions studied, showed significantly higher links of vulnerable communication.

In these images presented, the differences in the vulnerable interactions were clear.
When talking to the ``Mundo Sano Foundation'' researchers participating in this project, they pointed to the fact that the detection of these antennas through the visualizations was of great value to their goals.

 % to the health Mundo Sano Foundation researchers participating in this project.

At the national level, the results evidenced by the maps were coherent with the expert's knowledge of the endemic zone's migration patterns.



In \cref{ch:machineLearning,ch:modelSelection} we gave a practical introduction to Machine Learning in general, and of Supervised Classification problems in specific.
This helped us set our Chagas problem inside a systematic process to analyze long-term migrations with the CDR dataset.

To do this, we broke the problem of long-term human migrations down to four tasks which were later analyzed through different classifiers.
In these two chapters, we introduced the machine learning theory necessary to structure our problem, along with its methods to solve them.
These models were some of the most common techniques found in the literature for the task we were trying to solve.

To start off we considered the Logistic Regression Classifier along with the logloss metric to evaluate model performance.
Taking to our advantage the model's more tractable formulation, we showed how model regularization fits inside the machine learning frame.
This concept was introduced along with the notion of model hyperparameters.
Both were relevant to subsequent sections and in particular to \cref{fig:log_loss_regularization_validation_curve}.
There we showed that there is importance in model regularization by fitting a Logistic Regression classifier and comparing how its logloss varias across different regularization values.
This affected both the training and test set performance in a similar way.


At the same time we introduced other relevant concepts in the Machine Learning and we exemplified them from the our long-term prediction problem.
As is common in the literature, in \cref{figure:dtree_overfit_problem_2} we saw that increasing a decision tree's complexity leads to a clear case of overfitting the training data when trying to approximate the generalization error.

The same procedure was used to illustrate the concept of ``Cross Validation'' in \cref{fig:cv_vs_test_score}.
We found that when making predictions on wich users were endemic in the past endemic, the test score was inside a one standard deviation band of the CV score, across a varying hyperparameter value for the Logistic Regression classifier.
These findings are consistent with lots of other experiments found in the literature where by cross validating the search of optimal hyperparamters, we can have a fair estimation of the test error. 


Towards the end of \cref{ch:modelSelection} we performed an extensive experimentation with this same problem and also on users that moved out of the endemic region.,
 % which characterized unidirectional migrants, from the endemic region. 
For this we used the same classifier as before but on the whole dataset in what defined our sysmetatic approach of analyzing classifiers performance.
% through cross validation, hyperparameter error assesment, feature selections through iterations and visualization of validaction curves.

 % and found we did this on the whole dataset in an approach which defined systematic procedure.
% In here we finished testing the Logistic Classifier on the whole dataset, and with a systematic approach.

The runtime for this experiment had a lower bound of one and a half hours.
This was for all of the cross-validated fits and for the whole set.
Also since we limited the number of iterations to at most one hundred steps, this runtime is actually smaller than the the real optimization time for this algorithm, 

Another issue found with this algorithm was its extremely poor $F1$ performance for unidirectional migrants. 
We found that it had a test score lower than $0.01\%$.
As we will see later in other statistical leaners, we had that
the overall precision of the classifier was very poor due to not being able to accurately predict positive samples.
The algorithm was very inefficient in this aspect of overestimating users that migrated out of the endemic region, and this misclassification had a direct impact in the observed $F1$.


However this low score was not repeated in the rest of the metrics we evaluated.
For this same problem, both the best $Recall$ and $Accuracy$ test and CV rates were over $0.61\%$.
Another relevant observation was that best-fit $C$ values were not consistent along all fits as the values differed in orders of magnitude.
Solutions ranged from to $1E-3$ to $1E-1$ on different fits.
With this we can still positively affirm that the best-fit models for this experiment were those which penalized the loss function with strong regularization terms.


In contrast, when using this classifier to predict past endemic users residents, we found that we could get a $ROC AUC$ score of $0.76\%$.
Moreover this score is achieved when cross validating solely on the $l1$ regularization parameter.

A similar test score of $0.744\%$ was reached when optimizing for the $l2$ hyperparameter of regularization.
Each were optimized separately in order to exemplify how the overall $ROC AUC$ varied along different regularization thresholds.

This outcome of better $ROC AUC$ scores, when compared with the experiments on migrations out of the endemic region, underscore the difference among the problems' difficulty.
If we recall the highly unbalanced class relations for the problem of predicting unidirectional migrants in \cref{target2}, we confirm our hypothesis that these predictions characterize a problem which is very difficult.

Overall, both procedures had best fits on the more regularized $C$values.
A similar result can be found for regularization models and on procedures where hyperparameters were cross validated.
Both optimizations improved improved the models predictive power in varying degrees.


The previous results outline a complete systematical Machine Learning approach and toolset used to evaluate the long-term migrations prediction problem.
In this way we provide a methodical way to analyze classifiers perfroming on this problem.
With this we tackled the task with the introduction of new algorithms in the following chapters.



In\cref{ch:ensembleMethods} we introduce four new classifiers along with their properties and characteristics. Three of these, \cref{section:decision_trees,section:random_forests,section:gradient_boosting}, were tree based methods whilst the last, \cref{section:naive_bayes}, was a Naive Bayes Classifier used for benchmarking purposes.
For each, we presented an introductory review and pointed the reader to further literature references where applicable.

All but the Decision Tree learners were put through a number of experiments to evaluate how they performed across all prediction problems.
For each model, we tried to draw the greatest prediction performance from the features extracted from the dataset.

Random Forest and Gradient Boosting models were evaluated in further detail however.
And even though Decision Trees were not systematically analyzed, they were added as a departure point for the following ones.

As a start, we showed through a validation curve that, for the problem of users moving out of the endemic regions, the learners would become overfit if we let the trees grow in depth.
We had that the cross-validated scores and the test scores diverged rapidly after a tree-depth of $7$ and this is plotted in \cref{figure:dtree_overfit_problem_2}.
The wide gap in the scores resulted in a difference larger than $0.3\%$.

This result is not surprising in the sense that Decision Trees are known to become highly complex and overfitting models after growing to a certain depth. 
% Ideally it serves as an example of what aspects of this greedy algorithm are enhanced by Random Forests. 

We also fit two Decision Tree instances for the same and problem and evaluated them on a $Accuracy$ score.
The first fit was ran over a smaller hyperparameter configuration.
In particular, this configuration did not
idea was
 and plotted their output in a graph.
We showed for each node the attribute used in that splitting decision and the actual split value.
Doing this provided some insights into what features were initially being used by the algorithm to separate samples, according to their split index performance.
Still, given the low $Accuracy$ scores for both fits ($43.2\%$,and $57.9\%$)
Interestingly enough, we found that

All of these methods were successively compared on the same difficult task of prediting users that migrated out of the endemic region from $T_0$ to $T_1 $.
This common ground for all algorithms settled a common base ground comparison for the methods. 
% settled to compare and evaluate them on common ground.

As we will see later in \cref{tab:master_table_results}, \cref{target2} happened to be the hardest task of all, where all classifiers had their lowest scores.

% By comparing this problem over the various tree methods we discuss on applications that these kind of datasets can provide.
% to these kinds of tasks was elaborated.




\todo{Erase this section}
\section{Plan / Random notes to add to results chapter}

\begin{description}
    \item Confirmation of decision tree overfitting on model complexity. Random Forests improving on this. Gradient Boosting better in bias with a slight incrase in overfit.
    \item Suprising low F1 overall when best-fitting, yet high and acceptable ROC AUC.\ Models seem to overvalue recall with a huge loss in precision. This would be interesting to weight under the eyes of a health researcher. Where focus might be either on one over the other. \todo{ask Diego Weinberg}.
    \item Other scoring metrics ended in satisfactory performing models which could accurately with maximum test scores over 81\%.
    \item Naive Bayes fast run time on the whole dataset. Yet with a modest performance which was first ranked on the $F1$ score for difficult problems (2 and 3).
    \item [DONE] Feature importance. Mobility and phone usage were, surprisingly, relevant attributes in most of the experiments run.
    \item [DONE] The map attributes were confirmed to be relevant by the best-feature ranker for random forests/ gradient tree boosting algorithms.
    \item Using best features from random forests in gradient boosting models incorporates some overfitting in the model since we use target information to select the best features. However, independent experiments confirm the performance of gradient tree boosting methods.
    \item As expected, we had strong past to present correlation in endemic home antennas. We found other strong variables interacting with the condition of being a past endemic user too.
    \item What's the predictive performance of CDRs for the long-term migrations problem?
    \item Review 4 problems and wrap the best results of each algorithm, what scorings did they have?


\end{description}








In all, we can say that for the prediction tasks, the feature selection heuristics of ensemble algorithms did agree upon a group of features.
These were highlighted over the rest for a number of times and they provide insight into what CDR information provided most
predictive value.

From the results we collected, the best features can be broken down into the following categories.

\begin{itemize}

    \item Calls made in older months and in December: Interactions of different type, that occured closer to the split month between both periods $T_0$ and $T_1$ (July), were also distinguished.
    The model frequently used interactions from the months of August and September, as well as December.
    We conjecture that this last result can be due to the fact that December is a month where user\'s increase their mobile activity with their families.

    \item Vulnerable calling patterns: As we first suspected in \cref{ch:descr-risk}, the vulnerability of the mobile interaction between users was relevant.
    Measurements such as the duration and volume of calls at different time periods where selected.
    We make the reminder here that these measurments were the basis of our construction of the heatmaps in that same chapter.
    Where the antennas plotted concentrated users with vulnerable calls by calling volume or time, for a given month.

    \item Mobility size: Finally, different measures of the distance in human mobility were determined to be important by the model. Both general mobility diameters and weekday or weeknight specific mobility endemic were relevant and on various occasions.
    We can informally add that Decision Tree Learners also selected this features in the higher nodes.
    This result can be seen in detail at \cref{subsection:decision_trees_experiment}.

\end{itemize}

These gruops identify strong predictors in long-term human mobility and provide additional insights into our original hypothesis of which predictors were most valuable for the task.
Understanding these factors can further provide information into the problem of chagasic disease spread in the long run.
A more thorough examination of these results can be seen in \cref{tab:random_forest_big_experiment_best_features, tab:boosting_big_experiment_best_features}.






From our findings, we can say that there is no absolute single method that can be single-handedly applied to all problems and evaluation performances.
However, our results outline that the Gradient Boosting methods were, in general, top performing for all tasks and classifiation metrics, except for the $F1$ score.

Surprisingly enough, for some problems the Naive Bayes was outperforming in this $F1$ metric.
The algorithm was more conservative in its positive predictions and this difference saw a tradeoff in its performance across other metrics.

However we can say that results show that the performance of Gradient Boosting models is best over other models such as Logistic Regression or Naive Bayes algorithms for most tasks, and slightly better over the Random Forests.
These models can find complex interactions in the data whilst handling enough care not to lose generalization power.


% Each of the tasks will have different scores across the algorithms used.
% and the same applies to the metrics used to evaluate them.

\todo{Reword sentences
Expand, correct and finish them in a cohesive structure.}



% ||||||||||||||||||||||||||||||||||||| SPLIT |||||||||


Also, the effects of overfitting the models were visible in some of the experiments where deep trees and complex configurations underpowered the resulting model's generalization error.



High discrepancies exist in the scores of the different machine learning models.
Some are better at capturing the relevant information to predict human movements in the long-term.
This is because not all social information is presented directly in the dataset, and feature interactions are helpful.


This work may give us some insights into future disease spreads in other similar contexts.
Where
It also helps to showcase interesting ways of using the CDRs for reasons other than logging a user\'s calling expenses.



The implications of these results argue that CDRs can be extensively used for non business purposes and, specifically in this case, there is light evidence that they might capture well the fluxes of diseases among humans.
This data was used as a surrogate to other, more traditional, methods of disease control and surveillance.
This work intends to help authorities with disease-control strategies in way which is not intrusive for cellphone users.
Measures can then be applied outside of the endemic region, and directed towards specific neighbourhoods and communities by using the recommendations output by the risk algorithm.


\cref{tab:master_table_results} shows a compendium of the results obtained from the CDR dataset, using all classification models.
For ech problem and model, we found the best performing learner with a cross validation procedure which searched over an extensive grid of hyperparameters.
In all cases, the hyperparameter configuration with highest cross-validated $ROC AUC$ was selected as the \textit{best-fit}.
%Due to time constraints limited by the use of full cross validation fitting procedures, not all models were run for all of the tasks.
The following table shows the best-fit classifier's performance across three scores on the test set $\mathcal{T_s}$: $Accuracy$, $F1$ and $ROC AUC$.

\begin{table}
\caption{Master resutls table comparing results for all of the classifiers run in this work.
For each task and classifer, we show the its $Accuracy$, $ROC AUC$ and $F1$ test-set scores, along with the runtimes of a full cross validation procedures on the learner.}
\label{tab:master_table_results}
\centering
    \begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill} }  l l l l l }
    %{|p{2cm}|p{2cm}|p{1.5cm}|p{1cm}|p{1.5cm}}
    \toprule
    Measure & Problem 1 & Problem 2 & Problem 3 & Problem 4  \\
    \midrule
    Naive Bayes     & (0.84,0.82,0.75)  & (0.64,0.61,0.31)  &  (0.65,0.63,0.45)   & (0.85,0.76,0.62)   \\
    Logistic Classifier   & (0.893,0.857,0.9)  & (0.71,0.72,0.058)  &  (0.705,0.754,0.107)   & (0.883,0.85,0.181)   \\
    Random Forest   & (0.878,0.857,0.9)  & (0.714,0.726,0.058)  &  (0.705,0.754,0.107)   & (0.883,0.85,0.181)   \\
    Gradient Boosting   & (0.974,0.978,0.952)  & (0.838,0.819,0.101)  &  (0.811,0.855,0.169)   & (0.885,0.873,0.194)   \\

    \bottomrule
    \end{tabular*}
\end{table}


\todo{Bigger, better and more esthetically pleasing table is commented in the code below this one. Yet the commented table does not fit in a whole page and spawns compilation errors. This needs to be fixed.}





% \begin{landscape}% Landscape page
%         \begin{table*}
%             \centering
%             \ra{1.3}
%             \begin{tabular}{@{}rrrrcrrrrcrrcrr@{}} \toprule
%                 &  \multicolumn{4}{c}{Problem 1} &  \multicolumn{4}{c}{Problem 2} & \multicolumn{4}{c}{Problem 3}  & \multicolumn{4}{c}{Problem 4}\\
%                  \cmidrule{2-5} \cmidrule{6-9} \cmidrule{10-13} \cmidrule{14-17}
%                 & $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ \\
%                 \midrule
%                 $Naive Bayes$ & 0.84 & 0.82 & 0.75 & 2 && 0.64 & 0.61 & 0.31 & 2 && 0.65 & 0.63 & 0.45 & 1 && 0.85 & 0.76 & 0.62 & 1 \\
%                 $Logistic \ Regression$ & 0.893 & 0.857 & 0.9 & 96 && 0.714& 0.726 & 0.058  & 119 && 0.705& 0.754 & 0.107 & 115 && 0.883 & 0.85 & 0.181 & 106 \\
%                 $Random \ Forest$ & 0.878 & 0.903 & 0.77 & 33  && 0.79 & 0.776 & 0.088 & 45 && 0.792 & 0.845 & 0.156 & 21 && 0.898 & 0.853 & 0.203 &  19\\
%                 $Gradient \ Tree \ Boosting$  & 0.974 & 0.978 & 0.952 & 41 && 0.838 & 0.819 &  0.101 & 54  && 0.811 & 0.855 & 0.169 & 33 && 0.885 & 0.873  & 0.194 & 47 \\
%                 \bottomrule
%             \end{tabular}
%             \caption{Caption}
%         \end{table*}
% \end{landscape}








%   ___  _     ___         ___________ __ __ _____ _____
%  /   \| |   |   \       / ___/      |  |  |     |     |
% |     | |   |    \     (   \_|      |  |  |   __|   __|
% |  O  | |___|  D  |     \__  |_|  |_|  |  |  |_ |  |_
% |     |     |     |     /  \ | |  | |  :  |   _]|   _]
% |     |     |     |     \    | |  | |     |  |  |  |
%  \___/|_____|_____|      \___| |__|  \__,_|__|  |__|

% % With a running time of 80s, 10 steps are generally needed to achieve a 0.977 validation accuracy score on
% The best model was a Logistic Regression Classifier with an $L2$-penalty value of 0.01.
% % Table~\cref{ts}
% The following table
% shows the scores obtained by the selected model on the out-of-sample set.

% \begin{table}\label{tab:results}[ht]
%   \caption{Resulting scores.}

%   \centering
%   \begin{tabular}{ l l }
%       \toprule
%       Score & Value \\
%       \midrule
%       F1 score & 0.964537  \\
%       Accuracy & 0.980670  \\
%       AUC    & 0.991593  \\
%       Precision & 0.970838  \\
%       Recall  & 0.958316  \\
%       \bottomrule
%   \end{tabular}
% \end{table}


% High values across all scoring measures are achieved
% with the best estimator, with the lowest metric starting at a value of 0.958.
% These results can be explained by the fact that
% %Scores results become less surprising when looking closer at the dataset.
%
%Si del test\_set ahora me quedo con lo users que Y\_target ==1 == EPIDEMIC\_gt pero que hoy en dia son epidemic ==0 (se mudaron)s
 % scores bajan mucho:
%
%+--------------+-----------------+-------+
%| target\_label | predicted\_label | count |
%+--------------+-----------------+-------+
%|   1    |    1    | 1944 |
%|   1    |    0    | 3525 |
%+--------------+-----------------+-------+
%'f1\_score': 0.5244840145690004, ''accuracy': 0.3554580,, 'precision': 1.0, 'recall': 0.35545803,

% \section{Key Results}

