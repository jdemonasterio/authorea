%===============================================================================
%     File: ch_results.tex
%    Author: Juan de Monasterio
%    Created: 1 Oct 2017
%  Description: Chapter Results
%===============================================================================


% Problem1: We want to infer which users lived in the endemic region in the past.
% Problem 2: We want to infer which users lived in $E_Z$ in the past and then migrated.
% Problem 3: Users that migrated to a different region (any direction movement)
% Problem 4: Users that migrated from the endemic region, but conditional to users which are currently not endemic i.e. the base of users wich

The idea of this chapter is to present a combined overview of the work done and the results encountered in
\cref{ch:descr-risk,ch:machineLearning,ch:modelSelection,ch:ensembleMethods}.
All in all, we intend to showcase most of the relevant information from the preceding sections.
The overall picture of this work shows that there is evidence to confirm the hypothesis which stipulates that cell phone usage logs are rich in social interactions and in turn, these interactions are informative enough on long-term migrations and human mobility.

\chapter{Summary of Results}\label{ch:results}

% handling data, models and features with acumen
% After extensive research of machine learning literature and CDR usage in health problems.

% A summary of the work done we can say we have discussed issues on the most of the following items:

In \cref{ch:descr-risk} we presented the CDR dataset and how it was relevant to the problem.
We showed how the data contained dynamic georeferenced data at the user level.

With a minuscule sample of the CDRs we showed that the dataset is rich in social interactions between users.
This gave us an idea of how this type of data could be leveraged to the general problem.
We then framed our problem in the context of long-term human migrations and gave an idea of the importance of them, in relation to the epidemic nature of the disease.

We noted that the dataset posed a very unbalanced classification problem as most Telco users did not show any migration pattern to or from the endemic region, in the time of analysis.
This also implied that we had a very correlated problem, where most users did not change their past and present endemic conditions.
This information, combined with the local calling nature, meant that we had correlations with the target feature.
This was noted by seeing that users living in the endemic region will most probably have lived there in the past as well.

In later sections we also noted that this strong past-to-present correlation in user's homes was relevant to the prediction problems.
This is because for tree based learners, knowing where the user lived in the present, was a strong indicator of where the user would be living in the past.
This information provided the algorithms with features that were highly informative.
Yet we decided to exclude this attribute for the problem of predicting a user's past home endemic condition because we wanted to evaluate the information present in other, less trivial, features.

More so, we found other variables having mild interactions with the condition of being a past endemic user.
As expected, people with high mobile interactions with the endemic region, in the form of calls duration and calls counts, also resulted in attributes which were related to users having past endemic homes.
This is most probably due to the locality effect in calling patterns, where most interactions between Telco users occur in local regions.
Thus users living in endemic regions would call other users nearby.


% in which there are few positive cases over all of the possible.
 % an exploratory We compared the relative sample of users

In this stage we also set to explore the data with feature transformations and visualizations to further discover available in it. % information contained in the data.
As a starting point we transformed the user-level data into a dataset at the antennas level.
This step aggregated information of call patterns to and from the endemic region.
It allowed us to present geolocalized visualizations of these social interactions to the vulnerable areas.
As an intermediate step in this process we had to introduce the definitions that we later used for the rest of this work.
We technically set what we meant was a vulnerable interaction and what we find is a user's home antenna.

% For the latter we base

The antenna level information was finally aggregated into heat maps.
In these, georeferenced call interactions and colored cellphone antennas according to their level of vulnerable interactions, as defined previously.
% We did this with the help of

The visualizations exposed a `temperature' descent from the core regions outwards.
As expected, the heat was noted to be concentrated in the ecoregion.
We also found out that the level of vulnerable interactions per antenna gradually descends as we move further away.
We say this is expected behavior because it is consistent with findings in the literature in which most calls are done to other local antennas. %in general of a local nature.
Given this period and Telco of analysis, we saw that ninety percent of the users limited usage to at most four Telco antennas.

We also discovered some unexpected findings that were highlighted by the risk maps.
Interactions from non-endemic antennas towards those in the endemic region were seen to be non-homogeneous in some areas.
As an example, \cref{fig:amba_map} outlines various antennas with higher vulnerability.
We suspect this non-uniformity in the vulnerable interactions can help detect communities with higher probability of disease prevalence.

Health experts agree that these anomalies can be a great starting point to start.
Where potential communities atypical in their neighboring region could be worthy of a latent endemic foci.
These antennas stood out for their strong communication ties with the regions studied, showed significantly higher links of vulnerable communication.

In these images presented, the differences in the vulnerable interactions were clear.
When talking to the ``Mundo Sano Foundation'' researchers participating in this project, they pointed to the fact that the detection of these antennas through the visualizations was of great value to their goals.

 % to the health Mundo Sano Foundation researchers participating in this project.

At the national level, the results evidenced by the maps were coherent with the expert's knowledge of the endemic zone's migration patterns.



In \cref{ch:machineLearning,ch:modelSelection} we gave a practical introduction to Machine Learning in general, and of Supervised Classification problems in specific.
This helped us set our Chagas problem inside a systematic process to analyze long-term migrations with the CDR dataset.

To do this, we broke the problem of long-term human migrations down to four tasks which were later analyzed through different classifiers.
In these two chapters, we introduced the machine learning theory necessary to structure our problem, along with its methods to solve them.
These models were some of the most common techniques found in the literature for the task we were trying to solve.

To start off we considered the Logistic Regression Classifier along with the logloss metric to evaluate model performance.
Taking to our advantage the model's more tractable formulation, we showed how model regularization fits inside the machine learning frame.
This concept was introduced along with the notion of model hyperparameters.
Both were relevant to subsequent sections and in particular to \cref{fig:log_loss_regularization_validation_curve}.
There we showed that there is importance in model regularization by fitting a Logistic Regression classifier and comparing how its logloss varies across different regularization values.
This affected both the training and test set performance in a similar way.


At the same time we introduced other relevant concepts in the Machine Learning and we exemplified them from the our long-term prediction problem.
As is common in the literature, in \cref{figure:dtree_overfit_problem_2} we saw that increasing a decision tree's complexity leads to a clear case of overfitting the training data when trying to approximate the generalization error.

The same procedure was used to illustrate the concept of ``Cross Validation'' in \cref{fig:cv_vs_test_score}.
We found that when making predictions on which users were endemic in the past, the test score was inside a one standard deviation band from the CV score, across a varying hyperparameter value for the Logistic Regression classifier.
These findings are consistent with lots of other experiments found in the literature where by cross validating the search of optimal hyperparameters, we can have a fair estimation of the test error.


Towards the end of \cref{ch:modelSelection} we performed a deeper experimentation with this same problem and also on users that moved out of the endemic region.,
 % which characterized unidirectional migrants, from the endemic region.
Here we used the same classifier as before but on the whole dataset in what defined our systematic approach of analyzing classifiers performance.
% through cross validation, hyperparameter error assessment, feature selections through iterations and visualization of validation curves.

 % and found we did this on the whole dataset in an approach which defined systematic procedure.
% In here we finished testing the Logistic Classifier on the whole dataset, and with a systematic approach.

The runtime for this experiment had a lower bound of one and a half hours.
This was for all of the cross-validated fits and for the whole set.
Also since we limited the number of iterations to at most one hundred steps, this runtime is actually smaller than the the real optimization time for this algorithm,

Another issue found with this algorithm was its extremely poor $F1$ performance for unidirectional migrants.
We found that it had a test score lower than $1\%$.
As we will see later in other statistical learners, we had that
the overall precision of the classifier was very poor due to not being able to accurately predict positive samples.
The algorithm was very inefficient in this aspect of overestimating users that migrated out of the endemic region, and this misclassification had a direct impact in the observed $F1$.


However this low score was not repeated in the rest of the metrics we evaluated.
For this same problem, both the best $Recall$ and $Accuracy$ test and CV rates were over $61\%$.

Another relevant observation was that best-fit $C$ values were not consistent along all fits, because these resulting values were seen to differ by orders of magnitude.
Solutions for $C$ ranged from to $1E-3$ to $1E-1$ on different fits.
With this we can still positively affirm that the best-fit models for this experiment were those which penalized the loss function with strong regularization terms.


In contrast, when using this classifier to predict past endemic users residents, we found that we could get a $ROC AUC$ score of $76\%$.
Moreover this score is achieved when cross validating solely on the $l1$ regularization parameter.

A similar test score of $74.4\%$ was reached when optimizing for the $l2$ hyperparameter of regularization.
Each were optimized separately in order to exemplify how the overall $ROC AUC$ varied along different regularization thresholds.

This outcome of better $ROC AUC$ scores, when compared with the experiments on migrations out of the endemic region, underscore the difference among the problems' difficulty.
If we recall the highly unbalanced class relations for the problem of predicting unidirectional migrants in \cref{target2}, we confirm our hypothesis that these predictions characterize a problem which is very difficult.

Overall, both procedures had best fits on the more regularized $C$ values.
A similar result can be found for regularization models and on procedures where hyperparameters were cross validated.
Both optimizations improved improved the model's predictive power in varying degrees.


The previous results outline a complete systematical Machine Learning approach and tool set used to evaluate the long-term migrations prediction problem.
In this way we provide a methodical way to analyze classifiers performing on this problem.
With this we tackled the task with the introduction of new algorithms in the following chapters.



In \cref{ch:ensembleMethods} we introduce four new classifiers along with their properties and characteristics. Three of these, \cref{section:decision_trees,section:random_forests,section:gradient_boosting}, were tree based methods whilst the last, \cref{section:naive_bayes}, was a Naive Bayes Classifier used for benchmarking purposes.
For each, we presented an introductory review and pointed the reader to further literature references where applicable.

All but the Decision Tree learners were put through a number of experiments to evaluate how they performed across all prediction problems.
For each model, we tried to draw the greatest prediction performance from the features extracted from the dataset.

Random Forest and Gradient Boosting models were evaluated in further detail however.
And even though Decision Trees were not systematically analyzed, they were added as a departure point for the following ones.

As a start, we showed through a validation curve that, for the problem of users moving out of the endemic regions, the learners would become overfit if we let the trees grow in depth.
We had that the cross-validated scores and the test scores diverged rapidly after a tree-depth of $7$ and this is plotted in \cref{figure:dtree_overfit_problem_2}.
The wide gap in the scores resulted in a difference larger than $30\%$.

In conclusion, the overfitting effect in this learner was visible in the experiments where deep trees and complex configurations underpowered the resulting model's generalization error.
This is not surprising as Decision Trees are known to become highly complex after a certain level of tree growth.

% Ideally it serves as an example of what aspects of this greedy algorithm are enhanced by Random Forests.

We also fit two Decision Tree instances for the same problem to evaluate the model with an $Accuracy$ score.
The first of two fits was ran over a smaller hyperparameter configuration and, in particular, this configuration did not consider a rebalancing of the samples when computing the value of the loss function over the whole set.
The second fit was done with this balancing to account on how this difference affects the algorithm's outcome.

To aid our understanding of this model's outcome we also decided to create visualizations for both fitted trees.
Each tree is plotted in a graph which shows up to five levels of splits.
With this we showed, for each node, the attribute used in that splitting decision and the actual split value.

These plots provided some insights into what features were initially being used by the algorithm to separate samples, according to their split index performance.
For instance, in both fits the root split was based on the user's mobility diameter, with split values near to $47.5$ kilometres in both cases.
This means that a mobility diameter of $47.5$ was the best split decision the algorithm could take in order to improve the Gini index of the sub groups.
Recall here that the mobility diameter quantifies the user's area of influence given by his mobile connection.

The other significant features, selected at splits near to the tree's root level, was the user's home antenna call count.
Both the volume of calls during the whole week and usage specifically during weeknights time was important to the node-splitting algorithm.

For both trees, the split values for these two variables were relatively similar, where in the volume of weeknight calls using the home antenna was of $10.5$, whilst for total volume of home antenna calls, at any given time, the values were $139$ and $186$ calls for each case.
These split values were characterizing segments of heavy users, that is users that are higher than the median values for the home antenna usage variables.% on the upper usage percentiles for home antennas.

% This result makes does not strike as wild as
% Still this result does not imply that all split all users
 % It seems that a big area when using his mobile phone and as such, this attribute might indicative of past migrations.


Here we must note however, that both best-fit trees result in a low $Accuracy$ scores of $43.2\%$,and $57.9\%$ respectively.
Under these circumstances we find that these top features selected when building the trees could not be very informative of the overall problem in users that migrated out of the endemic region.
This means that, due to the low performance of these learners, the interpretations we build from this output can be based from the wrong assumptions.


% we could be tempted to interpret the

Interestingly enough, in these experiments we found that for this learner, rebalancing the set helped us increase the learner's score by a great amount.
We hypothesize that this is a valuable hyperparameter that helps when facing problems with target classes are unbalanced.

% With this score, noting which features were selected near the root leaves of the tree would not be very informative to the overall problem.
% Experiments with the Decision Tree was used


The rest of the \cref{ch:ensembleMethods} follows with the presentation of other learners.
All of them are introduced with a brief explanation of their formulation and their distinctive characteristics.

For each of these, we successively experiment the same difficult predictive task.
Where we look to tag users that migrated out of the endemic region, from time periods $T_0$ to $T_1$.
In this way   set a common ground to compare and evaluate the algorithms.
% settled to compare and evaluate them on common ground.

As we will see later in \cref{tab:master_table_results}, this task, \cref{target2}, happened to be the hardest one, as it resulted in the lowest metric scores for all classifiers.

When tested over with the Random Forest classifier, we saw better $Accuracy$ scores than with the Decision Learner over a 10-fold cross-validation procedure.
This model improved the mean cross validated score to reach a high score of $81\%$, for trees of depth bigger than $15$.
Note that in this experiment we reweighted samples in the loss function by default, following the improved results we obtained by doing this for the Decision Learner classifier.

We also saw that there was barely no change in the average CV score when we increased the number of trees used in the ensemble.
Having an ensemble of $20$ was enough to capture most of the predictive information we needed to correctly tag the unidirectional migrants of the endemic region.
From this it seems that adding more trees would not improve the generalization error in great values.
Still, the optimal number of trees was found to be around $130$ for the CV procedure, and this gives scores which are only $3\%$ better than the ones for an ensemble of $20$ trees.

In comparison to the previous model, the Forest also overfit the data when we deepened the tree depth.
However it did so at higher depths, where the first deviation across the mean error rates for both the CV and training sets was at a depth of $9$, compared to the previous level of $7$.
% It is also interesting to see that the de

At the same time we had that for this model, the biggest differences between the average training and CV errors, across hyperparameter values, was at most of $10\%$.
This result is expected in this learner which is designed to lower the base models' variance.
The downside in this learner was that the optimizations run for longer, taking at least 20 minutes for both cases.

These results for the Random Forest model were also compared to the Gradient Boosting model.
The same two hyperparameters were validated across a range of values, and for each we looked at how the Training and CV scores varied.
Similarly, we found that this model's score won't be greatly affected by the number of trees used in the ensemble, and that most of its predictive performance can be captured by using only $20$ trees.
Still, we note that the best-fit CV value was of $95$.

For this learner, we also found that it will rapidly overfit the data once the base learners get a maximum depth of $6$.
What was interesting here was that the mean CV $Accuracy$ constantly hovered around $86\%$, whilst the score for the training gradually improved as the tree-depth grew, up to a score of $96\%$.
This algorithm rapidly over fits users that migrated out of the endemic region, which is similar to what we saw with the Decision Tree Learner.
Still, and compared to the Forest learner, we get a better CV error, at the expense of a runtime which at least $50\%$ slower.

% By comparing this problem over the various tree methods we discuss on applications that these kind of datasets can provide.
% to these kinds of tasks was elaborated.

In a second stage of experiments, we performed extensive testing both on the Random Forest and Gradient Tree Boosting learners for the problem of tagging the same users that migrated out of the endemic region, but only considering as user-base, those users which are currently living outside of the endemic region.

For both, we ran a full CV procedure in two feature versions, one with all the features and one in which target-correlated features were removed.

As a general statement we can say that the Random Forests experiments have lower runtimes because take better advantage over the algorithm's parallelizability.
Also, we can not affirm that one algorithms is strictly better over the other.
Both of them exceed each other across different metrics.
It seems overall that the Random Forest is better at correctly classifying positive instances, while the Booster is slightly better over the $Recall$.
These asymmetrical performances across $Precision$ and $Recall$ scores resulted in that no algorithm outperformed the other when evaluated using the $F1$ score.
Due to the low $Precision$ rates, both achieved low $F1$ scores with similar outcomes, where the maximum of these values was $37\%$.

On other scores such as the $Accuracy$ and $ROC AUC$ the experiments showed high classifier rates, where all of these ranged between percentages of $83\%$ and $90\%$.
These high scores show there is enough predictive capacity in using these algorithms for this problem of tagging migrants from the endemic region.

On the best-fit hyperparameter values, we see that both algorithm's values agree in that a large number of base learners are needed to perform better.
This is evidenced by the fact that, for all experiments, the optimal value was higher than $100$ trees, up to a maximum of $300$ base learners.
A similar result is seen with the tree's depth, where all best-fit values were, at least, over a depth of $12$.


%%%%% LAST PART

Both the Random Forest and Gradient Boosting models played an important role in all of the prediction tasks we made.
The high scores that result in some of the metrics indicate that there is predictive information for the problems of long-term human migrations.
Another advantage of these two algorithms, is that they provide heuristics on the best-features that we can use from the dataset.

From the process of calculating those best-features, we can say that for this prediction task of finding, of those currently non-endemic users, which are the migrants that moved out of the endemic region, the feature selection heuristics of ensemble algorithms agreed upon a group of features.
These were highlighted, for a number of times, over the rest of the features and they provide insights into what CDR variables provided most predictive value.

Below we can find a descriptive collection of those features for these learners, where we have broken down all of this information into feature categories:

\begin{itemize}

    \item Calls made in older months and in December: Interactions of different type, that occurred closer to the split month between both periods $T_0$ and $T_1$ (July), were also distinguished.
    The model frequently used interactions from the months of August and September, as well as December.
    We conjecture that this last result can be due to the fact that December is a month where user's increase their mobile activity with their families.

    \item Vulnerable calling patterns: As we first suspected in \cref{ch:descr-risk}, the vulnerability of the mobile interaction between users was relevant.
    Measurements such as the duration and volume of calls at different time periods where selected.
    We make the reminder here that these measurements were the basis of our construction of the heatmaps in that same chapter.
    Where the antennas plotted concentrated users with vulnerable calls by calling volume or time, for a given month.

    \item Mobility size: Finally, different measures of the distance in human mobility were determined to be important by the model. Both general mobility diameters and weekday or weeknight specific mobility endemic were relevant and on various occasions.
    We can informally add that Decision Tree Learners also selected this features in the higher nodes.
    This result can be seen in detail at \cref{subsection:decision_trees_experiment}.

\end{itemize}

These factors identify strong predictors in long-term human mobility and provide additional insights into our original hypothesis of which predictors were most valuable for the task.
Understanding these factors can further provide information into the problem of chagasic disease spread in the long run.

As a final remark that we must add here, the previous best-features list carry some bias introduced by the model's feature-target correlations, because part of these best-features were selected from the experiments run with these same features.
A similar argument can be made for the best-features calculated from the Gradient Boosting experiment that used only a filtered set of features, previously filtered by the Random Forest experiments.
There is a strong feature-target correlation and there is a leakage of target information within this experiment.

To look at a more thorough examination of these results and how they were selected, the reader can refer to \cref{tab:random_forest_big_experiment_best_features,tab:boosting_big_experiment_best_features}.

% \section{Missing parts Plan to add to this section }
% \begin{description}

%     \item [DONE] Feature importance. Mobility and phone usage were, surprisingly, relevant attributes in most of the experiments run.
%     \item [DONE] The map attributes were confirmed to be relevant by the best-feature ranker for random forests/ gradient tree boosting algorithms.
%     \item [DONE] What's the predictive performance of CDRs for the long-term migrations problem?
% \end{description}



Given all of the previous findings we collected, we can say that there is no absolute single method that can be single-handedly applied to all problems and evaluation performances.
However, our results outline that the Gradient Boosting methods were, in general, top performing for all tasks and classification metrics, except for the $F1$ score.

Surprisingly enough, for some problems the Naive Bayes was outperforming in this $F1$ metric.
The algorithm was more conservative in its positive predictions and this difference saw a trade off in its performance across other metrics.

However we can say that results show that the performance of Gradient Boosting models is best over other models such as Logistic Regression or Naive Bayes algorithms for most tasks, and slightly better over the Random Forests.
These models can find complex interactions in the data whilst handling enough care not to lose generalization power.


% Each of the tasks will have different scores across the algorithms used.
% and the same applies to the metrics used to evaluate them.

% ||||||||||||||||||||||||||||||||||||| SPLIT |||||||||

\section{Master Table of Results}\label{sec:master_table}

\cref{tab:master_table_results} shows a compendium of the results obtained from the CDR dataset, using all classification models.
For each problem and model, we found the best performing learner with a cross validation procedure which searched over an extensive grid of hyperparameters.
In all cases, the hyperparameter configuration with highest cross-validated $ROC AUC$ was selected as the \textit{best-fit}.
%Due to time constraints limited by the use of full cross validation fitting procedures, not all models were run for all of the tasks.
The table shows the best-fit classifier's performance across three scores on the test set $\mathcal{T_s}$: $Accuracy$, $F1$ and $ROC AUC$.


%% OLD UGLY TABLE
% \begin{table}
% \caption{Master results table comparing results for all of the classifiers run in this work.
% For each task and classifier, we show the its $Accuracy$, $ROC AUC$ and $F1$ test-set scores, along with the runtimes of a full cross validation procedures on the learner.}
% \label{tab:master_table_results}
% \centering
%     \begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill} }  l l l l l }
%     %{|p{2cm}|p{2cm}|p{1.5cm}|p{1cm}|p{1.5cm}}
%     \toprule
%     Measure & Problem 1 & Problem 2 & Problem 3 & Problem 4  \\
%     \midrule
%     Naive Bayes     & (0.84,0.82,0.75)  & (0.64,0.61,0.41)  &  (0.65,0.63,0.45)   & (0.75,0.76,0.62)   \\
%     Logistic Classifier   & (0.893,0.857,0.9)  & (0.71,0.72,0.248)  &  (0.705,0.754,0.307)   & (0.883,0.85,0.331)   \\
%     Random Forest   & (0.878,0.857,0.9)  & (0.79,0.776,0.278)  &  (0.792,0.845,0.346)   & (0.898,0.853,0.393)   \\
%     Gradient Boosting   & (0.974,0.978,0.952)  & (0.838,0.819,0.291)  &  (0.811,0.855,0.359)   & (0.885,0.873,0.384)   \\
% %$Naive Bayes$               & 0.84  & 0.82  & 0.75  & 2  && 0.64  & 0.61  & 0.31  & 2  && 0.65   & 0.63  & 0.45  & 1   && 0.85  & 0.76  & 0.62 & 1 \\
% %$Logistic \ Regression$     & 0.893 & 0.857 & 0.9   & 96 && 0.714 & 0.726 & 0.248 & 119 && 0.705 & 0.754 & 0.307 & 115 && 0.883 & 0.85  & 0.331 & 106 \\
% %$Random \ Forest$            & 0.878 & 0.857 & 0.9  & 33 && 0.79  & 0.776 & 0.278 & 45  && 0.792 & 0.845 & 0.346 & 21  && 0.898 & 0.853 & 0.393 & 19\\
% %$Gradient \ Tree \ Boosting$ & 0.974 & 0.978 & 0.952 & 41 && 0.838 & 0.819 & 0.291 & 54  && 0.811 & 0.855 & 0.359 & 33  && 0.885 & 0.873 & 0.384 & 47 \\

%     \bottomrule
%     \end{tabular*}
% \end{table}


%\begin{landscape}% Landscape page
			% RAW NON TABLE SYNTHAXED VALUES
%			$Naive Bayes$               & 0.84  & 0.82  & 0.75  & 2  && 0.64  & 0.61  & 0.31  & 2  && 0.65   & 0.63  & 0.45  & 1   && 0.85  & 0.76  & 0.62 & 1 \\
			% $Logistic \ Regression$     & 0.893 & 0.857 & 0.9   & 96 && 0.714 & 0.726 & 0.248 & 119 && 0.705 & 0.754 & 0.307 & 115 && 0.883 & 0.85  & 0.331 & 106 \\
			% $Random \ Forest$            & 0.878 & 0.857 & 0.9  & 33 && 0.79  & 0.776 & 0.278 & 45  && 0.792 & 0.845 & 0.346 & 21  && 0.898 & 0.853 & 0.393 & 19\\
			% $Gradient \ Tree \ Boosting$ & 0.974 & 0.978 & 0.952 & 41 && 0.838 & 0.819 & 0.291 & 54  && 0.811 & 0.855 & 0.359 & 33  && 0.885 & 0.873 & 0.384 & 47 \\


	\begin{table*}[htp]\centering
		%\hskip-4.0cm
		\caption{Master results table comparing results for all of the classifiers run in this work.
			For each task and classifier, we show the its $Accuracy$, $ROC AUC$ and $F1$ test-set scores, along with the runtimes of a full cross validation procedures on the learner.}\label{tab:master_table_results}
		\ra{1.3}
		%\hskip-4.0cm
		\begin{tabular}{@{}rrrrrc@{}} \toprule
			&  \multicolumn{4}{c}{Problem 1} \\
			\cmidrule{2-5}
			& $Accuracy$ & $ROC AUC$ & $F1$ & $Runtime  (m)$ \\ \midrule
			$Naive Bayes$               & 0.84  & 0.82  & 0.75  & 2  \\
			$Logistic \ Regression$     & 0.893 & 0.857 & 0.9   & 96\\
			$Random \ Forest$            & 0.878 & 0.857 & 0.9  & 33 \\
			$Gradient \ Tree \ Boosting$ & 0.974 & 0.978 & 0.952 & 41  \\

			\bottomrule
		\end{tabular}

		%\addtocounter{table}{-1} % This dirty hack will fake both tables as if they were the same one.

		\medskip
		%\hskip-4.0cm
		\ra{1.3}
		%\hskip-4.0cm
		\begin{tabular}{@{}rrrrrc@{}} \toprule
			&  \multicolumn{4}{c}{Problem 2} \\
			\cmidrule{2-5}
			& $Accuracy$ & $ROC AUC$ & $F1$ & $Runtime  (m)$ \\ \midrule
			$Naive Bayes$               & 0.64  & 0.61  & 0.31  & 2   \\
			$Logistic \ Regression$     & 0.714 & 0.726 & 0.248 & 119 \\
			$Random \ Forest$            & 0.79  & 0.776 & 0.278 & 45  \\
			$Gradient \ Tree \ Boosting$ & 0.838 & 0.819 & 0.291 & 54 \\

			\bottomrule
		\end{tabular}

		\medskip

		%\hskip-4.0cm
		\ra{1.3}
		%\hskip-4.0cm
		\begin{tabular}{@{}rrrrrc@{}} \toprule
			&  \multicolumn{4}{c}{Problem 3} \\
			\cmidrule{2-5}
			& $Accuracy$ & $ROC AUC$ & $F1$ & $Runtime  (m)$ \\ \midrule
			$Naive Bayes$               & 0.65  & 0.63  & 0.45  & 1  \\
			$Logistic \ Regression$     & 0.705 & 0.754 & 0.307 & 115 \\
			$Random \ Forest$            &  0.792 & 0.845 & 0.346 & 21 \\
			$Gradient \ Tree \ Boosting$ & 0.811 & 0.855 & 0.359 & 33 \\

			\bottomrule
		\end{tabular}
		%\hskip-4.0cm
		\ra{1.3}
		%\hskip-4.0cm
		\begin{tabular}{@{}rrrrrc@{}} \toprule
			&  \multicolumn{4}{c}{Problem 4} \\
			\cmidrule{2-5}
			& $Accuracy$ & $ROC AUC$ & $F1$ & $Runtime  (m)$ \\ \midrule
			$Naive Bayes$               & 0.85  & 0.76  & 0.62 & 1 \\
			$Logistic \ Regression$     & 0.883 & 0.85  & 0.331 & 106 \\
			$Random \ Forest$            & 0.898 & 0.853 & 0.393 & 19 \\
			$Gradient \ Tree \ Boosting$ & 0.885 & 0.873 & 0.384 & 47 \\

			\bottomrule
		\end{tabular}
	\end{table*}

\medskip

%\begin{table*}\centering
%%\hskip-4.0cm
%% \caption{Master results table comparing results for all of the classifiers run in this work. For each task and classifier, we show the its $Accuracy$, $ROC AUC$ and $F1$ test-set scores, along with the runtimes of a full cross validation procedures on the learner.}
%\ra{1.3}
%%\hskip-4.0cm
%\begin{tabular}{@{}rrrrrcrrrr@{}} \toprule
%&  \multicolumn{4}{c}{Problem 3} &  \phantom{abc} & \multicolumn{4}{c}{Problem 4} \\
%\cmidrule{2-5} \cmidrule{7-10} %\cmidrule{10-12}
%& $Accuracy$ & $ROC AUC$ & $F1$ & $Runtime (m)$ $$ && $Accuracy$ & $ROC AUC$ & $F1$ & $Runtime  (m)$ \\ \midrule
%$Naive Bayes$                & 0.65  & 0.63  & 0.45  & 1   && 0.85  & 0.76  & 0.62 & 1 \\
%$Logistic \ Regression$      & 0.705 & 0.754 & 0.307 & 115 && 0.883 & 0.85  & 0.331 & 106 \\
%$Random \ Forest$            & 0.792 & 0.845 & 0.346 & 21  && 0.898 & 0.853 & 0.393 & 19\\
%$Gradient \ Tree \ Boosting$ & 0.811 & 0.855 & 0.359 & 33  && 0.885 & 0.873 & 0.384 & 47 \\
%\bottomrule
%\end{tabular}
%\end{table*}
%
%%\end{landscape}
%
%\medskip

\cref{tab:master_table_results} exposes all combinations in model performance and runtime across the problems studied for this work.
At a first glance there are some notable results exposed in this table.

\subsection{Runtime Performance}\label{subsec:master_table_runtime}

It is clear that the Naive Bayes algorithms outperforms all other models in runtime and, in some cases, this improvement is in orders of magnitude.
The algorithm's worst performance for all problems is of two minutes whilst the best runtime for any other classifier is for the Random Forest, with 19 minutes of runtime for the problem of labeling users that traveled out of the endemic region, yet only for a subset of the users base: namely those users which are currently settled outside of the endemic region.

The Naive Bayes algorithm's runtime shows a strong difference with the Logistic Regression which was the slowest method overall, with the fastest runtime for all problems being greater than an hour and a half.
Having the Logistic Regression as the slowest performing algorithm may be surprising to the reader.
Consider that in the runtimes measure timings on the cross validation procedures and that, the logistic classifier, has a short list of hyperparameters combinations to optimize, when compared to tree based methods.
Still, we suspect that the time bottleneck in this algorithm comes in the slow optimization procedure where, for every iteration, the whole dataset needs to be reused to build the direction of steepest descent.

Another result here is that the Gradient Boosting algorithm had a worst runtime than the Random Forest, in cases that almost doubled the runtime.
Even though the boosting algorithms implement several heuristics to speed up the algorithms with local gradient approximations, we also have that when building the Random Forest ensemble, the weak learners need not be dependent, so the optimization routine can create them in parallel computations.
Thus this algorithm benefits strongly from parallelization procedures.
On the contrary, the boosting procedure builds successive trees that improve on the most current model.
This dependency limits the optimization routine parallelization opportunities.

At this point it is worth to mention that the runtimes resulted from experiments that were not performed under controlled computing environments.
For example, we cannot assure that all cross-validation procedures were done in settings with equal available computational resources.
Even though results seemed consistent across all problems, we know that these results shouldn't be considered as benchmarks.


\subsection{Low F1 Scores}\label{subsec:master_table_low_f1}

Overall, we can say that, excepting the problem of predicting users that had lived in the endemic region in the past, no model had satisfactory $F1$ scores for any of the problems. The top two $F1$ scores for \cref{target2,target3,target4} and for any best-fit learners, were $0.62$ and $0.45$.
Both these scores were attained by the Naive Bayes algorithm for the problems of currently non-endemic users that were living in the endemic region in the past, and for the problem of tagging users that had migrated in any direction.

Interestingly enough, these poor scores for the Naive Bayes algorithm were, in comparison, much higher than the ones reached by any other classifier, when compared in relative terms.
All of the other classifiers had extremely poor $F1$ performances in all of \cref{target2,target3,target4}.
The best $F1$ score for the rest of the classifiers, across these problems, was $0.393$.
This was attained by the Random Forest learner when tagging people that had migrated out of the endemic region, from a user-base of those who are currently non-endemic.

We mentioned this situation earlier for the experiments specifically performed on this same \cref{target4}.
We saw that all of the learner's output a very low $Precision$ score and this impacted the overall $F1$ score.

The overconfidence shown by the models derived a high misclassification rate for the samples in the positive class.
This error was so strong, that it drove the $F1$ scores to low values, even when they output acceptable $Recall$ scores.
Still, the trade-off in the the precision of their positive predictions was very strong.

We mentioned that this situation might appear in highly unbalanced classification problems, such as the ones we are working with here.
The overconfidence effect in learners is affected by the unbalanced target class.
With this we can say that precisely tagging migrant users is a very difficult prediction problem, with the data available.

It was surprising to discover that a very modest and fast model could, under this metric, outperform all of the other more complex ones.
We suspect that the Naive Bayes's formulation helps limit the overconfidence of the final model, allowing it to still reach acceptable $Recall$ scores.
In all, we saw that this was the best model for all problems of migrant users, and when evaluated on the $F1$ score.

In contrast, the problem of predicting users that lived in the past, by only knowing their information from the present, is a task that does not present this ill-conditioned class imbalance.
Results show that the Gradient Tree Boosting learner reaches very high $F1$ scores of $95\%$ whilst both the Logistic Regression and Random Forests reach values of $90\%$.

A similar situation was observed in other scores and across all problems.


\subsection{Other Scores}\label{subsec:master_table_other_scores}

For the $ROC AUC$ and $Accuracy$ test scores, we can say that, in general, the best performing learner for all problems were the Gradient Boosting Trees models.\footnote{There is only one specific case where the Boosted learner does not achieve the highest score, and that is for the problem of tagging users that moved out of the endemic region, but only for those users that are currently not endemic.}
It scored high across all problems in the $ROC AUC$ metric, where values ranged from $81.9\%$ for the problem of tagging unidirectional migrants, up to $97\%$ for the task of predicting which users had lived in the endemic region in the past.

We confirm there that this learner, being more complex than the others presented, effectively outputs higher scores in these metrics and
that the model can better predict those different user migrations with lower bias.
We still had to be careful enough not to overfit the model on the training set by configuring shallow base learners or by using stronger regularization of the parameters.

A similar statement can be said for the Random Forest model.
For these metrics only, the results show that this model was in a $5\%$ relative score distance to the Gradient Boosting model, except for the first problem of predicting which users were from the endemic region in the past, where the model was outperformed for more than $10\%$.
We still see that this model had extracted enough predictive information from the dataset to correctly classify users according to these scores.

Under this view we would say that with this dataset provides enough information to effectively predicted long-term migrations with the Random Forest and Gradient Tree Boosting learners.
The performance metrics were more than satisfactory, building best-fit models which could accurately tag users under the $ROC AUC$ and $Accuracy$ metrics.
Also, when compared to the Logistic Classifier, in some cases we see that they both have notable gain in the metrics and for others, such as for the first problem, the Logistic model is on par.

On the other end of the spectrum, we had that the Naive Bayes model had the worst $ROC AUC$ and $Accuracy$ scores for all problems where it reached a maximum $Accuracy$ of $84\%$ for the problem of predicting users that were from the endemic region in $T_0$.

And the next best metric was a maximum $ROC AUC$ of $76\%$ for the case of tagging migrants that moved out of the endemic region, but only making predictions over users that are non-endemic in $T_1$.
These results show that we also had acceptable rates for a model that is not expected to perform.

Overall we found that there are discrepancy in the evaluation scores across the machine learning models here analyzed.
Some are better at capturing the relevant information to predict human movements in the long-term.
This is because not all social information is presented directly in the dataset, and feature interactions are helpful.

With this we showed that it is possible to use the mobile phone records of users during a bounded period (of 5 months) in order to predict whether they have lived in the endemic zone $E_Z$ in a previous time frame (of 19 months).
This task provided to be the easiest classification one, since it resulted in the highest scores across all learners.

The other effective tasks were the prediction of users that migrated in any direction to and from the endemic region, and the tagging of users migrating out of the endemic region, but only for those base of users which are currently non endemic.


%   ___  _     ___         ___________ __ __ _____ _____
%  /   \| |   |   \       / ___/      |  |  |     |     |
% |     | |   |    \     (   \_|      |  |  |   __|   __|
% |  O  | |___|  D  |     \__  |_|  |_|  |  |  |_ |  |_
% |     |     |     |     /  \ | |  | |  :  |   _]|   _]
% |     |     |     |     \    | |  | |     |  |  |  |
%  \___/|_____|_____|      \___| |__|  \__,_|__|  |__|

% % With a running time of 80s, 10 steps are generally needed to achieve a 0.977 validation accuracy score on
% The best model was a Logistic Regression Classifier with an $L2$-penalty value of 0.01.
% % Table~\cref{ts}
% The following table
% shows the scores obtained by the selected model on the out-of-sample set.

% \begin{table}\label{tab:results}[ht]
%   \caption{Resulting scores.}

%   \centering
%   \begin{tabular}{ l l }
%       \toprule
%       Score & Value \\
%       \midrule
%       F1 score & 0.964537  \\
%       Accuracy & 0.980670  \\
%       AUC    & 0.991593  \\
%       Precision & 0.970838  \\
%       Recall  & 0.958316  \\
%       \bottomrule
%   \end{tabular}
% \end{table}


% High values across all scoring measures are achieved
% with the best estimator, with the lowest metric starting at a value of 0.958.
% These results can be explained by the fact that
% %Scores results become less surprising when looking closer at the dataset.


