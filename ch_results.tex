
%===============================================================================
%     File: ch_results.tex
%    Author: Juan de Monasterio
%    Created: 1 Oct 2017
%  Description: Chapter Results
%===============================================================================


% Problem1: We want to infer which users lived in the endemic region in the past.
% Problem 2: We want to infer which users lived in $E_Z$ in the past and then migrated.
% Problem 3: Users that migrated to a different region (any direction movement)
% Problem 4: Users that migrated from the endemic region, but conditional to users which are currently not endemic i.e. the base of users wich


\chapter{Summary of Results}\label{ch:results}

% handling data, models and features with acumen
% After extensive research of machine learning literature and CDR usage in health problems.

The idea of this chapter is to present a combined overview of the work done and the results encountered in
\cref{ch:descr-risk,ch:machineLearning,ch:modelSelection,ch:ensembleMethods}.
All in all, we intend to showcase most of the relevant information from the preceding sections.
The overall picture of this work shows that there is evidence to confirm the hypothesis which stipulates that cell phone usage logs are rich in social interactions and in turn, these interactions are informative enough on long-term migrations and human mobility.

% A summary of the work done we can say we have discussed issues on the most of the following items:

In \cref{ch:descr-risk} we presented the CDR dataset and how it was relevant to the problem.
We showed how the data contained dynamic georeferenced data at the user level.

With a minuscule sample of the CDRs we showed that the dataset is rich in social interactions between users.
This gave us an idea of how this type of data could be leveraged to the general problem.
We then framed our problem in the context of long-term human migrations and gave an idea of the importance of them, in relation to the epidemic nature of the disease.

We noted that the dataset posed a very unbalanced classification problem as most telco users did not show any migration pattern to or from the endemic region, in the time of analysis.
This also implied that we had a very correlated problem, where most users did not change their past and present endemic conditions.
This information, combined with the local calling nature, meant that we had correlations with the target feature.
This was noted by seeing that users living in the endemic region will most probably have lived there in the past as well.

In later sections we also noted that this strong past-to-present correlation in user's homes was relevant to the prediction problems.
This is because for tree based learners, knowing where the user lived in the present, was a strong indicator of where the user would be living in the past.
This information provided the algorithms with features that were highly informative.
Yet we decided to exclude this attribute for the problem of predicting a user's past home endemic condition because we wanted to evaluate the information present in other, less trivial, features.

Moreso, we found other variables having mild interactions with the condition of being a past endemic user.
As expected, people with high mobile interactions with the endemic region, in the form of calls durations and calls counts, also resulted in attributes which were related to users having past endemic homes.
This is most probably due to the locality effect in calling patterns, where most interactions between TelCo users occur in local regions.
Thus users living in endemic regions would call other users nearby.


% in which there are few positive cases over all of the possible.
 % an exploratory We compared the relative sample of users

In this stage we also set to explore the data with feature transformations and visualizations to further discover available in it. % information contained in the data.
As a starting point we transformed the user-level data into a dataset at the antennas level.
This step aggregated information of call patterns to and from the endemic region.
It allowed us to present geolocalized visualizations of these social interactions to the vulnerable areas.
As an intermediate step in this process we had to introduce the definitions that we later used for the rest of this work.
We technically set what we meant was a vulnerable interaction and what we find is a user's home antenna.

% For the latter we base

The antenna level information was finally aggregated into heat maps.
In these, georeferenced call interactions and colored cellphone antennas according to their level of vulnerable interactions, as defined previously.
% We did this with the help of

The visualizations exposed a `temperature' descent from the core regions outwards.
As expected, the heat was noted to be concentrated in the ecoregion.
We also found out that the level of vulnerable interactions per antenna gradually descends as we move further away.
We say this is expected behavior because it is consistent with findings in the literature in which most calls are done to other local antennas. %in general of a local nature.
Given this period and TelCo of analysis, we saw that ninety percent of the users limited usage to at most four Telco antennas.

We also discovered some unexpected findings that were highlighted by the risk maps.
Interactions from non-endemic antennas towards those in the endemic region were seen to be non-homogeneous in some areas.
As an example, \cref{fig:amba_map} outlines various antennas with higher vulnerability.
We suspect this non-uniformity in the vulnerable interactions can help detect communities with higher probability of disease prevalence.

Health experts agree that these anomalies can be a great starting point to start.
Where potential communities atypical in their neighboring region could be worthy of a latent endemic foci.
These antennas stood out for their strong communication ties with the regions studied, showed significantly higher links of vulnerable communication.

In these images presented, the differences in the vulnerable interactions were clear.
When talking to the ``Mundo Sano Foundation'' researchers participating in this project, they pointed to the fact that the detection of these antennas through the visualizations was of great value to their goals.

 % to the health Mundo Sano Foundation researchers participating in this project.

At the national level, the results evidenced by the maps were coherent with the expert's knowledge of the endemic zone's migration patterns.



In \cref{ch:machineLearning,ch:modelSelection} we gave a practical introduction to Machine Learning in general, and of Supervised Classification problems in specific.
This helped us set our Chagas problem inside a systematic process to analyze long-term migrations with the CDR dataset.

To do this, we broke the problem of long-term human migrations down to four tasks which were later analyzed through different classifiers.
In these two chapters, we introduced the machine learning theory necessary to structure our problem, along with its methods to solve them.
These models were some of the most common techniques found in the literature for the task we were trying to solve.

To start off we considered the Logistic Regression Classifier along with the logloss metric to evaluate model performance.
Taking to our advantage the model's more tractable formulation, we showed how model regularization fits inside the machine learning frame.
This concept was introduced along with the notion of model hyperparameters.
Both were relevant to subsequent sections and in particular to \cref{fig:log_loss_regularization_validation_curve}.
There we showed that there is importance in model regularization by fitting a Logistic Regression classifier and comparing how its logloss varias across different regularization values.
This affected both the training and test set performance in a similar way.


At the same time we introduced other relevant concepts in the Machine Learning and we exemplified them from the our long-term prediction problem.
As is common in the literature, in \cref{figure:dtree_overfit_problem_2} we saw that increasing a decision tree's complexity leads to a clear case of overfitting the training data when trying to approximate the generalization error.

The same procedure was used to illustrate the concept of ``Cross Validation'' in \cref{fig:cv_vs_test_score}.
We found that when making predictions on which users were endemic in the past, the test score was inside a one standard deviation band from the CV score, across a varying hyperparameter value for the Logistic Regression classifier.
These findings are consistent with lots of other experiments found in the literature where by cross validating the search of optimal hyperparamters, we can have a fair estimation of the test error.


Towards the end of \cref{ch:modelSelection} we performed a deeper experimentation with this same problem and also on users that moved out of the endemic region.,
 % which characterized unidirectional migrants, from the endemic region.
Here we used the same classifier as before but on the whole dataset in what defined our systematic approach of analyzing classifiers performance.
% through cross validation, hyperparameter error assessment, feature selections through iterations and visualization of validation curves.

 % and found we did this on the whole dataset in an approach which defined systematic procedure.
% In here we finished testing the Logistic Classifier on the whole dataset, and with a systematic approach.

The runtime for this experiment had a lower bound of one and a half hours.
This was for all of the cross-validated fits and for the whole set.
Also since we limited the number of iterations to at most one hundred steps, this runtime is actually smaller than the the real optimization time for this algorithm,

Another issue found with this algorithm was its extremely poor $F1$ performance for unidirectional migrants.
We found that it had a test score lower than $1\%$.
As we will see later in other statistical learners, we had that
the overall precision of the classifier was very poor due to not being able to accurately predict positive samples.
The algorithm was very inefficient in this aspect of overestimating users that migrated out of the endemic region, and this misclassification had a direct impact in the observed $F1$.


However this low score was not repeated in the rest of the metrics we evaluated.
For this same problem, both the best $Recall$ and $Accuracy$ test and CV rates were over $61\%$.

Another relevant observation was that best-fit $C$ values were not consistent along all fits, because these resulting values were seen to differ by orders of magnitude.
Solutions for $C$ ranged from to $1E-3$ to $1E-1$ on different fits.
With this we can still positively affirm that the best-fit models for this experiment were those which penalized the loss function with strong regularization terms.


In contrast, when using this classifier to predict past endemic users residents, we found that we could get a $ROC AUC$ score of $76\%$.
Moreover this score is achieved when cross validating solely on the $l1$ regularization parameter.

A similar test score of $74.4\%$ was reached when optimizing for the $l2$ hyperparameter of regularization.
Each were optimized separately in order to exemplify how the overall $ROC AUC$ varied along different regularization thresholds.

This outcome of better $ROC AUC$ scores, when compared with the experiments on migrations out of the endemic region, underscore the difference among the problems' difficulty.
If we recall the highly unbalanced class relations for the problem of predicting unidirectional migrants in \cref{target2}, we confirm our hypothesis that these predictions characterize a problem which is very difficult.

Overall, both procedures had best fits on the more regularized $C$values.
A similar result can be found for regularization models and on procedures where hyperparameters were cross validated.
Both optimizations improved improved the model's predictive power in varying degrees.


The previous results outline a complete systematical Machine Learning approach and toolset used to evaluate the long-term migrations prediction problem.
In this way we provide a methodical way to analyze classifiers performing on this problem.
With this we tackled the task with the introduction of new algorithms in the following chapters.



In\cref{ch:ensembleMethods} we introduce four new classifiers along with their properties and characteristics. Three of these, \cref{section:decision_trees,section:random_forests,section:gradient_boosting}, were tree based methods whilst the last, \cref{section:naive_bayes}, was a Naive Bayes Classifier used for benchmarking purposes.
For each, we presented an introductory review and pointed the reader to further literature references where applicable.

All but the Decision Tree learners were put through a number of experiments to evaluate how they performed across all prediction problems.
For each model, we tried to draw the greatest prediction performance from the features extracted from the dataset.

Random Forest and Gradient Boosting models were evaluated in further detail however.
And even though Decision Trees were not systematically analyzed, they were added as a departure point for the following ones.

As a start, we showed through a validation curve that, for the problem of users moving out of the endemic regions, the learners would become overfit if we let the trees grow in depth.
We had that the cross-validated scores and the test scores diverged rapidly after a tree-depth of $7$ and this is plotted in \cref{figure:dtree_overfit_problem_2}.
The wide gap in the scores resulted in a difference larger than $30\%$.

In conclusion, the overfitting effect in this learner was visible in the experiments where deep trees and complex configurations underpowered the resulting model's generalization error.
This is not surprising as Decision Trees are known to become highly complex after a certain level of tree growth.

% Ideally it serves as an example of what aspects of this greedy algorithm are enhanced by Random Forests.

We also fit two Decision Tree instances for the same problem to evaluate the model with an $Accuracy$ score.
The first of two fits was ran over a smaller hyperparameter configuration and, in particular, this configuration did not consider a rebalancing of the samples when computing the value of the loss function over the whole set.
The second fit was done with this balancing to account on how this difference affects the algorithm's outcome.

To aid our understanding of this model's outcome we also decided to create visualizations for both fitted trees.
Each tree is plotted in a graph which shows up to five levels of splits.
With this we showed, for each node, the attribute used in that splitting decision and the actual split value.

These plots provided some insights into what features were initially being used by the algorithm to separate samples, according to their split index performance.
For instance, in both fits the root split was based on the user's mobility diameter, with split values near to $47.5$ kilometres in both cases.
This means that a mobility diameter of $47.5$ was the best split decision the algorithm could take in order to improve the gini index of the sub groups.
Recall here that the mobility diameter quantifies the user's area of influence given by his mobile connection.

The other significant features, selected at splits near to the tree's root level, was the user's home antenna call count.
Both the volume of calls during the whole week and usage specifically during weeknights time was important to the node-splitting algorithm.

For both trees, the split values for these two variables were relatively similar, where in the volume of weeknight calls using the home antenna was of $10.5$, whilst for total volume of home antenna calls, at any given time, the values were $139$ and $186$ calls for each case.
These split values were characterizing segments of heavy users, that is users that are higher than the median values for the home antenna usage variables.% on the upper usage percentiles for home antennas.

% This result makes does not strike as wild as
% Still this result does not imply that all split all users
 % It seems that a big area when using his mobile phone and as such, this attribute might indicative of past migrations.


Here we must note however, that both best-fit trees result in a low $Accuracy$ scores of $43.2\%$,and $57.9\%$ respectively.
Under these circumstances we find that these top features selected when building the trees could not be very informative of the overall problem in users that migrated out of the endemic region.
This means that, due to the low performance of these learners, the interpretations we build from this output can be based from the wrong assumptions.


% we could be tempted to interpret the

Interestingly enough, in these experiments we found that for this learner, rebalancing the set helped us increase the learner's score by a great amount.
We hypothesize that this is a valuable hyperparameter that helps when facing problems with target classes are unbalanced.

% With this score, noting which features were selected near the root leaves of the tree would not be very informative to the overall problem.
% Experiments with the Decision Tree was used



The rest of the \cref{ch:ensembleMethods} follows with the presentation of other learners.
All of them are introduced with a brief explanation of their formulation and their distinctive characteristics.

For each of these, we successively experiment the same difficult predictive task.
Where we look to tag users that migrated out of the endemic region, from time periods $T_0$ to $T_1$.
In this way se set a common ground to compare and evaluate the algorithms.
% settled to compare and evaluate them on common ground.

As we will see later in \cref{tab:master_table_results}, this task, \cref{target2}, happened to be the hardest ones, as it resulted in the lowest metric scores for all classifiers.

We performed extensive experimentation both on the Random Forest and Gradient Tree Boosting learners.
In both we compared the results with those previously obtained for the Decision Tree, where we pointed to the improvements brought by these ensemble learners.

\todo{FILL in with experimentation from RF and GBT.}

% By comparing this problem over the various tree methods we discuss on applications that these kind of datasets can provide.
% to these kinds of tasks was elaborated.

\todo{Add that random Forests improving decision tree overfitting. Gradient Boosting better in bias with a slight incrase in overfit}


% The Random Forest model played an important role in having the highest scores across all tasks and in presenting the best features of the dataset.
% This information was later availed by the boosting models trained on said set of features.



\todo{ Fill in: Using best features from random forests in gradient boosting models incorporates some overfitting in the model since we use target information to select the best features. However, independent experiments confirm the performance of gradient tree boosting methods.}



In summary, we can say that for the prediction tasks, the feature selection heuristics of ensemble algorithms did agree upon a group of features.
These were highlighted over the rest for a number of times and they provide insight into what CDR information provided most
predictive value.

From the results we collected, the best features can be broken down into the following categories.

\begin{itemize}

    \item Calls made in older months and in December: Interactions of different type, that occured closer to the split month between both periods $T_0$ and $T_1$ (July), were also distinguished.
    The model frequently used interactions from the months of August and September, as well as December.
    We conjecture that this last result can be due to the fact that December is a month where user's increase their mobile activity with their families.

    \item Vulnerable calling patterns: As we first suspected in \cref{ch:descr-risk}, the vulnerability of the mobile interaction between users was relevant.
    Measurements such as the duration and volume of calls at different time periods where selected.
    We make the reminder here that these measurments were the basis of our construction of the heatmaps in that same chapter.
    Where the antennas plotted concentrated users with vulnerable calls by calling volume or time, for a given month.

    \item Mobility size: Finally, different measures of the distance in human mobility were determined to be important by the model. Both general mobility diameters and weekday or weeknight specific mobility endemic were relevant and on various occasions.
    We can informally add that Decision Tree Learners also selected this features in the higher nodes.
    This result can be seen in detail at \cref{subsection:decision_trees_experiment}.

\end{itemize}

These groups identify strong predictors in long-term human mobility and provide additional insights into our original hypothesis of which predictors were most valuable for the task.
Understanding these factors can further provide information into the problem of chagasic disease spread in the long run.
A more thorough examination of these results can be seen in \cref{tab:random_forest_big_experiment_best_features, tab:boosting_big_experiment_best_features}.




\section{Missing parts Plan to add to this section }

\begin{description}

    \item [DONE] Feature importance. Mobility and phone usage were, surprisingly, relevant attributes in most of the experiments run.
    \item [DONE] The map attributes were confirmed to be relevant by the best-feature ranker for random forests/ gradient tree boosting algorithms.

    \item What's the predictive performance of CDRs for the long-term migrations problem?

\end{description}




From our findings, we can say that there is no absolute single method that can be single-handedly applied to all problems and evaluation performances.
However, our results outline that the Gradient Boosting methods were, in general, top performing for all tasks and classifiation metrics, except for the $F1$ score.

Surprisingly enough, for some problems the Naive Bayes was outperforming in this $F1$ metric.
The algorithm was more conservative in its positive predictions and this difference saw a tradeoff in its performance across other metrics.

However we can say that results show that the performance of Gradient Boosting models is best over other models such as Logistic Regression or Naive Bayes algorithms for most tasks, and slightly better over the Random Forests.
These models can find complex interactions in the data whilst handling enough care not to lose generalization power.


% Each of the tasks will have different scores across the algorithms used.
% and the same applies to the metrics used to evaluate them.

% ||||||||||||||||||||||||||||||||||||| SPLIT |||||||||

\section{Master Table of Results}\label{sec:master_table}

\cref{tab:master_table_results} shows a compendium of the results obtained from the CDR dataset, using all classification models.
For ech problem and model, we found the best performing learner with a cross validation procedure which searched over an extensive grid of hyperparameters.
In all cases, the hyperparameter configuration with highest cross-validated $ROC AUC$ was selected as the \textit{best-fit}.
%Due to time constraints limited by the use of full cross validation fitting procedures, not all models were run for all of the tasks.
The following table shows the best-fit classifier's performance across three scores on the test set $\mathcal{T_s}$: $Accuracy$, $F1$ and $ROC AUC$.

\begin{table}
\caption{Master resutls table comparing results for all of the classifiers run in this work.
For each task and classifer, we show the its $Accuracy$, $ROC AUC$ and $F1$ test-set scores, along with the runtimes of a full cross validation procedures on the learner.}
\label{tab:master_table_results}
\centering
    \begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill} }  l l l l l }
    %{|p{2cm}|p{2cm}|p{1.5cm}|p{1cm}|p{1.5cm}}
    \toprule
    Measure & Problem 1 & Problem 2 & Problem 3 & Problem 4  \\
    \midrule
    Naive Bayes     & (0.84,0.82,0.75)  & (0.64,0.61,0.31)  &  (0.65,0.63,0.45)   & (0.85,0.76,0.62)   \\
    Logistic Classifier   & (0.893,0.857,0.9)  & (0.71,0.72,0.058)  &  (0.705,0.754,0.107)   & (0.883,0.85,0.181)   \\
    Random Forest   & (0.878,0.857,0.9)  & (0.79,0.776,0.088)  &  (0.792,0.845,0.156)   & (0.898,0.853,0.203)   \\
    Gradient Boosting   & (0.974,0.978,0.952)  & (0.838,0.819,0.101)  &  (0.811,0.855,0.169)   & (0.885,0.873,0.194)   \\
%$Naive Bayes$         & 0.84  & 0.82  & 0.75  & 2  && 0.64  & 0.61  & 0.31  & 2  && 0.65   & 0.63  & 0.45  & 1   && 0.85  & 0.76  & 0.62  & 1 \\
%$Logistic \ Regression& 0.893 & 0.857 & 0.9   & 96 && 0.714 & 0.726 & 0.058 & 119 && 0.705 & 0.754 & 0.107 & 115 && 0.883 & 0.85  & 0.181 & 106 \\
%$Random \ Forest$     & 0.878 & 0.903 & 0.77  & 33 && 0.79  & 0.776 & 0.088 & 45  && 0.792 & 0.845 & 0.156 & 21  && 0.898 & 0.853 & 0.203 & 19\\
%$Gradient \ Tree \ Boo& 0.974 & 0.978 & 0.952 & 41 && 0.838 & 0.819 & 0.101 & 54  && 0.811 & 0.855 & 0.169 & 33  && 0.885 & 0.873 & 0.194 & 47 \\

    \bottomrule
    \end{tabular*}
\end{table}


\todo{Bigger, better and more esthetically pleasing table is commented in the code below this one. Yet the commented table does not fit in a whole page and spawns compilation errors. This could be fixed by splitting the table in half.}

% \begin{landscape}% Landscape page
%         \begin{table*}
%             \centering
%             \ra{1.3}
%             \begin{tabular}{@{}rrrrcrrrrcrrcrr@{}} \toprule
%                 &  \multicolumn{4}{c}{Problem 1} &  \multicolumn{4}{c}{Problem 2} & \multicolumn{4}{c}{Problem 3}  & \multicolumn{4}{c}{Problem 4}\\
%                  \cmidrule{2-5} \cmidrule{6-9} \cmidrule{10-13} \cmidrule{14-17}
%                 & $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ \\
%                 \midrule
%                 $Naive Bayes$ & 0.84 & 0.82 & 0.75 & 2 && 0.64 & 0.61 & 0.31 & 2 && 0.65 & 0.63 & 0.45 & 1 && 0.85 & 0.76 & 0.62 & 1 \\
%                 $Logistic \ Regression$ & 0.893 & 0.857 & 0.9 & 96 && 0.714& 0.726 & 0.058  & 119 && 0.705& 0.754 & 0.107 & 115 && 0.883 & 0.85 & 0.181 & 106 \\
%                 $Random \ Forest$ & 0.878 & 0.903 & 0.77 & 33  && 0.79 & 0.776 & 0.088 & 45 && 0.792 & 0.845 & 0.156 & 21 && 0.898 & 0.853 & 0.203 &  19\\
%                 $Gradient \ Tree \ Boosting$  & 0.974 & 0.978 & 0.952 & 41 && 0.838 & 0.819 &  0.101 & 54  && 0.811 & 0.855 & 0.169 & 33 && 0.885 & 0.873  & 0.194 & 47 \\
%                 \bottomrule
%             \end{tabular}
%             \caption{Caption}
%         \end{table*}
% \end{landscape}


\cref{tab:master_table_results} exposes all combinations in model performance and runtime across the problems studied for this work.
At a first glance there are some notable results exposed in this table.

\subsection{Runtime Performance}\label{subsec:master_table_runtime}

It is clear that the Naive Bayes algorithms outperforms all other models in runtime and, in some cases, this improvement is in orders of magnitude.
The algorithm's worst performance for all problems is of two minutes whilst the best runtime for any other classifier is for the Random Forest, with 19 minutes of runtime for the problem of labeling users that travelled out of the endemic region, yet only for a subset of the users base.
Namely those which are currently settled outside of the endemic region.

The Naive Bayes algorithm's runtime shows a strong difference with the Logistic Regression which was the slowest method overall, with the fastest runtime for all problems being greater than an hour and a half.
Having the Logistic Regression as the slowest performing algorithm may be surprising to the reader considering that in these runtimes are measured on the cross validation procedures and that this classifier has a short list of hyperparameters to optimize, when compared to tree based methods.
Still, we suspect that the time bottleneck in this algorithm comes in the slow optimization procedure where, for every iteration, the whole dataset needs to be reused to build the direction of steepest descent.

Interestingly enough the Gradient Boosting algorithm had a worst runtime than the Random Forest, in cases that almost doubled the runtime.
The reason for this is that when building the Random Forest ensemble we have that the weak learners need not be dependent, so the optimization routine can be easily benefited with parallelization procedures.
On the other hand boosting trees build successive weak learners that improve on the most current model.
This dependency limits the optimization routine parallelization opportunities.

Still, at this point it is worth to mention that the runtimes resulted from experiments that were not performed under controlled computing environments.
For example, we can no assure that all cross-validation procedures were done in settings with equal available computational resources.
Even though results seemed consistent across all problems, we know that these results shouldn't be considered as benchmarks.


\subsection{Low F1 Scores}\label{subsec:master_table_low_f1}

Overall, we can say that, excepting the problem of predicting users that had lived in the endemic region in the past, no model had satisfactory $F1$ scores for any of the problems. The top two $F1$ score for \cref{target2,target3,target4} and for any best-fit learners, were $0.62$ and $0.45$.
Both these scores were attained by the Naive Bayes algorithm for the problems of currently non-endemic users that were living in the endemic region in the past, and for the problem of tagging useres tht had migrated in any direction.

Interestingly enough, these poor scores for the Naive Bayes algorithm were in comprison much higher than the ones reached by any other classifier, when compared in relative terms.
All of the other classifiers had extremely poor $F1$ performances in all of \cref{target2,target3,target4}.
The best $F1$ score for all of these classifieres and for any of these problems, was $0.194$.

We mentioned this situation earlier for the experiments specifically performed on the problem of tagging unidirectional migrants, for
Where for all of the learners output a very low $Precision$ score.

The overconfidence shown by the models dervied a high misclassification rate for the samples in the positive class.
This error was so strong, that it drove the $F1$ scores to very low values, even when showing acceptable $Recall$ scores.
Models seemed to output acceptable $Recall$ with a huge tradeoff in the the precision of their positive predictions.

Also, in highly unbalanced classification problems, such as the ones we are working with here, we see this overconfidence effect in learners affected by the unbalanced target class.
With this we can say that precisely tagging migrant users can be a very difficult prediction problem.

It was surprising to discover that a very modest and fast model could, under this metric, outperform all of the other more complex ones.
We suspect that the Naive Bayes's formulation helps limit the overconfidence of the final model, allowing it to still reach acceptable $Recall$ scores.
In all, we saw that this was the best model for all problems of migrant users, and when evaluated on the $F1$ score.

In contrast, the problem of predicting users that lived in the past, by only knowing their information from the present, is a task that does not present this ill-conditioned class imbalance.
Results show that the Gradient Tree Boosting learner reaches very high $F1$ scores of $95\%$ whilst both the Logistic Regression and Random Forests reach values of $90\%$.

A very similar situation was observed across other scores and for all of the problems.


\subsection{Other Scores}\label{subsec:master_table_other_scores}

For the $ROC AUC$ and $Accuracy$ test scores, we can say that, in general, the best performing learner for all problems were the Gradient Boosting Trees.\footnote{There is only one specific case where the Boosted learner does not achieve the highest score, and that is for the problem of tagging users that moved out of the endemic region, but only for those users that are currently not endemic.}

It scored high across all problems in the $ROC AUC$ metric, where values ranged from $81.9\%$ for the problem of tagging unidirectional migrants, up to $97\%$ for the task of predicting which users had lived in the endemic region in the past.

We confirm there that this learner, being more complex than the others presented, effectively outputs higher scores for these metrics.
The model can better predict those different user migrations with lower bias and with enough regularization that it does not overfit the training set.

\todo{ Overall Naive Bayes has the lowest performance on roc and accuracy.}

On the other end of the spectrum, we


\todo{High and acceptable ROC AUC for RF as well.}


Under this view we would say that with this dataset provides enough information to effectively predicted long-term migrations with the Random Forest and Gradient Tree Bossting learners.
The resulting scores were more than satisfactory, building best-fit models which could accurately tag users under the $ROC AUC$ and $Accuracy$ metrics.
 % with maximum test scores over 81\%.


High discrepancies exist in the scores of the different machine learning models.
Some are better at capturing the relevant information to predict human movements in the long-term.
This is because not all social information is presented directly in the dataset, and feature interactions are helpful.

All in all, we showed that it is possible to use the mobile phone records of users during a bounded period (of 5 months) in order to predict whether they have lived in the endemic zone $E_Z$ in a previous time frame (of 19 months).
This task provided to be the easiest classification one, since it resulted in the highest scores across all learners.

The other effective tasks were the prediction of users that migrated in any direction to and from the endemic region, and the tagging of users migrating out of the endemic region, but only for those base of users which are currently non endemic.


%   ___  _     ___         ___________ __ __ _____ _____
%  /   \| |   |   \       / ___/      |  |  |     |     |
% |     | |   |    \     (   \_|      |  |  |   __|   __|
% |  O  | |___|  D  |     \__  |_|  |_|  |  |  |_ |  |_
% |     |     |     |     /  \ | |  | |  :  |   _]|   _]
% |     |     |     |     \    | |  | |     |  |  |  |
%  \___/|_____|_____|      \___| |__|  \__,_|__|  |__|

% % With a running time of 80s, 10 steps are generally needed to achieve a 0.977 validation accuracy score on
% The best model was a Logistic Regression Classifier with an $L2$-penalty value of 0.01.
% % Table~\cref{ts}
% The following table
% shows the scores obtained by the selected model on the out-of-sample set.

% \begin{table}\label{tab:results}[ht]
%   \caption{Resulting scores.}

%   \centering
%   \begin{tabular}{ l l }
%       \toprule
%       Score & Value \\
%       \midrule
%       F1 score & 0.964537  \\
%       Accuracy & 0.980670  \\
%       AUC    & 0.991593  \\
%       Precision & 0.970838  \\
%       Recall  & 0.958316  \\
%       \bottomrule
%   \end{tabular}
% \end{table}


% High values across all scoring measures are achieved
% with the best estimator, with the lowest metric starting at a value of 0.958.
% These results can be explained by the fact that
% %Scores results become less surprising when looking closer at the dataset.
%
%Si del test\_set ahora me quedo con lo users que Y\_target ==1 == EPIDEMIC\_gt pero que hoy en dia son epidemic ==0 (se mudaron)s
 % scores bajan mucho:
%
%+--------------+-----------------+-------+
%| target\_label | predicted\_label | count |
%+--------------+-----------------+-------+
%|   1    |    1    | 1944 |
%|   1    |    0    | 3525 |
%+--------------+-----------------+-------+
%'f1\_score': 0.5244840145690004, ''accuracy': 0.3554580,, 'precision': 1.0, 'recall': 0.35545803,

% \section{Key Results}

