\subsubsection{Notation and Terminology}

For the purpose of this work we will be talking of the training set, noted by $\mathrm{T}$ as the set of data examples. The objective is to build a probabilistic model which has the capacity to predict correctly the class instance of new data objects based on having seen information of other data objects. 

For concreteness, let's consider a reduced dataset built from Call Detail Records (CDRs) where samples are calls being made by users who can belong to any of following provinces : \textit{Buenos Aires}, \textit{Cordoba} and \textit{Santa Fe}. 
%\url{http://stackoverflow.com/}
%\footnote{For more information on this set, a historic review is given at \url{https://en.wikipedia.org/wiki/Iris_flower_data_set}}.
Five measurements were made on all of the observations to account for the user's number of calls and total duration of calls  during a week of measurements. A short example of this dataset can be seen below:

\begin{table}[ht]
\caption{Head of the CDR dataset}
\label{tab:sample_CDR}
\centering
\begin{tabular}{ l l l l l l }
\toprule
User & CallsWeekend & TimeWeekend & CallsWeekDays & TimeWeekday & Province \\
\midrule
BA343E  & 15 &  89 & 8 & 24 &  \textit{Santa Fe}\\
73F169  & 10 &  121 & 2 & 98  &  \textit{Cordoba} \\
EA23AD  & 12 &  43 & 5 & 154 &  \textit{Buenos Aires} \\
\bottomrule
\end{tabular}
\end{table}

In this form a row is representing the available data acquired from each acquired and columns represent types of measurements or information. In general, most machine learning problems will be associated with a training set $\mathrm{T}$ of similar form as the one shown before. Where rows represent objects or \textit{samples} and columns are measurements or \textit{features} of our samples. Here $\mathrm{T}$ will take the form of a paired couple of datasets $(X,Y)$ where $X \in \mathbb{R}^{n \ x \ p}$  and $Y$ is, in general, a real valued vector of length $n$. The $jth$ column of $X$ or equivalently the $jth$ feature of the data is denoted by $X_j$. Similarly, the $ith$ row or sample of $X$ is denoted by $X^i$ or $x$ when the superscript is understood from context. A similar notation is used with the outputs, where $Y^i$ of $y$ will be used to denote the target of a specific sample depending on context. 

In this particular case, even though the last column of the dataset in \ref{tab:sample_CDR} is not a measurement \textit{per se}, it gives information on each user's province of residence. From here, there are various questions one could try to answer. Examples of problems that a machine learning algorithm could tackle from this data could be: 

\begin{itemize}
	\item Predict a user's province when given information on only the first four measurements.
	\item Predict a user's number of calls made on weekends when given information on the last four measurements.
	\item Give an estimate of the probability density function for a user's calling time during weekdays.
\end{itemize}

The first two problems are examples of supervised learning where the $Y$ variables or \textit{labels} are the last and first features (columns) respectively. Even more, the first problem is a classification one since users are to be classified according to one of the three possible provinces whilst the second one is a regression problem for which the output could be any of a range of numerical values. The labels in classification problems are numerically encoded with a finite range of numbers where $\{0,1\}$ or $\{-1,1\}$ are usually used for the binary case.

Note that there are no assumptions made here about the data which we take as is given. %to us in this way. 
This is common in machine learning applications and because of this, algorithms tend to be designed to account for this situation. The type of problems and questions that could be done then depend entirely on the training set. 

The last example problem in the list before, belongs to the unsupervised learning category where there is no need to have a label on the data. Here the question is on the structure of a specific column, namely the unobservable probability distribution underlying the calling time. In this setting there is no output expected for new samples.

To illustrate further the supervised classification scenario, a brief notation outline is described below following a standard logistic regression classifier example:

Let's suppose that the problem is to determine if a user belongs to the province of \textit{Cordoba} vs the rest of the provinces. This brings us to a learning problem with only two classes. %us to a two class learning problem.
 
The objective is to build a learner $f$ from a linear combination of the input features that will predict the target output. Ideally we would have that for every sample $y = f(x) = h\left(\sum_{j}\theta_jx_j\right)$ \label{formula:1} \footnote{In formula \ref{formula:1} we have omitted the intercept parameter , generally noted as $\theta_0$. The reason for this is that one can include this parameter implicitly if we allow for a synthetic feature $X_0$ in $X$ which is a vector with all of its components equal to 1.  } where $\theta$ is unknown and generally referred to as the coefficient or parameter. 

If we let $ t^i = \theta \cdot x^i  \ \forall \ i $,in the ideal case we will have that $\exists \theta s.t. \forall i $ 
\[
    %\begin{equation}
    t^i
      \begin{cases}
        &>0 \ \mbox{if} \ y^i=1 \\
        &<0 \ \mbox{if} \ y^i=0.
      \end{cases}
    %\end{equation}
    \]
%       \forall 1 \leq i \leq n = y^i $

Then if we were to use the heavside step function $h(z)$  where

\[
    %\begin{equation}
    h(z) =
      \begin{cases}
        &0 \ \mbox{if} \ z<0 \\
        &\frac{1}{2} \ \mbox{if} \  z=0 \\
        &1 \ \mbox{if} \  z>0.
      \end{cases}
    %\end{equation}
    \]

Under the assumption that $\nexists\  i \  s.t. \ t^i = 0$, the algorithm would then correctly classify all samples to their targets. This situation is hardly the case, so a different approach is taken. A more tractable function for this task is the logistic function which is an approximation of the heavyside step function. Here  

\begin{equation} \label{eq:logit}
h(z)  = \frac{e^{z}}{1 + e^{z}} = \frac{1}{1 + e^{-z}}  \\
and \  H(z) = \lim_{k \to \infty} \left(\frac{1}{2} + \frac{1}{2}tanh(kz) \right) = \lim_{k \to \infty} \left(\frac{1}{1+e^{-kz}} \right)  
\end{equation}


The logistic function has the advantages of being smooth, invertible, well defined for all real numbers and its image is $(0,1)$. This property helps to read outputs as probabilities. For the scenario characterized in \ref{formula:1} we would have to finally assign each output to a specific class. A common approach for this is to categorize each output whether $\hat{y} > 0.5$ \label{formula:logitThreshold}. Notice that having $h(x \cdot \theta) = 0.5$ implies that $x \cdot  \theta = 0$ and thus our classifier is separating samples in feature space (the space of the inputs) with the hyperplane defined by the parameter $\theta$. Also, having a higher $\hat{y}$ for a given sample implies that it is further away from the hyperplane. The same goes for low estimated targets. If we were to read this as a probability, we can interpret that the algorithm is showing a higher confidence in class  then it corresponds to having a higher confidence in the classification.

%This means that 

%It also happens to be an approximating function to the

As we know, the learner will approximate the target with $y \approx \hat{y} = h\left(\sum_{j}\theta_jx_j\right)$ and the first idea leads to find the hyperplane that best separates the two classes by estimating parameter $\hat{\theta}$. Given that we have now a problem of $p$ degrees of freedom, what is left is to find the parameter by optimizing a certain criteria. This choice will certainly depend on the way we decide that one parameter is better than another. For example the choice of $0.5$ as a threshold in \ref{formula:logitThreshold} is ad-hoc and certainly one which we could try to fit in the optimization process. In the context of machine learning the criteria used to choose the best parameters is called a \textit{metric}.

A naive approach to find the parameters in our problem would be to minimize the residual sum of squares. Typical of other scenarios such as linear regression, one would like to minimize the residual sum of squares (RSS)  

\begin{equation} \label{eq:rss}
RSS(\theta_0,..,\theta_p)  = \sum_{i=1}^n [y^i - \hat{y}^i]^2  \\
=  \sum_{i=1}^n [y^i - h( \theta \cdot x^i)]^2
\end{equation}

If we assume that $y  =  f(x)  +  \epsilon $ is a good relationship for our data, where $\epsilon \sim \calN(0,1) $ 
