

\subsubsection{Notation and Terminology}

For the purpose of this work we will be talking of the training set, noted by $\mathrm{T}$ as the set of data examples. The objective is to build a probabilistic model which has the capacity to predict correctly the class instance of new data objects based on having seen information of other data objects. 

As an example let's consider a reduced dataset built from Call Detail Records (CDRs) where samples are calls being made by users who can belong to any of following provinces : \textit{Buenos Aires}, \textit{Cordoba} and \textit{Santa Fe}. 
%\url{http://stackoverflow.com/}
%\footnote{For more information on this set, a historic review is given at \url{https://en.wikipedia.org/wiki/Iris_flower_data_set}}.
Five measurements were made on all of the observations to account for the user's number of calls and total duration of calls  during a week of measurements. A short example of this dataset can be seen below:

\begin{table}[ht]
\caption{Head of the CDR dataset}
\label{tab:sample_CDR}
\centering
\begin{tabular}{ l l l l l l }
\toprule
User & CallsWeekend & TimeWeekend & CallsWeekDays & TimeWeekday & Province \\
\midrule
BA343E  & 15 &  89 & 8 & 24 &  \textit{Santa Fe}\\
73F169  & 10 &  121 & 2 & 98  &  \textit{Cordoba} \\
EA23AD  & 12 &  43 & 5 & 154 &  \textit{Buenos Aires} \\
\bottomrule
\end{tabular}
\end{table}

In this form a row is representing the available data acquired from each acquired and columns represent types of measurements or information. In general, most machine learning problems will be associated with a training set $\mathrm{T}$ of similar form as the one shown before. Where rows represent objects or \textit{samples} and columns are measurements or \textit{features} of our samples. Here $\mathrm{T}$ will take the form of a paired couple of datasets $(X,Y)$ where $X$ is a matrix and $Y$ is, in general, a vector. The $jth$ column of $X$ or equivalently the $jth$ feature of the data is denoted by $X_j$. Similarly, the $ith$ row or sample of $X$ is denoted by $X^i$ or $x$ when the superscript is understood from context. A similar notation is used with the outputs, where $Y^i$ of $y$ will be used to denote the target of a specific sample depending on context. 

In this particular case, even that even though the last column of the dataset in \ref{tab:sample_CDR} is not a measurement \texit{per se}, it gives information on each user's province of residence. From here, there are various questions one could try to answer. Examples of problems that a machine learning algorithm could tackle from this data could be: 

\begin{itemize}
	\item Predict a user's province when given information on only the first four measurements.
	\item Predict a user's number of calls made on weekends when given information on the last four measurements.
	\item Give an estimate of the probability density function for a user's calling time during weekdays.
\end{itemize}

The first two problems are examples of supervised learning where the $Y$ variables or \textit{labels} are the last and first features (columns) respectively. Even more, the first problem is a classification one since users are to be classified according to one of the three possible provinces whilst the second one is a regression problem for which the output could be any of a range of numerical values. The labels in classification problems are numerically encoded with a finite range of numbers where $\{0,1\}$ or $\{-1,1\}$ are usually used for the binary case.

Note here that there are no assumptions made here about the data which we take as is given %to us in this way. 
This is common in machine learning applications and algorithms tend to be designed to account for this situation. The type of problems and questions that could be done then depend entirely on the training set. 

The last example problem in the list before, belongs to the unsupervised learning category where there is no need to have a label on the data. Here the question is on the structure of a specific column, namely the unobservable probability distribution underlying the calling time. In this setting there is no output expected for new samples.

To illustrate further the supervised classification scenario, a brief notation outline is described below following a standard logistic regression classifier example:

Let's suppose that the problem is to determine if a user belongs to the province of \textit{Cordoba} vs the rest of the provinces. This brings us to a learning problem with only two classes. %us to a two class learning problem.
 
The objective is to build a learner $f$ from a linear combination of the input features that will predict the target output. Ideally we would have that for every sample $y =  f\left(\theta_0 + \sum_{j}\theta_jx_j\right)$ where $\theta$ is unknown generally referred to as coefficient or parameter. However the learner will approximate the target with $y \approx \hat{y} = f\left(\theta_0 +\sum_{j}\theta_jx_j\right)$ and the first idea will be to find the best fit parameters $\hat{\theta}$. Then 

\footnote{In this setting it is common to say that the classifier is a generalized linear model. This might confuse the reader when observing that a logistic regression's output is certainly \textbf{not} linear on the inputs. However, the term referrs to the fact that there exists a bijective function $g(x) = f^{-1}(x)$ such that the transformed outputs are linear in the inputs i.e. $g(y) = \langle x, \theta \rangle$ }

At this point it is important to start noting certain subtleties in the standard terminology used for machine learning. Most terms have equivalent or similar notations in other closely related areas of knowledge like statistics. As it was shown before the \textit{dependent} $Y$ is called the target or label. Also, the \textit{independent} variables or \textit{covariates} are named as features and their 
Labels that are representing categorical or discrete variables are also named factors or \textit{qualitative} variables. 



A typical machine learning problem starts with an object of study, for example flowers, and data collected from these objects. In the flower scenario 

Other subtleties worth noting is the 

\textit{any reasonable machine learning method can be formulated as a formal probabilistic model, so in this sense machine learning is very much the same as statistics, but it differs in that it generally doesn't care about parameter estimates (just prediction) and it focuses on computational efficiency and large datasets.}

(CITAR BREIMAN \cite{breiman-statisticalmodeling} TWO cultures IN STATISTICal modelling)



\section{Bias, Variance, Generalization and Model Complexity}

\subsection{Hastie Tibshiranie Friedman}

\textbf{Gralization}
Given a problem setting, it is said that an algorithm's predictive power relies on its ability to correctly label samples on independent test data. Measuring and comparing this ability to correctly label data amongst models is central to supervised learning.  The best learner will be said to have the lowest \textit{predictive error} where this error can be divided into an "error" due to \textit{bias} and an "error" due to \textit{variance}.

Conceputally the first type of error grows 


$EPE\left(f \right) = E\left(L\left(Y - f(X) \right)\right)  $

%\textit{The generalization performance of a learning method relates to its prediction capability on independent test data. Assessment of this performance is extremely important in practice, since it guides the choice of learning method or model, and gives us a measure of the quality of the ultimately chosen model.}



\textbf{Cross Validation}:  
\textit{simplest and most widely used method... This method directly estimates the expected extra-sample error
$Error = E[L(Y,\hat{f}(X))] $ i.e. the average the average generalization error when the method $\hat{f(X)}$ is applied to an independent test sample from the joint distribution of $X$ and $Y$ . As mentioned earlier, we might hope that cross-validation estimates the conditional error, with the training set. $\mathrm{T}$ held fixed. But cross-validation typically estimates well only the expected prediction error.}

\textit{Ideally, if we had enough data, we would set aside a validation set and use it to assess the performance of our prediction model. Since data are often scarce, this is usually not possible. To finesse the problem, K-fold cross-validation uses part of the available data to fit the model, and a different part to test it. We split the data into K roughly equal-sized parts; }

\textit{Let $k : \{1,..,N\} \mapsto \{1, .., K\}$ be an indexing function are more details be an indexing
function that indicates the partition to which observation i is allocated by the randomization. Denote by}


\subsection{Bengio, GrandValet - No Unbiased Estimator of the Variance of K-Fold Cross-Validation}
\textbf{Cross Validation}: 
\textit{The standard measure of accuracy for trained models is the prediction error (PE), i.e. the expected loss on future examples. Learning algorithms themselves are often compared on their average performance, which estimates expected value of prediction error (EPE) over training sets.
The hold-out technique does not account for the variance with respect to the training set, and may thus be considered inappropriate for the purpose of algorithm comparison [4]. Moreover, it makes an inefficient use of data which forbids its application to small sample sizes. In this situation, one resorts to computer intensive resampling methods such as cross-validation or bootstrap to estimate PE or EPE. We focus here on K-fold cross-validation. While it is known that cross-validation provides an unbiased estimate of EPE, it is also known that its variance may be very large.
Some distribution-free bounds on the deviations of cross-validation are available, but they are specific to locally defined classifiers, such as nearest neighbors.
We focus on the standard K-fold cross-validation procedure, with no overlap between test sets: each example is used once and only once as a test example.
}




\textbf{HyperParameters}:
Como van apareciendo en algunos algoritmos y are different from the "parameters" or coefficients of the learners. They appear as a consequence of numerical, computational and sometimes statistical fine-tuning of algorithms (give an example?). 
La relacion entre cross-validation y la busqueda de hiper parametros. 

\textit{}

\textit{}


