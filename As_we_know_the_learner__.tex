As we know, the learner will be fit to best approximate the target with $y \approx \hat{y} = h\left(\sum_{j}\theta_j x^j\right)$. A first approach led to find the hyperplane that best separates the two classes by estimating parameter $\hat{\theta}$. Given that we have built a problem with $p$ degrees of freedom, we re left to find the parameter. To do this we must give an optimizing criteria to minimize. This choice will certainly depend on the way we want to decide that one parameter is better than another. 

For example the choice of $0.5$ as a threshold in \ref{formula:logitThreshold} is ad-hoc and certainly one which we could try to fit in the optimization process. In the context of machine learning, the functions that define a quantitative measure of a parameter's performance are called \textit{loss functions}.
%the criteria used to choose the best parameters

A naive approach to find the parameters in our problem would be to minimize the residual sum of squares. Typical of other scenarios such as linear regression, one would like to minimize the following sum:  


\begin{equation} \label{eq:rss}
RSS(\theta_0,..,\theta_p)  = \sum_{i=1}^n [y_i - \hat{y}_i]^2  \\
=  \sum_{i=1}^n [y_i - h( \theta \cdot x_i)]^2
\end{equation}

The equation reflects our goal to correctly match a training sample with their targets. Naturally those parameters that give bigger differences between the predictions and the target values will give a higher value to $RSS$.

Our main interest will be in having a generalized model, one which can make a \textit{good} prediction on any sample, even new ones. And simply minimizing the previous equation could be a bad attempt to generalize the classification model constructed from the data. The idea is that the learner is to fit the actual dataset but we wouldn't have a measure of how it would've performed with new, unseen samples of the \textit{true} distribution for $\mathrm{T}$. 

The prediction error is a measure that tries to characterize the generalization aspect of a learner, before actually gathering new data.

\begin{definition}{Prediction Error}
	Given a choice of parameter $\theta$, the \textbf{prediction error} for the resulting classifier $f_\theta$ is
	
	\[
	Err_{true}(\theta)  = \Expect_{ \mathrm{T}}[(\textbf{y} - h(\textbf{x} \cdot \theta) )^2] \\
	= \int (y - h(x \cdot \theta) )^2 P(x,y)dxdy
	\]
\end{definition}

Note that integral is done over the joint distribution of inputs and outputs. In practical cases, we will only have incomplete information on $P(x,y)$ given the finite data sample.

We must assume then that calculating this integral is not feasible for any $\theta$ and must look upon ways of estimating this error.

For example, given a parameter $\theta$ we could use our sample of $n$ iid points from $P(x,y)$ to approximate the integral by a Monte Carlo scheme such as 

\begin{equation} \label{eq:mcarlo-approx}
Err_{true}(\theta)  \approx \frac{1}{n} \sum_i^n ( y - h(x \cdot \theta) )^2
\end{equation}

This equation closely resembles \ref{eq:rss} but the difference lies in that the sampling process should be repeated for each specific $\theta$. Again, we have only a limited amount of samples to do this.

As a first limitation, our models will be learning to optimize on a reduced dataset from the \textit{true} distribution. The dataset we will call the \textit{training} set and the error $Err_{train}$ will be the learner's prediction error on this specific set.


%In classification problems, the residual sum of squares is not the loss function used to estimate the model\'s parameters. Instead, they rely on other \textit{loss} functions that we will introduce later.

Our clss of learners will be any function of the form $f: X \rightarrow Y$ mapping feature space to the target space. For now, assume that $y  =  f(x)  +  \epsilon $ is a good relationship for our data, where $\epsilon \sim \calN(0,1) $ then the equation in \ref{eq:rss} can be read as the parameter error given the training set.

With learner's of this form, our interest is now in minimizing the training error as an approximation of the true prediction error. This is:
\[
Err_{train}(\theta) \approx \Expect_{ \mathrm{T}}[(y - h(x \cdot \theta) )^2]
\]

%, and can be decomposed into two types of errors
