
As we know, the learner will approximate the target with $y \approx \hat{y} = h\left(\sum_{j}\theta_jx_j\right)$ and the first idea leads to find the hyperplane that best separates the two classes by estimating parameter $\hat{\theta}$. Given that we have now a problem of $p$ degrees of freedom, what is left is to find the parameter by optimizing a certain criteria. This choice will certainly depend on the way we decide that one parameter is better than another. For example the choice of $0.5$ as a threshold in \ref{formula:logitThreshold} is ad-hoc and certainly one which we could try to fit in the optimization process. In the context of machine learning the criteria used to choose the best parameters is called a \textit{metric}.

A naive approach to find the parameters in our problem would be to minimize the residual sum of squares. Typical of other scenarios such as linear regression, one would like to minimize the residual sum of squares (RSS)  

\begin{equation} \label{eq:rss}
RSS(\theta_0,..,\theta_p)  = \sum_{i=1}^n [y^i - \hat{y}^i]^2  \\
=  \sum_{i=1}^n [y^i - h( \theta \cdot x^i)]^2
\end{equation}

The equation reflects our goal to correctly classify all training samples with their targets. But in truth we are interested in having a generalized model, one which can make a \textit{good} prediction on any sample, even new ones. The previous equation is a bad attempt to generalize the classification model to any sample of the \textit{true} distribution for \mathrm{T}. 

Given a choice of parameter \theta, the \textbf{prediction error} for the resulting classifier $f_\theta$ is

\[
    Err_{true}(\theta)  = \Expect_{ \mathrm{T}}[(\textbf{y} - h(\textbf{x} \cdot \theta) )^2] \\
    = \int (y - h(x \cdot \theta) )^2 p(x,y)dxdy
\]

In practical cases though, this integral is not solvable for $\theta$. Sampling $M$ iid points from $p(x,y)$ and approximating by a Monte Carlo scheme such as 

\begin{equation} \label{eq:mcarlo-approx}
    Err_{true}(\theta)  \approx \frac{1}{M} \sum_i^M (y - h(x \cdot \theta) )^2
\end{equation}

would be unfeasible as well, since the sampling process should be done for each specific $\theta$. Notice however that this last equation \ref{eq:mcarlo-approx} resembles the form in \ref{eq:rss}. However, using the equation from the residual sum of squares to 


virtually  incomplete information on $p(x,y)$ it is then assumed that 

If we assume that $y  =  f(x)  +  \epsilon $ is a good relationship for our data, where $\epsilon \sim \calN(0,1) $ then the equation in \ref{eq:rss} can be read as the parameter error given the training set.

Our interest should then focus on minimizing the error
\[
Err_{train}(\theta) \approx \frac{1}{n} \Expect_{ \mathrm{T}}[(y - h(x \cdot \theta) )^2]
\]



%, and can be decomposed into two types of errors