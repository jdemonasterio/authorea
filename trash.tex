%
% AGRANDA 2016 Camera ready version!
%

\documentclass{article}%{llncs}

\usepackage{graphicx}
\usepackage[space]{grffile}
\usepackage{latexsym}
\usepackage{textcomp}
\usepackage{longtable}
\usepackage{booktabs}
% \usepackage{row}
\usepackage{amsfonts,amsmath,amssymb} %paquetes de matematica
\usepackage{amsthm}
\usepackage{url}
% \usepackage{hyperref}d
% \hypersetup{colorlinks=false,pdfborder={0 0 0}}
% You can conditionalize code for latexml or normal latex using this.
% \newif\iflatexml\latexmlfalse
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{algorithm2e}
\usepackage{qtree}
\usepackage{amssymb}
\usepackage{hyperref}

% \setcounter{tocdepth}{3}
%\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{nicefrac}

% for confusion matrix building
\usepackage{array}
\usepackage{multirow}

\newcommand\MyBox[2]{
\fbox{\lower0.75cm
	\vbox to 1.7cm{\vfil
		\hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
		\vfil}%
}%
}


% mathcal certain letters
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calL}{\mathcal{L}}

% \renewcommand{\labelitemi}{$\bullet$}
\setlength{\tabcolsep}{6pt}

%comandos de operadores de esperanza, varianza etc.
%\newcommand{\Expect}{{\rm I\kern-.3em E}}
\newcommand{\Expect}{{\mathbb{E}}}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}} 

%comandos de teoremas, corolarios, lemmas, pruebas y definiciones
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[subsection]
\theoremstyle{definition}


\begin{document}

% \mainmatter  % start of an individual contribution
% first the title is needed
\title{Trash Tex file to test different formulas and stuff}




\author{
	Juan de Monasterio
	\and Carlos Sarraute
}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Uncovering the Diffusion of an Infectious Disease with Mobile Phone Data
%%Unveiling Chagas with Big Data}

\maketitle
\begin{abstract}
	
	Texstudio is better at helping to write fast tex documents using spell-checkers, pre-loaded commands and \textit{stuff}. All of the following text is just copy/paste trash being tested before being added to a real doc.
	
%%%% HOW TO USE SPLITS IN EQUATIONS (same level for equal signs)
%\begin{split}
%	var(mr) & =  \Expect_{\Theta, \Theta'} 
%	\left[ 
%	cov_{\textbf{x},\textbf{y}}
%	(rmg(\Theta,\textbf{x},\textbf{y} )rmg(\Theta',\textbf{x},\textbf{y} )) 
%	\right]
%\end{split}

%%%% Equations with no \$ chars
%\[
%l(\theta) = \sum_{i=1}^N \big(y_i log(P(y_i|x_i,\theta)) + (1-y_i)log(1 - P(y_i|x_i,\theta)) \big)
%\]

%%%% Equations with no \$ chars
%\begin{equation}
%l(\theta) = \sum_{i=1}^N \big(y_i log(P(y_i|x_i,\theta)) + (1-y_i)log(1 - P(y_i|x_i,\theta)) \big)
%\end{equation}

%\begin{definition}{Vapnikâ€“Chervonenkis (VC) Dimension}
%\end{definition}
\end{abstract} 

\section{Brief Technical Notes on Machine Learning Applications}\label{section-technicalObservations}
Note from own experience  in data processing and machine learning with Python.

Built from experience, might be helpful for others trying to deal with the same problems. Not intended to be a reference manual or a 

Data storage: easiest data format is in `csv` (comma separated values) which is a universal, very general and straight-forward format for data storage. Very few previous knowledge is needed to read this format. The downside is that it's also very inefficient. Even when compressed, it can be orders of magnitude bigger and slower to read than other formats.

Specific data storage types and database engines exist to cope with this problem. Storing and reding data will be quicker but significant overhead can be added to the project itself since building and mantaining is a more complex, though not very difficult, task. The same applies for the person trying to read the data. Most databases allow querying of the data in an SQL-type query which is a specific language synthaxed to express relational algebra operations. Examples of this type might include MySQL\footnote{ Oracle Coproration, \url{http://dev.mysql.com/doc/refman/5.7/en/}} , PostgresSQL \footnote{ PostgreSQL Global Development Group, \url{https://www.postgresql.org/}} or MongoDB \footnote{ MongoDB Inc., \url{https://www.mongodb.com/}}.

2TB of csv.gz data split in 24 month daily call records

Recommended websites:

\url{stackoverflow.com}

\url{https://www.kaggle.com/} 

\url{http://austinrochford.com/posts.html}

\url{http://stats.stackexchange.com/}

Python as a general programming language. Mostly for scripting, data preprocessing and data visualization. 

Working with Python provides a direct integration to a very wide array of software solutions. Its community is very active and has ready-to-use applications in a variety of problems. This will allow us to work with tools which are both flexible and/or powerful. Ranging from graph visualization to computer cluster connections or parallelization of implementations of machine learning algorithms.

In our experience, this language is the most complete and it has a very high-level interperetation of its synthax. This becomes a gradual learning curve for anyone directly

jupyter notebooks
Flexible and powerful for research, collaboration and 
\url{https://github.com/jdemonasterio/authorea/blob/master/Notebooks/small_tests.ipynb}

A good starting point for a working python workframe would be to use anaconda 
\url{https://docs.continuum.io/}
And for an on-point working tutorial (any working environment)
\url{http://opentechschool.github.io/python-data-intro/core/notebook.html}



Online courses:
\url{https://www.coursera.org/specializations/data-science-python}
\url{https://www.edx.org/course/introduction-python-data-science-microsoft-dat208x-3} 




Recommended modules:

Numpy for linear algebra operations, routines and data structures.
scipy for stats.
Pandas for data loading, data processing, data wrangling, data management, relational algebra  operations (joins, group bys, aggregations, apply a function to each cell value), data transformation. 

Mostly in dataframe format which is a matrix where columns need not be of the same data type i.e. string vs. numerical vs. dictionary, etc.

Sframes for resilient disk-distributed dataframes where more data can be processed than by the limit which the computer supports. Prepared to load huge data files of various formats. Uses its own disk data format which allows to load and save data with near real-time speed, when compared to files which are plain comma-separated values.

Hashing trick to split the data.


lti-processing libraries.

xgboost

Theano for neural network algorithms mostly.


Python, sklearn, pandas, graphlab, etc
python, jupyter, sklearn, pandas, sframes, graphlab, spark mllib, theano, tensor flow, xgboost

Data process raw data by reading in chunks from huge files (compressed filesizes amount to 1TB), applying filters like modulus 10.

Dealing with the computational issues of algorithms and data processing. When computed, algorithms need to take into account things such as such as memory and/or disk size, parallelization, multi-core, linear algebra routines.

In general, algorithms will load all data in RAM and execute optimization routines. 

For example if $K$Folds is used, in most cases we will be running multiple  learning routines simultaneously in each fold group and keep the "best" scores at the end.

Joblib, sklearn and Graphlab are all Python modules



\end{document}

