%
% AGRANDA 2016 Camera ready version!
%

\documentclass{article}%{llncs}

\usepackage{graphicx}
\usepackage[space]{grffile}
\usepackage{latexsym}
\usepackage{textcomp}
\usepackage{longtable}
\usepackage{booktabs}
% \usepackage{row}
\usepackage{amsfonts,amsmath,amssymb} %paquetes de matematica
\usepackage{amsthm}
\usepackage{url}
% \usepackage{hyperref}d
% \hypersetup{colorlinks=false,pdfborder={0 0 0}}
% You can conditionalize code for latexml or normal latex using this.
% \newif\iflatexml\latexmlfalse
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{algorithm2e}
\usepackage{qtree}
\usepackage{amssymb}
\usepackage{hyperref}

% \setcounter{tocdepth}{3}
%\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{nicefrac}

% for confusion matrix building
\usepackage{array}
\usepackage{multirow}

\newcommand\MyBox[2]{
\fbox{\lower0.75cm
	\vbox to 1.7cm{\vfil
		\hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
		\vfil}%
}%
}


% mathcal certain letters
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calL}{\mathcal{L}}

% \renewcommand{\labelitemi}{$\bullet$}
\setlength{\tabcolsep}{6pt}

%comandos de operadores de esperanza, varianza etc.
%\newcommand{\Expect}{{\rm I\kern-.3em E}}
\newcommand{\Expect}{{\mathbb{E}}}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}} 

%comandos de teoremas, corolarios, lemmas, pruebas y definiciones
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[subsection]
\theoremstyle{definition}


\begin{document}

% \mainmatter  % start of an individual contribution
% first the title is needed
\title{Trash Tex file to test different formulas and stuff}




\author{
	Juan de Monasterio
	\and Carlos Sarraute
}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Uncovering the Diffusion of an Infectious Disease with Mobile Phone Data
%%Unveiling Chagas with Big Data}

\maketitle
\begin{abstract}
	
	Texstudio is better at helping to write fast tex documents using spell-checkers, pre-loaded commands and \textit{stuff}. All of the following text is just copy/paste trash being tested before being added to a real doc.
	
%%%% HOW TO USE SPLITS IN EQUATIONS (same level for equal signs)
%\begin{split}
%	var(mr) & =  \Expect_{\Theta, \Theta'} 
%	\left[ 
%	cov_{\textbf{x},\textbf{y}}
%	(rmg(\Theta,\textbf{x},\textbf{y} )rmg(\Theta',\textbf{x},\textbf{y} )) 
%	\right]
%\end{split}

%%%% Equations with no \$ chars
%\[
%l(\theta) = \sum_{i=1}^N \big(y_i log(P(y_i|x_i,\theta)) + (1-y_i)log(1 - P(y_i|x_i,\theta)) \big)
%\]

%%%% Equations with no \$ chars
%\begin{equation}
%l(\theta) = \sum_{i=1}^N \big(y_i log(P(y_i|x_i,\theta)) + (1-y_i)log(1 - P(y_i|x_i,\theta)) \big)
%\end{equation}

%\begin{definition}{Vapnikâ€“Chervonenkis (VC) Dimension}
%\end{definition}
\end{abstract} 


As we know, the learner will be fit to best approximate the target with $y \approx \hat{y} = h\left(\sum_{j}\theta_j x^j\right)$. A first approach led to find the hyperplane that best separates the two classes by estimating parameter $\hat{\theta}$. Given that we have built a problem with $p$ degrees of freedom, we re left to find the parameter. To do this we must give an optimizing criteria to minimize. This choice will certainly depend on the way we want to decide that one parameter is better than another. 

For example the choice of $0.5$ as a threshold in \ref{formula:logitThreshold} is ad-hoc and certainly one which we could try to fit in the optimization process. In the context of machine learning, the functions that define a quantitative measure of a parameter's performance are called \textit{loss functions}.
%the criteria used to choose the best parameters

A naive approach to find the parameters in our problem would be to minimize the residual sum of squares. Typical of other scenarios such as linear regression, one would like to minimize the following sum:  


\begin{equation} \label{eq:rss}
RSS(\theta_0,..,\theta_p)  = \sum_{i=1}^n [y_i - \hat{y}_i]^2  \\
=  \sum_{i=1}^n [y_i - h( \theta \cdot x_i)]^2
\end{equation}

The equation reflects our goal to correctly match a training sample with their targets. Naturally those parameters that give bigger differences between the predictions and the target values will give a higher value to $RSS$.

Our main interest will be in having a generalized model, one which can make a \textit{good} prediction on any sample, even new ones. And simply minimizing the previous equation could be a bad attempt to generalize the classification model constructed from the data. The idea is that the learner is to fit the actual dataset but we wouldn't have a measure of how it would've performed with new, unseen samples of the \textit{true} distribution for $\mathrm{T}$. 

The prediction error is a measure that tries to characterize the generalization aspect of a learner, before actually gathering new data.

\begin{definition}{Prediction Error}
	Given a choice of parameter $\theta$, the \textbf{prediction error} for the resulting classifier $f_\theta$ is
	
	\[
	Err_{true}(\theta)  = \Expect_{ \mathrm{T}}[(\textbf{y} - h(\textbf{x} \cdot \theta) )^2] \\
	= \int (y - h(x \cdot \theta) )^2 P(x,y)dxdy
	\]
\end{definition}

Note that integral is done over the joint distribution of inputs and outputs. In practical cases, we will only have incomplete information on $P(x,y)$ given the finite data sample.

We must assume then that calculating this integral is not feasible for any $\theta$ and must look upon ways of estimating this error.

For example, given a parameter $\theta$ we could use our sample of $n$ iid points from $P(x,y)$ to approximate the integral by a Monte Carlo scheme such as 

\begin{equation} \label{eq:mcarlo-approx}
Err_{true}(\theta)  \approx \frac{1}{n} \sum_i^n ( y - h(x \cdot \theta) )^2
\end{equation}

This equation closely resembles \ref{eq:rss} but the difference lies in that the sampling process should be repeated for each specific $\theta$. Again, we have only a limited amount of samples to do this.

As a first limitation, our models will be learning to optimize on a reduced dataset from the \textit{true} distribution. The dataset we will call the \textit{training} set and the error $Err_{train}$ will be the learner's prediction error on this specific set.


%In classification problems, the residual sum of squares is not the loss function used to estimate the model\'s parameters. Instead, they rely on other \textit{loss} functions that we will introduce later.

Our clss of learners will be any function of the form $f: X \rightarrow Y$ mapping feature space to the target space. For now, assume that $y  =  f(x)  +  \epsilon $ is a good relationship for our data, where $\epsilon \sim \calN(0,1) $ then the equation in \ref{eq:rss} can be read as the parameter error given the training set.

With learner's of this form, our interest is now in minimizing the training error as an approximation of the true prediction error. This is:
\[
Err_{train}(\theta) \approx \Expect_{ \mathrm{T}}[(y - h(x \cdot \theta) )^2]
\]


%, and can be decomposed into two types of errors

\end{document}

