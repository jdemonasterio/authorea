

%
% AGRANDA 2016 Camera ready version!
%

\documentclass{article}%{llncs}

\usepackage{graphicx}
\usepackage[space]{grffile}
\usepackage{latexsym}
\usepackage{textcomp}
\usepackage{longtable}
\usepackage{booktabs}
% \usepackage{row}
\usepackage{amsfonts,amsmath,amssymb} %paquetes de matematica
\usepackage{amsthm}
\usepackage{url}
% \usepackage{hyperref}d
% \hypersetup{colorlinks=false,pdfborder={0 0 0}}
% You can conditionalize code for latexml or normal latex using this.
% \newif\iflatexml\latexmlfalse
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{algorithm2e}
\usepackage{qtree}
\usepackage{amssymb}
\usepackage{hyperref}

% \setcounter{tocdepth}{3}
%\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{nicefrac}

% for confusion matrix building
\usepackage{array}
\usepackage{multirow}

\newcommand\MyBox[2]{
\fbox{\lower0.75cm
	\vbox to 1.7cm{\vfil
		\hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
		\vfil}%
}%
}


% mathcal certain letters
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calL}{\mathcal{L}}

% \renewcommand{\labelitemi}{$\bullet$}
\setlength{\tabcolsep}{6pt}

%comandos de operadores de esperanza, varianza etc.
%\newcommand{\Expect}{{\rm I\kern-.3em E}}
\newcommand{\Expect}{{\mathbb{E}}}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}} 

%comandos de teoremas, corolarios, lemmas, pruebas y definiciones
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[subsection]
\theoremstyle{definition}

\begin{document}

% \mainmatter  % start of an individual contribution

% first the title is needed
\title{Trash Tex file to test different formulas and stuff}




\author{
	Juan de Monasterio
	\and Carlos Sarraute
}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Uncovering the Diffusion of an Infectious Disease with Mobile Phone Data
%%Unveiling Chagas with Big Data}

\maketitle
\begin{abstract}
	
	Texstudio is better at helping to write fast tex documents using spell-checkers, pre-loaded commands and \textit{stuff}. All of the following text is just copy/paste trash being tested before being added to a real doc.
	
%%%% HOW TO USE SPLITS IN EQUATIONS (same level for equal signs)
%\begin{split}
%	var(mr) & =  \Expect_{\Theta, \Theta'} 
%	\left[ 
%	cov_{\textbf{x},\textbf{y}}
%	(rmg(\Theta,\textbf{x},\textbf{y} )rmg(\Theta',\textbf{x},\textbf{y} )) 
%	\right]
%\end{split}

%%%% Equations with no \$ chars
%\[
%l(\theta) = \sum_{i=1}^N \big(y_i log(P(y_i|x_i,\theta)) + (1-y_i)log(1 - P(y_i|x_i,\theta)) \big)
%\]

%%%% Equations with no \$ chars
%\begin{equation}
%l(\theta) = \sum_{i=1}^N \big(y_i log(P(y_i|x_i,\theta)) + (1-y_i)log(1 - P(y_i|x_i,\theta)) \big)
%\end{equation}

%\begin{definition}{Vapnikâ€“Chervonenkis (VC) Dimension}
%\end{definition}
	
	
\end{abstract} 

\section{Classifier : Naive Bayes}

The Naive Bayes model encompasses a group of simple and computationally efficient algorithms which assume a strong independence relationship among the features. Even though this assumption is in practice wrong, the model still achieves acceptable classification rates for some problems. Alos, it does not suffer in problems of high-dimensionality, where $p >> n$.  

It is presented here mostly for benchmarking purposes, where in practice the classification rate achieved by this model serves as a baseline for other, more complex, models. With a linear complexity in the number of features and samples, the algorithm can be extended to \textit{bigger} problem implementations. Maximum-likelihood estimation of the parameters has a closed form solution which is better over other iterative methods such as gradient descent techniques.

Let $x = (x_1,...,x_p)$ be any given data sample and $C_k$ be one of $K$ possible output classes of the problem. We take $p(C_k \mid x)$  to be the  class posterior probability of this class given the sample. 

In section \ref{section-example}
we have used 
\[
p(C_k| x) = \frac{P(x|C_k)P(C_k)}{P(x)}
\]\label{equation-posteriorProbabilties}

and argued that if our data is given, then our model can only improve the posterior probability by optimizing $P(x|C_k)P(C_k)$ which is just the joint probability of the sample and the class. We can then approximate the posterior as

\[
P(C_k \mid x) \approx p(C_k) * \prod_{j=1}^{p}    P(x_j \mid \bigcap_{k=j+1}^{p} x_k \cap C_k)
\]\label{equation-posteriorProbabilityDecomposition1}

We now assume independence between features, given the target class, to let the conditional probabilities factors become the probability of each feature. %This assumption is what gives the model its 

This yields a posterior probability which depends only on the prior probability and on the likelihood of that feature.

\[
P(C_k \mid x) \approx p(C_k) * \prod_{j=1}^{p}    P(x_j | C_k)
\]\label{equation-posteriorProbabilityDecomposition2}

As we have said before, the parameters of the model can only reweigh the likelihood factors, so if we look to maximize the posterior probability, our final estimate of the posterior will take the following form.

\[
P(C_k \mid x) = \frac{1}{Z} p(C_k) * \prod_{j=1}^{p}    P(x_j | C_k)
\]\label{equation-posteriorProbabilityDecomposition3}

where $Z = p(x)$ is the scaling factor in the equation and is already fixed by the dataset.

In practice the model will stem into different algorithms where each variant will have a different assumption on the likelihoods $p(x_j \mid C_k)$ of the model and on the priors $p(C_k)$. We can treat both of decisions as our model's hyper-paramters.

For the former assumption, it is common to chose among using a nonparametric density estimations from the data or assuming that the data comes from an exponential family distribution such as a Gaussian, Bernoulli or Multinomial distributions. Different choices will certainly lead to different cross validation scores among problems.

For the latter assumption, the most usual options include chosing equiprobable class priors or by using average estimates from the dataset.

Finally, the output class for a given sample will be given by taking the class $k'$ which maximizes the probability  $P(C_k' \mid x)$. 

\textit{}

\textit{}

\textit{}


\end{document}

