%
%

\documentclass{article}%{llncs}

\usepackage{graphicx}
\usepackage[space]{grffile}
\usepackage{latexsym}
\usepackage{textcomp}
\usepackage{longtable}
% \usepackage{row}
\usepackage{amsfonts,amsmath,amssymb} %paquetes de matematica
\usepackage{amsthm}
\usepackage{url}
% \usepackage{hyperref}d
% \hypersetup{colorlinks=false,pdfborder={0 0 0}}
% You can conditionalize code for latexml or normal latex using this.
% \newif\iflatexml\latexmlfalse
\usepackage{lmodern}                      % Use Latin Modern fonts
\usepackage[T1]{fontenc}        % Better output when a diacritic/accent is used
\usepackage[utf8]{inputenc}
\usepackage[
backend=biber,
sortlocale=us_EN,
natbib=true,
url=false, 
doi=true,
eprint=false
]{biblatex}

%\usepackage[english]{babel}
\usepackage{algorithm2e}
\usepackage{qtree}
\usepackage{amssymb}
\usepackage{hyperref}

% \setcounter{tocdepth}{3}
%\usepackage{graphicx}
% \usepackage{subfigure}


%----------------+
% Table packages |
%----------------+

\usepackage{array}          % Flexible column formatting
% \usepackage{spreadtab}  % Spreadsheet features
\usepackage{multirow}       % Allows table cells that span more than one row
\usepackage{booktabs}       % Enhance quality of tables
\setlength{\heavyrulewidth}{1pt}

% for confusion matrix building
\usepackage{array}
% to sidewaysfigure rotate vertically or horizontally
\usepackage{rotating}
% rotates tables
\usepackage{pdflscape}
% Table colors
\usepackage[table,x11names]{xcolor}


% \newcommand\MyBox[2]{
% \fbox{\lower0.75cm
% 	\vbox to 1.7cm{\vfil
% 		\hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
% 		\vfil}%
% }%
% }

% to build master table
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

% mathcal certain letters
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calL}{\mathcal{L}}

% \renewcommand{\labelitemi}{$\bullet$}
\setlength{\tabcolsep}{6pt}

%comandos de operadores de esperanza, varianza etc.
%\newcommand{\Expect}{{\rm I\kern-.3em E}}
\newcommand{\Expect}{{\mathbb{E}}}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}} 

%comandos de teoremas, corolarios, lemmas, pruebas y definiciones
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[subsection]
\theoremstyle{definition}


\begin{document}

% \mainmatter  % start of an individual contribution
% first the title is needed
\title{Trash Tex file to test different formulas and stuff}




\author{
	Juan de Monasterio
	\and Carlos Sarraute
}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Uncovering the Diffusion of an Infectious Disease with Mobile Phone Data
%%Unveiling Chagas with Big Data}

\maketitle
\begin{abstract}
	Your text should have one idea per paragraph and use transitions between sentendes. The following linnk provides a whole set of transition words to use.
	\url{http://grammar.ccc.commnet.edu/Grammar/transitions.htm}
	
	
%%%% HOW TO USE SPLITS IN EQUATIONS (same level for equal signs)
%\begin{split}
%	var(mr) & =  \Expect_{\Theta, \Theta'} 
%	\left[ 
%	cov_{\textbf{x},\textbf{y}}
%	(rmg(\Theta,\textbf{x},\textbf{y} )rmg(\Theta',\textbf{x},\textbf{y} )) 
%	\right]
%\end{split}

%%%% Equations with no \$ chars
%\[
%l(\theta) = \sum_{i=1}^N \big(y_i log(P(y_i|x_i,\theta)) + (1-y_i)log(1 - P(y_i|x_i,\theta)) \big)
%\]

%%%% Equations with no \$ chars
%\begin{equation}
%l(\theta) = \sum_{i=1}^N \big(y_i log(P(y_i|x_i,\theta)) + (1-y_i)log(1 - P(y_i|x_i,\theta)) \big)
%\end{equation}

\end{abstract} 

%\begin{tabular}{|*{16}{c|}}  % repeats {c|} 18 times
%	\hline
%	\multicolumn{4}{|c}{Problem1} & \multicolumn{4}{|c|}{Problem 2} \\ \hline 
%%	& \multicolumn{4}{|c|}{Problem 3} & \multicolumn{4}{|c|}{Problem 4} \\ \hline
%	\multicolumn{1}{|c}{$Accuracy$} & \multicolumn{1}{|c|}{$AUC$} & \multicolumn{1}{|c|}{$F1$} & \multicolumn{1}{c|}{$Runtime$} & 
%	\multicolumn{1}{|c}{$Accuracy$} & \multicolumn{1}{|c|}{$AUC$} & \multicolumn{1}{|c|}{$F1$} & \multicolumn{1}{c|}{$Runtime$}
%	\\ \hline 
%	1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\ \hline
%	
%%	& SVD & CJ & HT & SVD & CJ & HT & SVD &CJ \\ \hline
%%	& & & & & & & & & & & & & & & & &  \\ \hline
%\end{tabular}


\bigskip

% rotates tables
%\usepackage{pdflscape}
%\usepackage{booktabs}
%\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
%
%\begin{landscape}% Landscape page
%		\begin{table*}
%			\centering
%			\ra{1.3}
%			\begin{tabular}{@{}rrrrcrrrrcrrcrr@{}} \toprule
%				&  \multicolumn{4}{c}{Problem 1} &  \multicolumn{4}{c}{Problem 2} & \multicolumn{4}{c}{Problem 3}  & \multicolumn{4}{c}{Problem 4}\\
%				 \cmidrule{2-5} \cmidrule{6-9} \cmidrule{10-13} \cmidrule{14-17}
%				& $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ \\
%				\midrule
%				$Naive Bayes$               & 0.84  & 0.82  & 0.75  & 2  && 0.64  & 0.61  & 0.31  & 2  && 0.65   & 0.63  & 0.45  & 1   && 0.85  & 0.76  & 0.62 & 1 \\
%				$Logistic \ Regression$     & 0.893 & 0.857 & 0.9   & 96 && 0.714 & 0.726 & 0.248 & 119 && 0.705 & 0.754 & 0.307 & 115 && 0.883 & 0.85  & 0.331 & 106 \\
%				$Random \ Forest$            & 0.878 & 0.857 & 0.9  & 33 && 0.79  & 0.776 & 0.278 & 45  && 0.792 & 0.845 & 0.346 & 21  && 0.898 & 0.853 & 0.393 & 19\\
%				$Gradient \ Tree \ Boosting$ & 0.974 & 0.978 & 0.952 & 41 && 0.838 & 0.819 & 0.291 & 54  && 0.811 & 0.855 & 0.359 & 33  && 0.885 & 0.873 & 0.384 & 47 \\
%				\bottomrule
%			\end{tabular}
%			\caption{Caption}
%		\end{table*}	
%\end{landscape}


%\begin{landscape}% Landscape page
%	\begin{table*}
%		\centering
%		\ra{1.3}
%		\begin{tabular}{@{}rrrrcrrrrcrrcrr@{}} \toprule
%			&  \multicolumn{4}{c}{Problem 1} &  \multicolumn{4}{c}{Problem 2} & \multicolumn{4}{c}{Problem 3}  & \multicolumn{4}{c}{Problem 4}\\
%			\cmidrule{2-5} \cmidrule{6-9} \cmidrule{10-13} \cmidrule{14-17}
%			& $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ \\
%			\midrule
%			$Naive Bayes$               & 0.84  & 0.82  & 0.75  & 2  && 0.64  & 0.61  & 0.31  & 2  && 0.65   & 0.63  & 0.45  & 1   && 0.85  & 0.76  & 0.62 & 1 \\
%			$Logistic \ Regression$     & 0.893 & 0.857 & 0.9   & 96 && 0.714 & 0.726 & 0.248 & 119 && 0.705 & 0.754 & 0.307 & 115 && 0.883 & 0.85  & 0.331 & 106 \\
%			$Random \ Forest$            & 0.878 & 0.857 & 0.9  & 33 && 0.79  & 0.776 & 0.278 & 45  && 0.792 & 0.845 & 0.346 & 21  && 0.898 & 0.853 & 0.393 & 19\\
%			$Gradient \ Tree \ Boosting$ & 0.974 & 0.978 & 0.952 & 41 && 0.838 & 0.819 & 0.291 & 54  && 0.811 & 0.855 & 0.359 & 33  && 0.885 & 0.873 & 0.384 & 47 \\
%			\bottomrule
%		\end{tabular}
%		\caption{Caption}
%	\end{table*}
%\end{landscape}
%



\begin{landscape}% Landscape page
			% RAW NON TABLE SYNTHAXED VALUES
%			$Naive Bayes$               & 0.84  & 0.82  & 0.75  & 2  && 0.64  & 0.61  & 0.31  & 2  && 0.65   & 0.63  & 0.45  & 1   && 0.85  & 0.76  & 0.62 & 1 \\
			% $Logistic \ Regression$     & 0.893 & 0.857 & 0.9   & 96 && 0.714 & 0.726 & 0.248 & 119 && 0.705 & 0.754 & 0.307 & 115 && 0.883 & 0.85  & 0.331 & 106 \\
			% $Random \ Forest$            & 0.878 & 0.857 & 0.9  & 33 && 0.79  & 0.776 & 0.278 & 45  && 0.792 & 0.845 & 0.346 & 21  && 0.898 & 0.853 & 0.393 & 19\\
			% $Gradient \ Tree \ Boosting$ & 0.974 & 0.978 & 0.952 & 41 && 0.838 & 0.819 & 0.291 & 54  && 0.811 & 0.855 & 0.359 & 33  && 0.885 & 0.873 & 0.384 & 47 \\

	\begin{table*}\centering
	\hskip-4.0cm\caption{Master results table comparing results for all of the classifiers run in this work.
			For each task and classifer, we show the its $Accuracy$, $ROC AUC$ and $F1$ test-set scores, along with the runtimes of a full cross validation procedures on the learner.}\label{tab:master_table_results}
		\ra{1.3}
		\hskip-4.0cm\begin{tabular}{@{}rrrrrcrrrr@{}} \toprule	
			&  \multicolumn{4}{c}{Problem 1} &  \phantom{abc} & \multicolumn{4}{c}{Problem 2} \\
			\cmidrule{2-5} \cmidrule{7-10} 
			& $Accuracy$ & $ROC AUC$ & $F1$ & $Runtime$ $$ && $Accuracy$ & $ROC AUC$ & $F1$ & $Runtime$ \\ \midrule
			$Naive Bayes$               & 0.84  & 0.82  & 0.75  & 2  && 0.64  & 0.61  & 0.31  & 2   \\
			$Logistic \ Regression$     & 0.893 & 0.857 & 0.9   & 96 && 0.714 & 0.726 & 0.248 & 119 \\			
			$Random \ Forest$            & 0.878 & 0.857 & 0.9  & 33 && 0.79  & 0.776 & 0.278 & 45  \\
			$Gradient \ Tree \ Boosting$ & 0.974 & 0.978 & 0.952 & 41 && 0.838 & 0.819 & 0.291 & 54 \\

			\bottomrule
		\end{tabular}
	\end{table*}
	\addtocounter{table}{-1} % This dirty hack will fake both tables as if they were the same one.

	\begin{table*}\centering
      \hskip-4.0cm\caption{Master resutls table comparing results for all of the classifiers run in this work.
			For each task and classifer, we show the its $Accuracy$, $ROC AUC$ and $F1$ test-set scores, along with the runtimes of a full cross validation procedures on the learner.}
		\ra{1.3}
		\hskip-4.0cm\begin{tabular}{@{}rrrrrcrrrr@{}} \toprule	
			&  \multicolumn{4}{c}{Problem 3} &  \phantom{abc} & \multicolumn{4}{c}{Problem 4} \\
			\cmidrule{2-5} \cmidrule{7-10} %\cmidrule{10-12}
			& $Accuracy$ & $ROC AUC$ & $F1$ & $Runtime$ $$ && $Accuracy$ & $ROC AUC$ & $F1$ & $Runtime$ \\ \midrule
			$Naive Bayes$                & 0.65  & 0.63  & 0.45  & 1   && 0.85  & 0.76  & 0.62 & 1 \\
			$Logistic \ Regression$      & 0.705 & 0.754 & 0.307 & 115 && 0.883 & 0.85  & 0.331 & 106 \\			
			$Random \ Forest$            & 0.792 & 0.845 & 0.346 & 21  && 0.898 & 0.853 & 0.393 & 19\\
			$Gradient \ Tree \ Boosting$ & 0.811 & 0.855 & 0.359 & 33  && 0.885 & 0.873 & 0.384 & 47 \\
			\bottomrule
		\end{tabular}
	\end{table*}
	
\end{landscape}

	
\bigskip




\begin{landscape}% Landscape page
	%\usepackage{booktabs}
	%\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
	\begin{table*}\centering
		\ra{1.3}
		\begin{tabular}{@{}rrrrcrrrcrrr@{}} \toprule
			&  \multicolumn{3}{c}{$w = 8$} &  \phantom{abc} & \multicolumn{3}{c}{$w = 16$} & \phantom{abc} & \multicolumn{3}{c}{$w = 32$}\\
			\cmidrule{2-4} \cmidrule{6-8} \cmidrule{10-12} 
			& $t=0$ & $t=1$ & $t=2$ && $t=0$ & $t=1$ & $t=2$ && $t=0$ & $t=1$ & $t=2$\\ \midrule
			$dir=1$\\
			$c$ & 0.0790 & 0.1692 & 0.2945 && 0.3670 & 0.7187 & 3.1815 && -1.0032 & -1.7104 & -21.7969\\
			$c$ & -0.8651& 50.0476& 5.9384 && -9.0714& 297.0923& 46.2143&& 4.3590& 34.5809& 76.9167\\
			$c$ & 124.2756& -50.9612&-14.2721&& 128.2265& -630.5455& -381.0930&& -121.0518& -137.1210& -220.2500\\
			$dir=0$\\
			$c$ & 0.0357& 1.2473& 0.2119&& 0.3593& -0.2755& 2.1764&& -1.2998& -3.8202& -1.2784\\
			$c$ & -17.9048& -37.1111& 8.8591&& -30.7381& -9.5952& -3.0000&& -11.1631& -5.7108& -15.6728\\
			$c$ & 105.5518& 232.1160& -94.7351&& 100.2497& 141.2778& -259.7326&& 52.5745& 10.1098& -140.2130\\
			\bottomrule
		\end{tabular}
		\caption{Master cross table on all best-fit learners for all four migration problems}
	\end{table*}
	
	
\end{landscape}



\bigskip


	\begin{table*}\centering
		%\hskip-4.0cm
		\caption{Master results table comparing results for all of the classifiers run in this work.
			For each task and classifer, we show the its $Accuracy$, $ROC AUC$ and $F1$ test-set scores, along with the runtimes of a full cross validation procedures on the learner.}\label{tab:master_table_results}
		\ra{1.3}
		%\hskip-4.0cm
		\begin{tabular}{@{}rrrrrc@{}} \toprule
			&  \multicolumn{4}{c}{Problem 1} \\
			\cmidrule{2-5} 
			& $Accuracy$ & $ROC AUC$ & $F1$ & $Runtime  (m)$ \\ \midrule
			$Naive Bayes$               & 0.84  & 0.82  & 0.75  & 2  \\
			$Logistic \ Regression$     & 0.893 & 0.857 & 0.9   & 96\\
			$Random \ Forest$            & 0.878 & 0.857 & 0.9  & 33 \\
			$Gradient \ Tree \ Boosting$ & 0.974 & 0.978 & 0.952 & 41  \\
			
			\bottomrule
		\end{tabular}
	\end{table*}

\end{document}
