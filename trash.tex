
%
% AGRANDA 2016 Camera ready version!
%

\documentclass{article}%{llncs}

\usepackage{graphicx}
\usepackage[space]{grffile}
\usepackage{latexsym}
\usepackage{textcomp}
\usepackage{longtable}
\usepackage{booktabs}
% \usepackage{row}
\usepackage{amsfonts,amsmath,amssymb} %paquetes de matematica
\usepackage{amsthm}
\usepackage{url}
% \usepackage{hyperref}d
% \hypersetup{colorlinks=false,pdfborder={0 0 0}}
% You can conditionalize code for latexml or normal latex using this.
% \newif\iflatexml\latexmlfalse
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{algorithm2e}
\usepackage{qtree}
\usepackage{amssymb}
\usepackage{hyperref}

% \setcounter{tocdepth}{3}
%\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{nicefrac}

% for confusion matrix building
\usepackage{array}
\usepackage{multirow}

\newcommand\MyBox[2]{
\fbox{\lower0.75cm
	\vbox to 1.7cm{\vfil
		\hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
		\vfil}%
}%
}


% mathcal certain letters
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calL}{\mathcal{L}}

% \renewcommand{\labelitemi}{$\bullet$}
\setlength{\tabcolsep}{6pt}

%comandos de operadores de esperanza, varianza etc.
%\newcommand{\Expect}{{\rm I\kern-.3em E}}
\newcommand{\Expect}{{\mathbb{E}}}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}} 

%comandos de teoremas, corolarios, lemmas, pruebas y definiciones
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[subsection]
\theoremstyle{definition}

\begin{document}

% \mainmatter  % start of an individual contribution

% first the title is needed
\title{Trash Tex file to test different formulas and stuff}




\author{
	Juan de Monasterio
	\and Carlos Sarraute
}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Uncovering the Diffusion of an Infectious Disease with Mobile Phone Data
%%Unveiling Chagas with Big Data}

\maketitle
\begin{abstract}
	
	Texstudio is better at helping to write fast tex documents using spell-checkers, pre-loaded commands and \textit{stuff}. All of the following text is just copy/paste trash being tested before being added to a real doc.
	
%%%% HOW TO USE SPLITS IN EQUATIONS (same level for equal signs)
%\begin{split}
%	var(mr) & =  \Expect_{\Theta, \Theta'} 
%	\left[ 
%	cov_{\textbf{x},\textbf{y}}
%	(rmg(\Theta,\textbf{x},\textbf{y} )rmg(\Theta',\textbf{x},\textbf{y} )) 
%	\right]
%\end{split}

%%%% Equations with no \$ chars
%\[
%l(\theta) = \sum_{i=1}^N \big(y_i log(P(y_i|x_i,\theta)) + (1-y_i)log(1 - P(y_i|x_i,\theta)) \big)
%\]

%%%% Equations with no \$ chars
%\begin{equation}
%l(\theta) = \sum_{i=1}^N \big(y_i log(P(y_i|x_i,\theta)) + (1-y_i)log(1 - P(y_i|x_i,\theta)) \big)
%\end{equation}

%\begin{definition}{Vapnikâ€“Chervonenkis (VC) Dimension}
%\end{definition}
	
	
\end{abstract} 

Notar que todo lo que esta en italica es tipo highlighter de textos de otros autores :)
%NOTAR QUE TO LO QUE ESTA EN ITALICA ES TIPO HIGHLIGHTER DE OTROS TEXTOS DE OTROS AUTORES}

\section{Classifier : Decision Tree Learners}

Decision Trees will not be described in detail in this work. Here we present only but a brief overview of this supervised learner used in regression and classification problems.

The models builds a tree in the graph-theory sense where each node has a rule based on set belonging. Each rule defines a linear (or similar) partition of input space and the rules that determine the partitions are built from the sample's features. At each node we would have a decision to include or exclude a sample $s$ from that partition by checking if $X_i(s) \in U$ where $X_i$ might be any given feature of the data and $U$ is a subset of said feature's space.

For numerical features, $U$ will be of the form $(-\infty,c]$ where $c \in \mathbb{R}$ is any given number predefined by each rule. For categorical features, $U$ will be a subset of possible the values of that feature. A tree defines a partition of feature space in disjoint regions $A_1,...,A_K$ such that the sample's predicted output $\hat{y}$ is $c_k$ if the sample belongs to $A_k$. Here $c_k$ is one of the possible values taken by the target variable $y$ in the training set. And by the way trees were built, each $A_k$ is a hyper-rectangles in feature space.

The learner can be characterized by 
\[
h(X) = \sum_{j=1}^J c_k I(X \in A_k)
\]\label{equation-decisionTreeModel}

where $c_k$ is the value that our model estimates forfor samples in the $A_k$ region. Both of these will have to be estimated by the model by minimizing its loss funciton.

The algorithm then needs to determine for the group of input samples that flows into a decision node, what is the best way to split them according, in order to optimize a loss metric. In this context, the criteria used to decide on node splits are called \textit{node impurity measures}. Most variations for this machine learning model build rules at each nodes in a greedy fashion, where node impurity measures are locally optimized at each node to decide what is the best splitting decision. This is done so because not doing so will result in an algorithm whose computational complexity is infeasible since the construction of optimal binary decision trees is NP-Complete \cite{decisionTreesNP}. The parameters for the tree will then be fit sequentially.

At any splitting node, we have to find the \textit{best} feature $X^p$ and split $t$ threshold for which to partition the data in
 $A_{left} = \{x \in \mathcal{T} \  / \ x^p \leq t \} $ and $A_{right} = \{x \in \mathcal{T}\  / \ x^p> t \} $.  To do so we optimize our misclassification loss at the split

\[
min_{p,t} \big[ min_{c_{left} }  \sum_{x \in A_{left}(p,t) } L(y,c_{left})          +   min_{c_{right}}  \sum_{x \in A_{right}(p,t) }  L(y,c_{right}) \big] 
\]\label{equation-decisionTreeGreedyOptimization}

where $y$ is the target associated to our sample $x$. Note that this can be done efficiently for a wide range of loss functions since the minimization can be done for each feature independently. 

A tree is then grown in an iterative way from the top down \footnote{In this context the \textit{top} of a tree refers to the root of the tree.}, estimating the appropriate parameters at each split. All of the training set's samples would start at the top (the root node) and then travel down through the trees branches, in accordance to their fulfillment or not of each node's split rule. The tree's leafs are the subsets that comprise the partition and once a learner is fit, predicting targets for new samples is straightforward: the prediction of their target class will be the output of traveling the sample down to the leaf node.  

To illustrate this method, an instance is show in figure \ref{rf-treeFigure}. This classification tree example is built for the two class problem of gender prediction using data from CDRs:
%[.{\textit{Woman}}]
\smallskip
\begin{figure}[h]\label{rf-treeFigure}
	\Tree[.{ $Calling\_Volume \leq 23$ } [.{$Province \in \{ San Luis, Chubut \} $} [.{$Time\_Weekend \geq 16$} [.{\textit{M}} ] [.{\textit{F}} ]  ]
	[.{$Calls\_Weekdays \leq 48$} 
	[.{ $Time\_Weekday \geq 17$} [.{\textit{M}} ] [.{\textit{F}} ]] [.{\textit{F}} ] ]  ]
	[.{$Calls\_Mondays \geq 2$} [.{$Province \in \{ Chubut, Cordoba \} $}  [.{\textit{M}} ] [.{\textit{F}} ] ]
	[.{\textit{M}}  ]]]
	
\end{figure}

\smallskip

%[.{\textit{M}} ] [.{\textit{F}} ]


The metrics used to build each rule score, for the resulting sets, are the \textit{Gini impurity measure} and the \textit{entropy} or \textit{information gain} criterion. The former optimizes for misclassification error in the resulting sets, if all samples were to be tagged with one predicted target-label.  The latter optimizes for information entropy, which is analogous to minimizing \textit{Kullback-Liebler divergence} of the resulting sets with respect to the original set previous to the split. This partitioning of the samples will continue iteratively until a predefined tuning parameter limit will stopthe optimization or iteration.


Hyper-parameters (tuning parameters) of tree models are set set before running the algorithm, also known as tuning parameters. In this algorithm examples of hyper-parameters of the model include the length of the tree,  the splitting rule threshold and the node impurity measures are also parameters of the model. 

From the previous algorithm descriptions we can list those directly:

\begin{itemize}
	\item Max depth of the tree, or the number of allowed level of splits.
	\item The criteria or measure used to select the best split feature at each node.
	\item The leaf size or the total number of minimum samples allowed per leaf. Note that this is a limit imposed on  branch depth. 
	\item Number of features selected to decide on the best split feature at each node.
\end{itemize}


Intuitively, it is natural to find that trees of longer depth will more easily overfit the data since more complex interactions among variables will be captured by refining the partition on input space. We consider this hyperparameter to be a measure of the model's complexity and describe the most common methods to control this dimension.

The idea is to grow a very large tree $T_0$ that will grow until it reaches a depth limit threshold that is very unrestrictive. Then the tree will be pruned by removing branches and nodes to remove model complexity and only slightly lose accuracy. 

Let $T \subset T_0$ be a subtree of the first tree, where $T$ is obtained by pruning $T_0$.  Here the partition regions $R_j$ will be associated to $T$'s terminal nodes or leafs, indexed by $j$, which $j \in \{1,...,|T|  \}$. 

We can define the following:





For a more complete explanation of a decision tree for classification or regression problems, please refer to \cite{breiman-cart84}.


\textit{}

\textit{}

\textit{}

\textit{}

\end{document}

