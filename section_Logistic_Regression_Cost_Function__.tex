\section{Logistic Regression Cost Function}

\begin{equation} \label{logit}
 \min_{\theta} \gamma\| \theta\|_{j}  + \sum_i=1^N log(e^{-y_i (X_i \cdot \theta + c )} +1) 
\end{equation}

Note that here there are two specific tuning parameters which must be predefined in the optimization procedure. Notably $\gamma$ which measure the weight on the regularization term and$j$ which determines the type of norm to measure the argument of the minimization.

The model outputs the predicted probabilities of the target variables belonging to the target class.

\textit{The conditional distribution of $y|x$ follows a Bernoulli distribution.}


\begin{definition}{Logistic Regression}
Given a choice of parameter \theta,
\[
    Err_{true}(\theta)  = \Expect_{ \mathrm{T}}[(\textbf{y} - h(\textbf{x} \cdot \theta) )^2] \\
    = \int (y - h(x \cdot \theta) )^2 p(x,y)dxdy
\]
\end{definition}

\textit{In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.}

\textit{The logistic regression model arises from the desire to model the posterior probabilities of the $K$ classes via linear functions in $x$, while at the same time ensuring that they sum to one and remain in [0,1]. The model has the form }


$$ log( \frac{P(C_i|x)}{P(C_j|x)}) = \beta_{i0}  + \beta_i^\intercal x  $$ for $i,j \in \{0,1\}, i\neq j$

With these same indices 

$$ P(C_i|x) = \frac{ exp(\beta_{i0}  + \beta_i^\intercal x)}{1 + exp(\beta_{j0}  + \beta_j^\intercal x)}   $$ for $i,j \in \{0,1\}, i\neq j$

The model is specified in terms of the log-odds for each class 


\textit{}
\textit{}
\textit{}