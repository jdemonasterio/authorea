\section{Logistic Regression Cost Function}

\begin{equation} \label{logit}
 \min_{\theta} \gamma\| \theta\|_{j}  + \sum_i=1^N log(e^{-y_i (X_i \cdot \theta + c )} +1) 
\end{equation}

Note that here there are two specific tuning parameters which must be predefined in the optimization procedure. Notably $\gamma$ which measure the weight on the regularization term and$j$ which determines the type of norm to measure the argument of the minimization.

The model outputs the predicted probabilities of the target variables belonging to the target class.

\textit{The conditional distribution of $y|x$ follows a Bernoulli distribution.}


\begin{definition}{Logistic Regression}
Given a choice of parameter \theta,
\[
    Err_{true}(\theta)  = \Expect_{ \mathrm{T}}[(\textbf{y} - h(\textbf{x} \cdot \theta) )^2] \\
    = \int (y - h(x \cdot \theta) )^2 p(x,y)dxdy
\]
\end{definition}

\textit{In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.}

\textit{The logistic regression model arises from the desire to model the posterior probabilities of the $K$ classes via linear functions in $x$, while at the same time ensuring that the posteriors sum to one and remain in [0,1]. The model has the form }


$$ log\big( \frac{P(C_i|x)}{P(C_j|x)}\big) = \theta_{i0}  + \theta_i^\intercal x  $$ for $i,j \in \{0,1\}, i\neq j$

With these same indices 

$$ P(C_i|x) = \frac{ exp(\theta_{i0}  + \theta_i^\intercal x)}{1 + exp(\theta_{j0}  + \theta_j^\intercal x)}   $$ 

The model is specified in terms of the log-odds for each class and is parameterized by $\theta$.

Maximum likelihood is the most common method used to fit the model, with multinomial distributions modelling the features. 


If we take into account that $P(y=1|x,\theta) = 1 - P(y=0|x,\theta)$ , then the estimation of $\theta$ for $N$ samples gives

l(\theta) = \sum_{i=1}^N \big(y_i log(P(y_i|x_i,\theta)) + (1-y_i)log(1 - P(y_i|x_i,\theta)) \big)



\textit{}
\textit{}
\textit{}