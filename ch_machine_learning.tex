%===============================================================================
%     File: ch_machine_learning.tex
%    Author: Juan de Monasterio
%    Created: 08 Feb 2017
%  Description: Chapter : Machine Learning
%===============================================================================
\chapter{Machine Learning}
\label{ch:machineLearning}


\section{Preamble}\label{section-preamble}
Most of the discussions in this chapter are based on information extracted from two distinguished textbooks: \textcite{bishop-patternRecognition} and \textcite{hastie-elemstatslearn}. Some fundamental concepts and material were also used from \label{scikit-learn}. While other texts were also used, it is not necessary that we point them out here as they have been correctly cited. Where not specified, the reader can assume that the proof is based from said texts.

\section{ Supervised classification problems}\label{section-supervised-learning}

Machine Learning is divided into two main categories: Supervised and Unsupervised Learning. Let $Y \in \mathbb{R}^n$ be the outputs of the model and $X \in \mathbb{R}^{n \times p}$ be the inputs: supervised learning algorithms produce outputs from input data i.e. for each instance $n$, the computer has access to examples of outputs and tries to reproduce them based on information contained in $X$. In this context, the algorithm is generally referred to as a \textit{learner}.

The second class of problems is where the output $Y$ is missing altogether from the data. In this scenario the most common objectives are clustering samples, density estimation and data compression. Linear regression and K-means clustering are examples of algorithms in each of these categories respectively.

In supervised learning tasks can be sub-categorized by the nature of the problem. When the type of the output variable $Y$ takes a (generally \textit{small} )discrete set of values, then it is said that it is a supervised \textit{classification} problem. On the other hand, when the output takes continuous (or dense in an open set of $\mathbb{R}$) range of \textit{quantitative} values, the problem is said to be of supervised \textit{regression}. Note that regression problems can be encoded into classification problems by grouping the output values into categories by taking ranges of output values.

Suppose our aim is to predict $y$ given a new sample $x$. In supervised regression problems, $y$ will comprise a continuous variable. On the other hand for classification problems $y$ will represent a label for a certain class. For the case of $K$ classes, $y$ most commonly takes values in the ranges $0$ through $K-1$ or $1$ through $K$. In both cases, the joint probability distribution $p(X, Y)$, called the \textit{true} distribution, gives all the information we need on these two variables. But the values of this probability is most often unknown. The idea is to then user estimations and inference on the most likely values for new samples and take decisions with the information at hand. These decisions will be based on the most probabilistic characterization of the data we have. In this work we will focus on the classification aspect of machine learning.

In these type of problems the theoretical and the computational aspects are both of interest. The algorithms used need to consider the technical requirements of the software and hardware at hand, as well as the time or resource constraints imposed by the problem setting. As such, they are expected to be executed in a reasonable amount of time, imposed by the task specification, and limited by the computing power available. \footnote{Here the word \textit{reasonable} is used in a broad sense. It will depend entirely on time constraints, computational capacity, usage and other aspects of each learning application.} There can be problems which require that the algorithm output predictions in \textit{real-time}, to the resolution of milliseconds. Picture a system where a credit-card transaction needs to be approved or labeled as a fraud. Here the system needs to respond in short time if the transaction is fraudulent.

Other use cases might require the system to process a big volume of data at once, not a single event, but a batch of these and output this answer. The system in use needs to be prepared to run \textit{lean} with a big inflow of data, without overloading the hardware capacity. These examples show that for a given problem, multiple algorithms are available to use but while all of them are theoretically doing the same task, we must also consider the practical advantages of these. Computational efficiency and \textit{scalability} are relevant when working with these problems. Even though we won't delve into these aspects in this work, it has an important consideration in the application of Machine Learning solutions.


In its essence, a machine learning method is a probabilistic model built from data so it is very similar to a statistical model. However, it differs specially in that its focus is generally on the models' predictive abilities more than in the model's parameters estimates. ~\textcite{breiman-statisticalmodeling} The algorithms will be built and used for a given phenomena, to try and replicate it as best as possible without really identifying the true nature of the mechanisms behind this phenomena. As such, most applications will try to \textit{imitate} the task's behavior rather than try to identify the real system behind it.
%on this matter It also puts a great amount of weight on efficiency and computational scalability.

%At this point it is important to start noting

These subtle differences in the machine learning approach of a problem is also reflected in the terminology used by the field. We know that different disciplines often speak about the same machine learning methods in different terms and this difference is most notable with classic statistics. Where we can, we will be identifying these differences along the text. As a start, the \textit{dependent} $Y$ is called the target or label and the \textit{independent} variables, \textit{covariates} or \textit{input variables} are named \textit{features} in this case.
%Labels that are representing categorical or discrete variables are also named factors or \textit{qualitative} variables.


\subsection{A working example}\label{section-example}

In this work we will be talking of the training set and the test set, noted by $\mathrm{T}$  and $\mathrm{Ts}$ as our two data sources.The objective is to build a probabilistic model which has the capacity to correctly predict the class instances of new data objects, such as those from $\mathrm{Ts}$, based on having seen information of objects from $\mathrm{T}$.

Concretely, let's consider a reduced training dataset built from Call Detail Records (CDRs) where samples are calls being made by users who can belong to any of the following provinces : \textit{Buenos Aires}, \textit{Cordoba} and \textit{Santa Fe}.
%\url{http://stackoverflow.com/}
%\footnote{For more information on this set, a historic review is given at \url{https://en.wikipedia.org/wiki/Iris_flower_data_set}}.
Five measurements were taken on all of the observations to account for the user's number of calls and total minute duration of calls. All data was extracted from a week of logging measurements. Below we print a short overview of this dataset:

\begin{table}[ht]
\caption{{Head of the CDR dataset}}
\label{tab:sample_CDR}
\centering
\begin{tabular}{ l l l l l l }
\toprule
User & CallsWeekend & TimeWeekend & CallsWeekDays & TimeWeekday & Province \\
\midrule
BA343E & 15 & 89 & 8 & 24 & \textit{Santa Fe}\\
73F169 & 10 & 121 & 2 & 98 & \textit{Cordoba} \\
EA23AD & 12 & 43 & 5 & 154 & \textit{Buenos Aires} \\
\bottomrule
\end{tabular}
\end{table}

In this form a row is representing the available data acquired from each user and the columns represent different types of measurements or information on that user. In general, most machine learning problems will be associated with a training set $\mathrm{T}$ of similar form as the one shown before. Where rows represent objects or \textit{samples} and columns are measurements or \textit{features} of our samples.

Both $\mathrm{T}$ and $\mathrm{Ts}$ will take the form of a paired couple of datasets $(X,Y)$ where $X \in \mathbb{R}^{n \times p}$ and $Y \in \mathbb{R}^n $. The difference will lie in the way each set is used to computationally build a probabilistic model. In essence the former is used to algorithmically create a function which maps inputs to outputs,  whilst the latter will be used to measure how \textit{well} this mapping would behave in real situations, given a way of measuring this performance.

More specifically, we note that the $jth$ column of $X$ or equivalently the $jth$ feature of the dataset is denoted by $X^j$. In a similar way, the $ith$ row or sample of $X$ is denoted by $X_i$ or $x$ when it is clear from context or when we are referring to a generic sample. A similar notation is used with the outputs, where $Y_i$ or $y$ will be used to denote the target of a specific sample depending on context.

In this particular example, even though the last column of the dataset in \cref{tab:sample_CDR} is not a measurement \textit{per se}, it provides information on each user's province of residence. With this we can map users to \textit{classes} which are elements of the set of provinces.

Hereon, there are various questions one could try to answer using this dataset. Examples of problems that a machine learning algorithm could tackle could be:

\begin{itemize}
\item Predict a user's province when given information on only the first four features.
\item Predict a user's number of calls made on weekends when given information on the last four features.
\item Give an estimate of the probability density function of user's calls duration, during weekdays.
\end{itemize}

The first two problems are examples of supervised learning. For each, the $Y$ variables or \textit{labels} are respectively the last and first features (columns in the dataset). Even more, the first problem is a classification one since users are to be categorized according to one of the three possible provinces, whilst the second one is a regression problem for which the output could be any of a range of numerical values. The labels in classification problems are numerically encoded with a finite range of numbers where $\{0,1\}$ or $\{-1,1\}$ are usually used in the binary class problem.

Note that we are not making any assumptions on the data which we take as given. %to us in this way.
This is common in machine learning applications and because of this, algorithms tend to be designed to account for this lack of context. The type of problems and questions that could be done, then depend entirely on the data.

From the list before, the last example belongs to the unsupervised learning category where there is no need to have a label on the data. A priori, the data doesnt' give any examples of this. Here the question is on the structure of a specific column, namely the estimation of the true probability distribution of the calling time. As such, there is no output expected from new data. For the most of this work, we will be talking about supervised classification scenario.

To further illustrate this scenario, a brief notation outline is described below. In this example, we follow a standard logistic regression classifier:

Let's suppose that we want to build a predictive model to determine the origin province of a user. A classification algorithm should assign samples to provinces. Let $\{C_1,..,C_k\}$ denote the set of possible target classes. Our aim is to maximize the probability of belonging to a certain class, given the phone data:
\begin{equation}
P(C_k| X) = \frac{P(x|C_k)P(C_k)}{P(x)} 
\end{equation}

In this interpretation, $P(C_k)$ is known as the prior probability of belonging to that certain class and $P(C_k|x)$ as the posterior probability, given the sample data.

Our classifiers will partition the input space into decision regions $R_k$ for which a class $C_k$ is uniquely associated to a region. It makes sense to try and \textit{minimize} the chances of assigning samples to incorrect regions. For example, take a problem with $K$ classes. Given a sample $x_i$, the probability $P$ of a correct classification,with probability density function $p$, is measured by

\begin{equation}\label{eq:goodclassification-equation}
\begin{split}
= & \sum_{k=1}^{K} P(x_i \in R_k, C_k ) \\
= & \sum_{k=1}^{K} \int_{R_k}p(x_i,C_k) dx \\
= & \sum_{k=1}^{K} \int_{R_k}p(C_k \mid x_i) p(x_i) dx
\end{split}
\end{equation}

Observe that the measure is completely characterized by the posterior probabilities. The factor $p(x)$ is common to all integrals so we only need to maximize the posteriors. And even when $p(x)$ might not be known or accurately estimated, the algorithm can only modify the decision regions $R_k$. This will have a direct effect on the probability of correctly classifying samples. Thus the goal of our algorithm will be to choose the best possible regions for the problem.

In our example, a reduced problem instance would be to determine if a user belongs to the province of \textit{Cordoba} vs the rest of the provinces. This brings us to a binary classification problem since there are two possible output classes. %us to a two class learning problem.

As a simple solution to predict the target variable, we could build a learner $f$ from a transformed linear combination of the input features. In the ideal case we would have that for every sample $y \approx f(x) = h\left(\sum_{j}\theta^jx^j\right)$ \label{formula:1} where $\theta$ is unknown and generally referred to as the coefficient or parameter \footnote{In formula \cref{formula:1} we have omitted the intercept parameter , generally noted as $\theta_0$. The reason for this is that one can include this parameter implicitly if we allow for a synthetic feature $X^0$ in $X$ which is a vector with all its components equal to 1. }. Here the model is said to be \textit{linear} because it is linear in the input space $X$.

If we let our decision boundary be $ t_i = \theta \cdot x_i \ \forall \ i $,in the ideal case we will have that $\exists \theta s.t. \forall i $

\begin{equation}
t_i
\begin{cases}
&>0 \ \mbox{if} \ y_i=1 \\
&<0 \ \mbox{if} \ y_i=0.
\end{cases}
\end{equation}
%    \forall 1 \leq i \leq n = y^i $

As an example, we could take the heavyside step function as our transformation $h(z)$ where

\begin{equation}
h(z) =
\begin{cases}
&0 \ \mbox{if} \ z<0 \\
&\frac{1}{2} \ \mbox{if} \ z=0 \\
&1 \ \mbox{if} \ z>0.
\end{cases}
\end{equation}

If the goal of the model is to maximize $P(Y_i = y_i | X_i = x) \ \forall i$
under the assumption that $\nexists\ i \ / \ t_i = 0$, the algorithm would then correctly classify all samples to their targets. Yet this situation is hardly the case. The common approach is then to rely on optimization procedures to minimize the amount of misclassification given by our algorithm. This would be analogous to maximizing the probability of a correct classification in \cref{eq:goodclassification-equation}.

For analytical convenience, a more tractable function for this task is the logistic which is an approximation of the heavyside step function. Here

\begin{equation} \label{eq:logisticFunction}
\sigma(z) = \frac{e^{z}}{1 + e^{z}} = \frac{1}{1 + e^{-z}}
\end{equation}

where we use the relation

\begin{equation}
 \ H(z) = \lim_{k \to \infty} \left(\frac{1}{2} + \frac{1}{2}tanh(kz) \right) = \lim_{k \to \infty} \left(\frac{1}{1+e^{-kz}} \right)
\end{equation}

The logistic function has the advantage of being smooth, well defined for all real numbers and with its image in $(0,1)$. This property lends itself to reading outputs as probabilities of targets belonging to a certain class.

 Its derivative can be put in terms of the logistic function itself, where

\begin{equation} \label{eq:derivativeLogisticFunction}
\sigma '(a) = \sigma(a)( 1 - \sigma(a) )
\end{equation}

It is also bijective, with the inverse given by the \textit{logit} function

\begin{equation} \label{eq:logitFunction}
\sigma^{-1}(z) = log( \frac{z}{1 - z})
\end{equation}

For the scenario characterized in \cref{formula:1} we would have to finally assign each output to a specific class. A common approach for this is to categorize each output whether $\hat{y} > 0.5$ \label{formula:logitThreshold}. Notice that having $h(x \cdot \theta) = 0.5$ implies that $x \cdot \theta = 0$ and thus our classifier is separating samples in feature space (the space of the inputs) with the hyper-plane defined by the parameter $\theta$. Having a higher $\hat{y}$ for a given sample implies that it is further away from the hyper-plane. The same goes for low estimated targets. If we were to read this as a probability, we can interpret that the algorithm is modeling the posterior probability $P(Y_i = y | X_i = x)$ and would correspond to having a higher confidence in the classification.

%This means that

%It also happens to be an approximating function to the


\section{Logistic Regression for Classification}\label{section-logisticRegression}

This model is named as a \textit{regression} model but is actually used for \textit{classification} and this convention has been made this way for historical reasons.


Let $\{C_1,..,C_k\}$ denote the set of possible target classes. Recall that in a classification problem we are interested in maximizing a sample's probability of belonging to a certain class, given the input data:

\begin{equation}
P(C_k| x) = \frac{P(x|C_k)P(C_k)}{P(x)} 
\end{equation}

As a simple case, in the two-class problem we have that


\begin{equation}
\begin{split}
P(C_1| x) & = \frac{P(x|C_1) }{P(x|C_1)p(C_1) + P(x|C_2)p(C_2)} \\
& = \frac{1 }
{1 + \exp(- \log( \frac{ P(x|C_1)p(C_1)}
{P(x|C_2)p(C_2)
}))
}
\end{split}
\end{equation}

Here we see the close resemblance to the logistic function which is acting on the \textit{log-odds ratio} $ \log( \frac{ P(x|C_1)p(C_1)}{P(x|C_2)p(C_2) })$. This equivalent form of the posterior distribution of one class with regards to the log-odds one property defining the model.


%\begin{definition}{Logistic Regression}
%xGiven a choice of parameter $\theta$,
%\end{definition}

The second defining property of logistic regression is that the log-odds are modeled with a transformation on a linear combination of inputs. It also conditions the posterior probabilities must sum to one. Let $j$ be a fixed class and denote

\begin{equation}\label{logit-logOddss}
 log\big( \frac{P(C_i|x)}{P(C_j|x)}\big) = \theta_{i0} + \theta_i^\intercal \cdot x 
 \end{equation}  for $i,j \in \{1,2,...,K\}, i\neq j$

With these same indices we have that

\begin{equation} P(C_i|x) = \frac{ exp(\theta_{i0} + \theta_i^\intercal x)}{1 + exp(\theta_{j0} + \theta_j^\intercal x)} 
\end{equation}

In this way the model is specified in terms of the log-odds for each class with respect to the base class $j$ and the model will be parameterized by $\theta$.

The model then sets the target as a Bernoulli random variable when conditioned on the input variables. Formally we have that,

\begin{equation}
\begin{split}
Y_i \mid X_i \ \sim & \operatorname{Bernoulli}(p_i) \\
\mathbb{E}[Y_i \mid X_i ] = & p_i
\end{split}
\end{equation}


The probability function of the target given the features $\Pr(Y_i=y\mid X_i)$ is given by
\begin{equation}\label{logit-probabilityDensity}
\Pr(Y_i=y \mid X_i = x_i) = p_i \ I(y=0) + (1-p_i) \ I(y=1) = p_i^{y} (1-p_i)^{(1-y)}
\end{equation}

where this depends on the class dependence of $y$.

Here the logit function is utilized to map log odds into conditional probabilities and the model outputs the predicted probabilities of the target variables belonging to a certain class.

This is why we are specifying a model where the target is a linear function of the inputs, corrected by an error term. Our model will then be characterized by:
\begin{equation}\label{logit-indicatorFunction}
Y_i = I(\theta_0 + \theta \cdot X_i + \epsilon > \frac{1}{2}) \ \forall i
\end{equation}. 

where $\epsilon$ is the error of the approximation and is distributed with the standard logistic distribution. %and \dfrac{num}{den}parameter $p_i$.

If we take the conditional probability in \cref{logit-indicatorFunction}, given the features we will have that
\begin{equation}
logit(p_i)= ln(\frac{p_i}{1-p_i}) = logit\big( \Expect[Y_i| X_i] \big) = \theta_0 + \theta \cdot X_i
\end{equation}

By an abuse of notation, we absorb $\theta_0$ into $X_i$. This can be safely done if we consider a dummy attribute $X_i^0$ which is always equal to $1$ for every sample $i$. Then from the equation above we have that

\begin{equation}
p_i = \sigma(\theta \cdot X_i) = \frac{exp(\theta \cdot X_i) }{1 + exp(\theta \cdot X_i)}
\end{equation}

Finally, using this $p_i$ representation and plugging it into the initial conditional probability of $Y_i$ in \cref{logit-probabilityDensity} we will have

\begin{equation}
 \Pr(Y_i=y \mid X_i = x_i) = p_i^{y} (1-p_i)^{(1-y)} = \frac{exp(\theta \cdot X_i) }{1 + exp(\theta \cdot X_i)}
 \end{equation}


Maximum likelihood is the most common method employed to fit the model. %with multinomial distributions modeling the features.
Given a parameter $\theta$, the probability of having a target vector $y$ is

\begin{equation}
P(Y =y \mid \theta ) = \prod_{i=1}^N P(y_1 \in C_1 \mid x_i, \theta)^y_i(1 - P(y_1 \in C_1 \mid x_i, \theta) )^{(1-y_i)}
\end{equation}

If we take into account that $P(y=1 \mid x,\theta) = 1 - P(y=0 \mid x,\theta)$ and the logistic function's derivative form \cref{eq:logitFunction}, then the estimation of $\theta$ for $N$ samples gives the following loss function

\begin{equation}
l(\theta) = \sum_{i=1}^N \big(y_i log(P(y_i \mid x_i,\theta)) + (1-y_i)log(1 - P(y_i \mid x_i,\theta) ) \big)
\end{equation}

Note the advantage that the loss function is concave in the parameters. Another strength of this model is that closed forms can be given for the gradient and the Hessian of the loss function with regard to the whole dataset $X$ as a sum involving individual samples $x$. The negative gradient can be expressed in the following way: %can thus be analytically expressed

\begin{equation}
- \nabla l(\theta) = \sum_{i=1}^N (y_i - P(y_i \mid x_i,\theta))\cdot x_i = \textbf{X}^{\intercal}(\textbf{y}-\textbf{p})
\end{equation}

whilst the Hessian takes the following form:

\begin{equation}
\frac{\partial^2 l(\theta)}{\partial \theta \partial \theta^\intercal} = \sum_{i=1}^N x_i \cdot x_i^\intercal P(y_i \mid x_i,\theta)(1 -P(y_i \mid x_i,\theta))
\end{equation}

From the derivations above, we can now firt $\theta$ using an optimization procedure that would take advantage of having the closed-form representations of the first and second derivatives of the maximum likellihood function. 


\section{Model Regularization and Hyper-parameters} \label{section-hyperParametersRegularization}


\begin{definition}{Model Regularization}

Regularization of models is the process in which restrictions and conditions are imposed on the model's function $f$ through a functional $ R(f)$. The regularization term is used in the optimization procedure through the loss function.

\end{definition}

Regularization is widely used in problems which are ill-posed either because the model is biased or overfit. In general, restrictions are subject to reduce the number of predictors used in the model. This \textit{shrinkage} of parameters can be forced by penalizing large values for $\theta$ either by the number of non-null components of this parameter or by measuring its distance to zero. By so doing, we hope that only the most relevant features are selected in the model. As we will see in \cref{section-VcDimension}, a shorter model has the benefit of being more accurate in the estimation of the prediction error. In other cases, the benefit of a reduced model is that the variance of the model is reduced, only with a slight increase in bias. A slightly more \textit{heuristic} argument for model parsimony is that as such, all models will approximate nature up to a certain level; and if two models have the same predictive power, the simpler model should be used.


There is a number of different functionals $R(\cdot)$ used to impose conditions on the model. The most common ones include introducing a penalty to the size of $\theta$ by measuring a given norm $\| \theta \|$. Most implementations of the algorithms use $l1$ and $l2$ norms.

Each has its advantages and the details exceed the nature of this work. As a summary, benefits are related to the robustness of the solution, convexity of the minimization function, unique minimum and model parsimony. The former case is known as \textit{ Lasso} regularization and the latter is known as \textit{ridge} or \textit{Tikhonov} regularization. Other variants include the \textit{elastic-net} penalty which is a weighted average of the previous two norms, or the $l0$ norm, which is the number of non null components of $\theta$.


and taking a weighted average of two different penalizations. T

Even though some regularization methods have closed-form solutions, in practice most of them are found by iterative optimization routines such as gradient descent.

The following example shows the logistic regression's model loss function with an $l2$ regularization term on $\theta$:

\begin{equation} \label{logitRegularization}
\begin{split}
\min_{\theta} & \sum_{i=1}^N \big(y_i ( \theta \cdot x_i ) + log(1 + e^{\theta \cdot x_i} ) \big) + \lambda \| \theta\|_{2}^2 \\
\min_{\theta} & \sum_{i=1}^N \big(y_i ( \theta \cdot x_i ) + log(1 + e^{\theta \cdot x_i} ) \big) + \lambda \sum_{j=1}^P \theta_j^2
\end{split}
\end{equation}

%\min_{\theta} & \sum_{i=1}^N L(f(X_i,\theta),Y_i)  + \lambda\| \theta\|_{2}^2 \\
%\min_{\theta} & \sum_{i=1}^N L(f(X_i,\theta),Y_i)  + \lambda \sum_{j=1}^{P} \theta_j^2

%\begin{equation} \label{logit-optimization}
%
%\end{equation}


The value $\lambda$ acts here as the weight that our optimization procedure will put to the regularization part of the minimization function and $P$ is the number of features used in the model. Note that in an abuse of notation, the functional takes the form $R(f) = \lambda\| \theta\|_{2}^2$ where the model is identified by its $\theta$ parameter.

In this work, we will be only considering model regularization with lasso, ridge, or elastic-net.

Note that in the model built from equation \cref{logitRegularization} there are two specific parameters which must be predefined before starting the optimization procedure. Notably $\lambda$ and the $l2$ norm used to measure the size of $\theta$. These values are deemed to be \textit{hyper-parameters} of the models since they are not directly part of the theoretical model. Also, some authors in the literature can refer to the \textit{hyper-parameters} of the model as \textit{tuning parameters}.

The hyper-parameters are the values set to configure the different possible variants in the loss function and in the model's training phase. They are values that directly affect the estimated fit $f_{\hat{\theta}}$ but are not the $\theta$ parameter themselves. They need to be instantiated before training the learner and as such, they can't be learned from the dataset.

Note that in a Bayesian context the definition of hyper-parameter is different. These appear in the prior distributions of the model's parameters and should not be equated. In this case, the parameter's of $\theta$'s distribution are called the hyper-parameters.


%\textit{}


As we know, the learner will approximate the target with $y \approx \hat{y} = h\left(\sum_{j}\theta_j x^j\right)$. The first idea led to finding the hyperplane that best separates the two classes by estimating parameter $\hat{\theta}$. Given that we now have a problem of $p$ degrees of freedom, what is left is to find the parameter by optimizing a certain criteria. This choice will certainly depend on the way we decide that one parameter is preferable than another. For example the choice of $0.5$ as a threshold in \cref{formula:logitThreshold} is ad-hoc and certainly one which we could try to fit in the optimization process. In the context of machine learning, functions that define a quantitative measure of a parameter's performance are called \textit{loss functions}.
%the criteria used to choose the best parameters

A naive approach to find the parameters in our problem would be to minimize the residual sum of squares. This is also known as the squared loss function which is defined as $L(z,w) = \left\Vert z-w \right\Vert^2_2$. Historically this function was favored over other functions because it is more anlytically tractable. 

In the context of linear regression with Gaussian residuals, minimizing the residual sum of squares is equal to maximizing the likelihood probability of the target, given the input data. This results in minimizing the following sum (RSS):
%Typical of other scenarios such as linear regression, one would like

\begin{equation} \label{eq:rss}
RSS(\theta_0,..,\theta_p) = \sum_{i=1}^n [y_i - \hat{y}_i]^2 \\
= \sum_{i=1}^n [y_i - h( \theta \cdot x_i)]^2
\end{equation}

The equation reflects our goal to correctly match a training sample with their targets (also known as labels). Nonetheless, we are interested in having a generalized model, one which can make a \textit{good} prediction on any sample, even new ones from the true distrubtion of the data. The preceding equation is a bad attempt to generalize the classification model constructed from the data. It is built to fit the actual dataset, but we wouldn't expect it to perform well with any other given sample of the \textit{true} distribution for $\mathrm{T}$.

