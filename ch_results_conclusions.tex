%!TEX root = main.tex

%===============================================================================
%     File: ch4_evaluation_results.tex
%    Author: Juan de Monasterio
%    Created: 15 Feb 2017
%  Description: Chapter: Evaluation and Results
%===============================================================================

\chapter{Summary of Results and Conclusion}\label{ch:results_conclusion}

\todo{Erase this section}
\section{Plan / Random notes to add to results chapter}

\begin{description}
    \item Confirmation of decision tree overfitting on model complexity. Random Forests improving on this. Gradient Boosting better in bias with a slight incrase in overfit.
    \item Suprising low F1 overall when best-fitting, yet high and acceptable ROC AUC.\ Models seem to overvalue recall with a huge loss in precision. This would be interesting to weight under the eyes of a health researcher. Where focus might be either on one over the other. \todo{ask Diego Weinberg}.
    \item Other scoring metrics ended in satisfactory performing models which could accurately with maximum test scores over 81\%.
    \item Naive Bayes fast run time on the whole dataset. Yet with a modest performance which was first ranked on the $F1$ score for difficult problems (2 and 3).
    \item [DONE] Feature importance. Mobility and phone usage were, surprisingly, relevant attributes in most of the experiments run.
    \item [DONE] The map attributes were confirmed to be relevant by the best-feature ranker for random forests/ gradient tree boosting algorithms.
    \item Using best features from random forests in gradient boosting models incorporates some overfitting in the model since we use target information to select the best features. However, independent experiments confirm the performance of gradient tree boosting methods.
    \item As expected, we had strong past to present correlation in endemic home antennas. We found other strong variables interacting with the condition of being a past endemic user too.
    \item What's the predictive performance of CDRs for the long-term migrations problem?
    \item Review 4 problems and wrap the best results of each algorithm, what scorings did they have?


\end{description}


\todo{Review whole results chapter, fix typos grammar mistakes, etc.}

% handling data, models and features with acumen
% After extensive research of machine learning literature and CDR usage in health problems.

The idea of this chapter is to present a combined overview of the work done and the results encountered in
\cref{ch:descr-risk,ch:machineLearning,ch:modelSelection,ch:ensembleMethods}.
All in all, we intend to showcase most of the relevant information from the preceding sections.
The overall picture of this work shows that there is evidence to confirm the hypothesis which stipulates that cellphone usage logs are rich in social interactions and in turn, these interactions are informative enough on long-term migrations and human mobility.

% A summary of the work done we can say we have discussed issues on the most of the following items:

In \cref{ch:descr-risk} we presented the CDR dataset and how it was relevant to the problem.
We showed how the data contained dynamic georeferrenced data at the user level. 

With a minuscule sample of the CDRs we showed that the dataset is rich in social interactions between users.
This gave us an idea of how this type of data could be leveraged to the general problem.
We then framed our problem in the context of long-term human migrations and gave an idea of the importance of them, in relation to the epidemic nature of the disease.

We noted that the dataset posed a very unbalanced classification problem. 
As expected, most telco users did not show any migration pattern to or from the endemic region, in the time of analysis. 
This also implied that we had a very correlated problem, where most users did not change their past and present endemic conditions. 
This information, combined with the local calling nature, meant that we had correlations with the target feature. 
As users living in the endemic region will most probably have lived there in the past as well.

% in which there are few positive cases over all of the possible.
 % an exploratory We compared the relative sample of users

In this stage we set to explore data transformations and  visualizations to explore further the information contained in the data.
As a starting point we transformed the user-level data into a dataset at the antennas level. 
This step aggregated information of call patterns to and from the endemic region. 
It allowed us to present geolocalized visualizations of these social interactions to the vulnerable areas.
As an intermediate step in this process we had to introduce the definitions that we later used for the rest of this work. 
We technically set what we meant was a vulnerable interaction and what we find is a user\'s home antenna.

% For the latter we base

The antenna level information was finally aggregated into heat maps. 
In these, georeferenced call interactions and colored cellphone antennas according to their level of vulnerable interactions, as defined previously.
% We did this with the help of

The visualizations exposed a `temperature' descent from the core regions outwards.
As expected, the heat was noted to be concentrated in the ecoregion.
We also found out that the level of vulnerable interactions per antenna gradually descends as we move further away.
We say this is expected behavior because it is consistent with findings in the literature in which most calls are done to other local antennas. %in general of a local nature.
Given this period and TelCo of analysis, we saw that ninety percent of the users limited usage to at most four Telco antennas.

We also discovered some unexpected findings that were higlighted by the risk maps.
Interactions from non-endemic antennas towards those in the endemic region were seen to be non-homogeneous in some areas.
As an example, \cref{fig:amba_map} outlines various antennas with higher vulnerability.
We suspect this non-uniformity in the vulnerable interactions can help detect communities with higher probability of disease prevalence.

Health experts agree that these anomalies can be a great starting point to start.
Where potential communities atypical in their neighboring region could be worthy of a latent endemic foci.
These antennas stood out for their strong communication ties with the regions studied, showed significantly higher links of vulnerable communication.

In these images presented, the differences in the vulnerable interactions were clear.
When talking to the ``Mundo Sano Foundation'' researchers participating in this project, they pointed to the fact that the detection of these antennas through the visualizations was of great value to their goals.

 % to the health Mundo Sano Foundation researchers participating in this project.

At the national level, the results evidenced by the maps were coherent with the expert's knowledge of the endemic zone's migration patterns.



In \cref{ch:machineLearning,ch:modelSelection} we gave a practical introduction to Machine Learning in general, and of Supervised Classification problems in specific. 
This helped us set our Chagas problem inside a systematic process to analyze long-term migrations with the CDR dataset.

To do this, we broke the problem of long-term human migrations down to four tasks which were later analyzed through different classifiers.
In these two chapters, we introduced the machine learning theory necessary to structure our problem, along with its methods to solve them.
These models were some of the most common techniques found in the literature for the task we were trying to solve.

To start off we considered the Logistic Regression Classifier along with the logloss metric to evaluate model performance.
Taking to our advantage the model's more tractable formulation, we showed how model regularization fits inside the machine learning frame.
This concept was introduced along with the notion of model hyperparameters. 
Both were relevant to subsequent sections and in particular to \cref{fig:log_loss_regularization_validation_curve}.
There we showed that there is importance in model regularization by fitting a Logistic Regression classifier and comparing how its logloss varias across different regularization values. 
This affected both the training and test set performance in a similar way.


At the same time we introduced other relevant concepts in the Machine Learning and we exemplified them from the our long-term prediction problem. 
As is common in the literature, in \cref{figure:dtree_overfit_problem_2} we saw that increasing a decision tree's complexity leads to a clear case of overfitting the training data when trying to approximate the generalization error.

The same procedure was used to illustrate the concept of ``Cross Validation'' in \cref{fig:cv_vs_test_score}. 
We found that for \cref{target1} the test score was inside a one standard deviation band of the CV score, across a varying hyperparameter value for the Logistic Regression classifier. 
These findings are consistent with lots of other experiments found in the literature. 


Towards the end of \cref{ch:modelSelection} we performed an extensive experimentation with both \cref{target1,target2}, using the same classifier.
In here we finished testing the Logistic Classifier on the whole dataset, and with a systematic approach.

As a minimum runtime, all of the cross-validated fits took greater than one and a half hours on the whole set.
In reality this value is a lower bound on the real optimization time for this algorithm, since we limited the number of iterations to at most one hundred steps.

Another issue found with this algorithm was its extremely poor $F1$ performance for \cref{target2} with a test score lower than $0.01\%$.
As we will see later in other statistical leaners, we had that 
the overall precision of the classifier was very poor due to not being able to accurately overestimating positive predictions.
The algorithm was very inefficient in this aspect where the misclassification had a direct impact in the observed $F1$. 


Yet this low score was not seen in other metrics evaluated. 
For \cref{target2} both the best $Recall$ and $Accuracy$ test and CV rates were all over $0.61\%$. 
The best-fit $C$ values were not consistent along all fits as the values differed in orders of magnitude.
Solutions ranged from to $1E-3$ to $1E-1$ on different fits.
With this we can still positively affirm that the best-fit models for this experiment were those which penalized the loss function with strong regularization terms.


On the other hand, when using this classifier to solve \cref{target1}, we find that we can attain a $ROC AUC$ score of $0.76\%$. 
And this score is achieved when cross validating solely on the $l1$ regularization parameter. 
A similar test score of $0.744\%$ was reached when optimizing for the $l2$ hyperparameter of regularization.
Each were optimized separately in order to exemplify how the overall $ROC AUC$ varied along different regularization thresholds.

This resulted in better $ROC AUC$ scores, when compared with the experiments on \cref{target1}, underscore the problems difference in difficulty.
From the skewed class balance perspective, we confirm our hypothesis about long-term migrations, that this is a harder problem to solve.

Overall, both procedures had best fits on the more regularized $C$values.
A similar result can be found for regularization models and on procedures where hyperparameters were cross validated.
Both optimizations improved improved the models predictive power in varying degrees.


The previous results outline a complete systematical Machine Learning approach and toolset used to evaluate the long-term migrations prediction problem. 
In this way we provide a methodical way to analyze classifiers perfroming on this problem.
With this we tackled the task with the introduction of new algorithms in the following chapters.



In\cref{ch:ensembleMethods} we introduce four new classifiers along with their properties and characteristics. Three of these, \cref{section:decision_trees,section:random_forests,section:gradient_boosting}, were tree based methods whilst the last, \cref{section:naive_bayes}, was a Naive Bayes Classifier used for benchmarking purposes.
For each, we presented an introductory review and pointed the reader to further literature references where applicable.

All were put through a number of experiments to evaluate how they performed across all prediction problems.
For each model, we tried to draw the greatest prediction performance from the features extracted from the dataset.

Tree based models were evaluated in further detail however.
All of these methods were successively compared on the same \cref{target2}, to compare and evaluate them on common ground.

As we will see later in \cref{tab:all_results}, \cref{target2} happened to be the hardest task of all, where all classifiers had their lowest scores.

By comparing this problem across the tree methods, we gave a thorough discussion of the applications that these kind of datasets can provide.
% to these kinds of tasks was elaborated.

As a first observation we 






 gave the lowest values for all classifiers. 
 
in successive 
in their overfit
 in succe








In all, we can say that for the prediction tasks, the feature selection heuristics of ensemble algorithms did agree upon a group of features. 
These were highlighted over the rest for a number of times and they provide insight into what CDR information provided most
predictive value.

From the results we collected, the best features can be broken down into the following categories.

\begin{itemize}

    \item Calls made in older months and in December: Interactions of different type, that occured closer to the split month between both periods $T_0$ and $T_1$ (July), were also distinguished.
    The model frequently used interactions from the months of August and September, as well as December.
    We conjecture that this last result can be due to the fact that December is a month where user\'s increase their mobile activity with their families.

    \item Vulnerable calling patterns: As we first suspected in \cref{ch:descr-risk}, the vulnerability of the mobile interaction between users was relevant.
    Measurements such as the duration and volume of calls at different time periods where selected.
    We make the reminder here that these measurments were the basis of our construction of the heatmaps in that same chapter.
    Where the antennas plotted concentrated users with vulnerable calls by calling volume or time, for a given month.

    \item Mobility size: Finally, different measures of the distance in human mobility were determined to be important by the model. Both general mobility diameters and weekday or weeknight specific mobility endemic were relevant and on various occasions. 
    We can informally add that Decision Tree Learners also selected this features in the higher nodes. 
    This result can be seen in detail at \cref{subsection:decision_trees_experiment}.

\end{itemize}

These gruops identify strong predictors in long-term human mobility and provide additional insights into our original hypothesis of which predictors were most valuable for the task.
Understanding these factors can further provide information into the problem of chagasic disease spread in the long run.
A more thorough examination of these results can be seen in \cref{tab:random_forest_big_experiment_best_features, tab:boosting_big_experiment_best_features}.




From our findings, we can say that there is no absolute single method that can be single-handedly applied to all problems and evaluation performances. 
However, our results outline that the Gradient Boosting methods were, in general, top performing for all tasks and classifiation metrics, except for the $F1$ score.

Surprisingly enough, for some problems the Naive Bayes was outperforming in this $F1$ metric. 
The algorithm was more conservative in its positive predictions and this difference saw a tradeoff in its performance across other metrics.

However we can say that results show that the performance of Gradient Boosting models is best over other models such as Logistic Regression or Naive Bayes algorithms for most tasks, and slightly better over the Random Forests. 
These models can find complex interactions in the data whilst handling enough care not to lose generalization power.


% Each of the tasks will have different scores across the algorithms used.
% and the same applies to the metrics used to evaluate them.

\todo{Reword sentences
Expand, correct and finish them in a cohesive structure.}



% ||||||||||||||||||||||||||||||||||||| SPLIT |||||||||


Also, the effects of overfitting the models were visible in some of the experiments where deep trees and complex configurations underpowered the resulting model's generalization error.



High discrepancies exist in the scores of the different machine learning models.
Some are better at capturing the relevant information to predict human movements in the long-term.
This is because not all social information is presented directly in the dataset, and feature interactions are helpful.


This work may give us some insights into future disease spreads in other similar contexts.
Where
It also helps to showcase interesting ways of using the CDRs for reasons other than logging a user\'s calling expenses.



The implications of these results argue that CDRs can be extensively used for non business purposes and, specifically in this case, there is light evidence that they might capture well the fluxes of diseases among humans.
This data was used as a surrogate to other, more traditional, methods of disease control and surveillance.
This work intends to help authorities with disease-control strategies in way which is not intrusive for cellphone users.
Measures can then be applied outside of the endemic region, and directed towards specific neighbourhoods and communities by using the recommendations output by the risk algorithm.


\cref{tab:all_results} shows a compendium of the results obtained from the CDR dataset, using all classification models.
For ech problem and model, we found the best performing learner with a cross validation procedure which searched over an extensive grid of hyperparameters.
In all cases, the hyperparameter configuration with highest cross-validated $ROC AUC$ was selected as the \textit{best-fit}.
%Due to time constraints limited by the use of full cross validation fitting procedures, not all models were run for all of the tasks.
The following table shows the best-fit classifier's performance across three scores on the test set $\mathcal{T_s}$: $Accuracy$, $F1$ and $ROC AUC$.

\begin{table}
\caption{Master resutls table comparing results for all of the classifiers run in this work.
For each task and classifer, we show the its $Accuracy$, $ROC AUC$ and $F1$ test-set scores, along with the runtimes of a full cross validation procedures on the learner.}
\label{tab:all_results}
\centering
    \begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill} }  l l l l l }
    %{|p{2cm}|p{2cm}|p{1.5cm}|p{1cm}|p{1.5cm}}
    \toprule
    Measure & Problem 1 & Problem 2 & Problem 3 & Problem 4  \\
    \midrule
    Naive Bayes     & (0.84,0.82,0.75)  & (0.64,0.61,0.31)  &  (0.65,0.63,0.45)   & (0.85,0.76,0.62)   \\
    Logistic Classifier   & (0.893,0.857,0.9)  & (0.71,0.72,0.058)  &  (0.705,0.754,0.107)   & (0.883,0.85,0.181)   \\
    Random Forest   & (0.878,0.857,0.9)  & (0.714,0.726,0.058)  &  (0.705,0.754,0.107)   & (0.883,0.85,0.181)   \\
    Gradient Boosting   & (0.974,0.978,0.952)  & (0.838,0.819,0.101)  &  (0.811,0.855,0.169)   & (0.885,0.873,0.194)   \\

    \bottomrule
    \end{tabular*}
\end{table}


\todo{Bigger, better and more esthetically pleasing table is commented in the code below this one. Yet the commented table does not fit in a whole page and spawns compilation errors. This needs to be fixed.}





% \begin{landscape}% Landscape page
%         \begin{table*}
%             \centering
%             \ra{1.3}
%             \begin{tabular}{@{}rrrrcrrrrcrrcrr@{}} \toprule
%                 &  \multicolumn{4}{c}{Problem 1} &  \multicolumn{4}{c}{Problem 2} & \multicolumn{4}{c}{Problem 3}  & \multicolumn{4}{c}{Problem 4}\\
%                  \cmidrule{2-5} \cmidrule{6-9} \cmidrule{10-13} \cmidrule{14-17}
%                 & $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ && $Accuracy$ & $AUC$ & $F1$ & $Runtime (m)$ \\
%                 \midrule
%                 $Naive Bayes$ & 0.84 & 0.82 & 0.75 & 2 && 0.64 & 0.61 & 0.31 & 2 && 0.65 & 0.63 & 0.45 & 1 && 0.85 & 0.76 & 0.62 & 1 \\
%                 $Logistic \ Regression$ & 0.893 & 0.857 & 0.9 & 96 && 0.714& 0.726 & 0.058  & 119 && 0.705& 0.754 & 0.107 & 115 && 0.883 & 0.85 & 0.181 & 106 \\
%                 $Random \ Forest$ & 0.878 & 0.903 & 0.77 & 33  && 0.79 & 0.776 & 0.088 & 45 && 0.792 & 0.845 & 0.156 & 21 && 0.898 & 0.853 & 0.203 &  19\\
%                 $Gradient \ Tree \ Boosting$  & 0.974 & 0.978 & 0.952 & 41 && 0.838 & 0.819 &  0.101 & 54  && 0.811 & 0.855 & 0.169 & 33 && 0.885 & 0.873  & 0.194 & 47 \\
%                 \bottomrule
%             \end{tabular}
%             \caption{Caption}
%         \end{table*}
% \end{landscape}







\section{Conclusions}\label{section:conclusions}


The analysis exposed allowed us to show that the Risk maps we developed are of interest to national health campaigns.
They pinpoint to likely spots of high disease prevalence, with a low cost to produce this outcome.
Added to this, the risk maps produced many interesting insights of specific antennas that have higher vulnerable interactions with the endemic region, where these are located outside of the endemic region.

Next to traditional health control actions, this research may provide a very insightful piece of information for decision-makers.
Capturing long-term human movements will also enable better simulations of disease spreads in different contexts.
In this work, the impact of this in our predictive capacity of disease prevalence is shown.

We believe it is acceptable to say that long-term human dynamics in the form of migration patterns can be captured by the use of this data.
Since this work was performed in two Latin-American countries and in one single endemic region, we expect that this CDR usage can be extended to other similar countries and diseases, with demographic, cultural and geographical similarities beyond Chagasic spread in Mexico or Argentina.



The supervised classification experiments done on the various prediction tasks were discussed in previous sections.
The results highlighted the benefit of using this type of data for the problem.


% in the preceding chapters to our tasks.
The best learners for the following tasks were the Random Forests and Gradient Boosting models.
Due to this, we focused our work on the strengths of the ensemble learners to achieve higher predictions.
Still, we saw differences in both ensemble methods, where the former was better at this score, whilst the latter at the second.


The features most brought up in the experiments of ensemble learners where A, B, c.
These indicated this and that.
They also pointed to information in  blablabla.
Some supervised classification models realized the importance of attributes tagged with vulnerable behavior obtained from the CDR.\@

The Random Forest model played an important role in having the highest scores across all tasks and in presenting the best features of the dataset.
This information was later availed by the boosting models trained on said set of features.



With this, we showed that it is possible to use the mobile phone records of users during a bounded period (of 5 months) in order to predict whether they have lived in the endemic zone $E_Z$ in a previous time frame (of 19 months).

Combining social and geolocated information, the data at hand has been given an innovative use, different from its original billing purpose.

% To conclude, the results presented in this work show that


Currently, epidemic counter-measures include coordinating national surveillance systems with institutions and primary healthcare organizations, vector-centered policy interventions through fumigation of vector-infested regions and individual in-field screening of people.

These measures require costly infrastructures to set up and be run.
On the other hand, this work shows built on top of existing mobile networks would demand lower costs, taking advantage of the already available infrastructure
The potential value these results could add to health research is hereby exposed.
Finally, the results stand as a proof of concept which can be extended to other countries or to diseases with similar characteristics.


\begin{itemize}

    \item \textbf{Conclusion}

    \item This method and dataset showed a systematic approach to analyze the information contained in the dataset.
    The idea was to try to explore the human mobility patterns, as captured by the phone call records.

    \item Heatmap allowed discovery of anomalous communities with attributes from the data which were later confirmed to be relevant in helping detect long-term movement of users.

    \item We compared the strengths and weaknesses of these methods.
    The algorithms  were used to estimate the probability of each user's migrations.
    Also, we showcased the method's results differences for the tasks defined in \cref{ch:machineLearning}.

    \item Where possible, we exposed the best features of the dataset to solve the general problem.
    These exposed the relationship between the attributes that characterize vulnerable user interactions, and the target variable of the task.
    Other relevant features, such as the user's mobility size, were also found to provide relevant information for the classification task.

    \item According to our results, there are algorithms which are reliable enough to detect users which have migrated from old regions. In some instances, we have achieved high values across all scoring measures. To summarize the work done we can say that:
     % the algorithms detected that the characterization of the user's mobility provided relevant information to the task

\end{itemize}



In this work on CDRs, the Argentinean and Mexican case studies allow us to find that it is possible to characterize human movements of long duration.
Where cellphone datasets are rich enough in information to detect detailed patterns of users moving to and from a particular area, at a national level.


Results show evidence indicating that a supervised classification predictor can be built to detect large patterns of human migrations over time.
They also point to the idea that CDRs are particularly well suited for this task, where it is possible to explore this data as a mean to tag human mobility.
These tests effectively helped understand how relevant the dataset is to the question, where the complexity and scale of the data can be, up to a certain degree, assimilated by probabilistic algorithms.
% , using the models outlined before.

Large at-scale and temporal user mobility were captured by the CDRs, allowing us to understand a variety of human movements thoughout the country. 


% \section{In-depth discussion}
% In this work we took advantage of the large user scale and temporal range of the CDRs allowed us to capture a variety of human movements throughout the country.


Migration patterns can then be used to suggest actual epidemic spread of this disease into areas not deemed endemic.



\section{Drawbacks from our methodology}

There are several points can be stated to point to weak spots on the methodology used.
An important observation is that we can not directly jump from the human mobility insights found in this study, to conclusions from a model of disease spread. 
From this work, we were looking solely at human mobility to and from endemic regions, which does not imply disease spread or prevalence.
However, on the advice of this topic's researchers, this work does bring valuable insights to the problem, reinforcing previous hypothesis that vulnerable users are not only to be found inside vector-infested regions.


Another issue with this work is the inherent biases in our dataset.
As shown in \cref{tab:distribution_by_state}, there are significant differences in the percentage of population represented by our dataset vs.\ current state distribution estimates.
Due to this, we have to interpret results with caution, since we do not hold any real-data comparisons. 
For this work we did not find any available georeferrenced disease data outside of the endemic region, at municipal or regional levels.\footnote{As a matter of fact, we tried to contact health institutions and research organizations working on the matter with the purpose of enriching the original dataset. 
This intent did not add any disease-related data of the kind.}

Other interesting biases stemming from the data are related to international pattern migrations.
For this work, we measured migrations only from phone owners within a single country.
Yet we have seen in \cref{ch:descr-risk}, that  epidemic regions traverse political borders.
The current dataset is limited to only capturing international migrations, and with this our analysis is limited to movements of national Telco users only.
A similar argument applies to the detection of mobility patterns for people with no cell-phone usage.
 % lack cell phone antennas.

Our dataset also presents problems when processing a user's home antenna.
To do this we had to define what we though were the expected ``working hours'' for all users.
In doing so, we also assumed that this coulb be generalized to all samples. %used in a generalized case.
Even though this decision was supported from past research we refereneced, the definitons might not be easily translated for datasets across other geographical and cultural regions.
% This was done by considering what we thought 


Finally, we know that we can have significant time seasonality and stationarity in the data, yet we did not consider statistical methods to reduce this effect in a thorough fashion. % but only in a limited fashion.
When processing the data, we specifically tagged weekend and working hour attributes separately from the rest of the features.
The same was done for month specific attributes on user calling patterns.
However we understand that there could be other longer term time effect which was not considered under our current codebase. %used in this work.
There are possibilities of migration patterns being strongly related to seasonal factors such as, for example, in agriculturarl workers having seasonal migrations around regions.% workforce.

The importance of other user specific bias, such as demographics unbalance, was not thoroughly examined.
Yet these were out of the scope of this work, due to the lack of ground truth in the data.


% discriminated in the construction of the features, yet there were no long-term time



\subsection{ Lines of Future Work }

\todo{Add try other ML methods such as SVM or neural nets to improve on the general F1 score or compete against Naive Bayes high F1 score.}

The mobility and social information extracted from CDRs analysis has been shown to be of practical use for long-term human migrations and for Chagas disease research.
it adds value and information to help make data driven decisions which in turn is key to support epidemiological policy interventions in the region.
For the purpose of continuing this reserach line, the following is a list of possible extensions:


\begin{description}
    \item [Results validation.] Compare observed experiment results of risk maps and best features against actual serology or disease prevalence surveys.
    Data collected from fieldwork could be fed to the algorithm in order to supervise the learning towards a target variable that is defined by the disease prevalence at a certain level.


    \item [Differentiating rural antennas from urban ones.] This is important as studies show that rural areas have epidemiological conditions which are more favorable to the expansion of the disease expansion.
    The vector-borne transmission through \textit{Trypanosoma cruzi} is helped with poor housing materials and domestic animals.
    All  contribute to complete the parasite's life-cycle.
    Using the CDR data, antennas could be automatically tagged as rural by analyzing the differences between the spatial distribution of the antennas in each area.
    A similar goal could be to identify precarious settlements within urban areas, with the help of census data sources.

    \item [International and seasonal migration analysis.] Experts from the \textit{Mundo Sano} Foundation underlined that many seasonal and international migrations occur in the \textit{Gran Chaco} region.
    Workers are known to leave the endemic area for several months possibly introducing the parasite to foreign populations.
    The same happens with foreign workers, which are not part of our dataset.
    The mobility analysis on particular time periods or events, for example on holidays, or specific migrations from bordering countries, can give information on which communities have a higher influx of people from the endemic zone during a certain period.
    This additional analysis would also lower the bias of the current dataset.


    \item [Search for epidemiological data at a detailed level.] For instance, specific historical infection cases.
    Splitting the endemic region according to the infection rate in different areas, or considering particular infections.
    With the use of a high-quality epidemic dataset, we assume that a more model complex can be built, to detect infected users nationwide.
    It is of primary importance though to have a good dataset available.
    Since otherwise, it would not be possible to reach a generalizing classifier.

    \item With the above, we could add a similar approach as in ths work, to explore features and determine for the most relevant attributes that are associated with being Chagas infected.

\end{description}




%   ___  _     ___         ___________ __ __ _____ _____ 
%  /   \| |   |   \       / ___/      |  |  |     |     |
% |     | |   |    \     (   \_|      |  |  |   __|   __|
% |  O  | |___|  D  |     \__  |_|  |_|  |  |  |_ |  |_  
% |     |     |     |     /  \ | |  | |  :  |   _]|   _] 
% |     |     |     |     \    | |  | |     |  |  |  |   
%  \___/|_____|_____|      \___| |__|  \__,_|__|  |__|   

% % With a running time of 80s, 10 steps are generally needed to achieve a 0.977 validation accuracy score on
% The best model was a Logistic Regression Classifier with an $L2$-penalty value of 0.01.
% % Table~\cref{ts}
% The following table
% shows the scores obtained by the selected model on the out-of-sample set.

% \begin{table}\label{tab:results}[ht]
%   \caption{Resulting scores.}

%   \centering
%   \begin{tabular}{ l l }
%       \toprule
%       Score & Value \\
%       \midrule
%       F1 score & 0.964537  \\
%       Accuracy & 0.980670  \\
%       AUC    & 0.991593  \\
%       Precision & 0.970838  \\
%       Recall  & 0.958316  \\
%       \bottomrule
%   \end{tabular}
% \end{table}


% High values across all scoring measures are achieved
% with the best estimator, with the lowest metric starting at a value of 0.958.
% These results can be explained by the fact that
% %Scores results become less surprising when looking closer at the dataset.
%
%Si del test\_set ahora me quedo con lo users que Y\_target ==1 == EPIDEMIC\_gt pero que hoy en dia son epidemic ==0 (se mudaron)s
 % scores bajan mucho:
%
%+--------------+-----------------+-------+
%| target\_label | predicted\_label | count |
%+--------------+-----------------+-------+
%|   1    |    1    | 1944 |
%|   1    |    0    | 3525 |
%+--------------+-----------------+-------+
%'f1\_score': 0.5244840145690004, ''accuracy': 0.3554580,, 'precision': 1.0, 'recall': 0.35545803,

% \section{Key Results}

