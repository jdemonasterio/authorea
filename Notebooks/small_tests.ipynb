{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Mini Tests ipynb\n",
    "used to create graphs and compute calculations that are later introduced in the thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seteamos los parametros del notebook\n",
    "%autosave 180\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "import sys\n",
    "import pandas as pd; \n",
    "import numpy as np; \n",
    "import os;\n",
    "import random;\n",
    "import time\n",
    "import seaborn as sns\n",
    "palette = sns.color_palette()\n",
    "import sklearn\n",
    "\n",
    "np.random.seed(2015)\n",
    "\n",
    "# for nice long graphic titles\n",
    "from textwrap import wrap\n",
    "\n",
    "#seteamos el path de los datos de trabajo\n",
    "\n",
    "PROJECTDIR = os.getcwd().split(os.sep)\n",
    "PROJECTDIR =  os.sep.join(PROJECTDIR[:PROJECTDIR.index('authorea') + 1])\n",
    "DATADIR = os.path.join(PROJECTDIR,'datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python version\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python libraries versions\n",
    "sns.__version__, sklearn.__version__, np.__version__, pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# roc curve graph\n",
    "test differnt plots for roc curves and show the *AUC* for these\n",
    "\n",
    "### 2 cases\n",
    "* Good algorithms\n",
    "* \"Random algorithms\"\n",
    "\n",
    "## Technical Requirements to run this script\n",
    "* 1gb of RAM\n",
    "* Read module versions after `import`s are done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import specific sklearn modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.naive_bayes import *\n",
    "\n",
    "#from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "# Import some data to play with\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X_train = cancer.data\n",
    "y = cancer.target\n",
    "#y = label_binarize(y, classes=[0, 1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#random_state.randn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 2#y.shape[1]\n",
    "\n",
    "# Get new dataset with noisy features to make the problem harder\n",
    "random_state = pd.np.random.RandomState(0)\n",
    "n_samples, n_features = X_train.shape\n",
    "mu, sigma = (40,8) # choose noise deviation\n",
    "X_noise = pd.np.c_[X_train, mu*np.random.randn(n_samples, 20 * n_features) + sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify on original X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle and split training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, y, test_size=.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(X_train) \n",
    "X_test = pd.DataFrame(X_test)\n",
    "#X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD \n",
    "data from CDRs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $DATADIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get_input_ frame \n",
    "url = DATADIR + '/data_balanced_sample.csv'\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['EPIDEMIC_gt'].sum(),data['EPIDEMIC'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.query('EPIDEMIC==0  ').STATE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split as \n",
    "p% of set as validation and the resulting  as train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_perc = 0.9\n",
    "mask = pd.np.random.rand(data.shape[0])< split_perc\n",
    "\n",
    "val_set = data[mask==0]\n",
    "data = data[mask==1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define our real X_trainvariable and Y vars\n",
    "\n",
    "exclude/include features. Decide our problem (multi-target, single_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this param will *force* the exclusion of these columns in the final X_train, no matter what.\n",
    "\n",
    "manual_exclude_cols = [     \n",
    "#     'EPIDEMIC',\n",
    "#      'EPIDEMIC_gt',\n",
    "#       'STATE',\n",
    "                ]\n",
    "\n",
    "iterable=data.columns\n",
    "\n",
    "comprehensive_exclude_cols = [col for col in iterable if col == 'USER' \n",
    "          or ('ANTENNA' in col) ]   \n",
    "                                \n",
    "exclude_cols = manual_exclude_cols + comprehensive_exclude_cols\n",
    "exclude_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define lables or Y vars\n",
    "\n",
    "and add to them a set of different possible CASEs/problems to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CASE = 1\n",
    "\n",
    "## people that used to live in the endemic area\n",
    "if CASE ==0:\n",
    "    CASE_TEXT = \"people that used to live in the endemic area\"\n",
    "    \n",
    "    Y_train= data['EPIDEMIC_gt'] == 1\n",
    "    Y_test = val_set['EPIDEMIC_gt'] == 1\n",
    "    add_exclusion_cols =[     \n",
    "            #     'EPIDEMIC',\n",
    "                 'EPIDEMIC_gt',\n",
    "#                   'STATE',\n",
    "                ]\n",
    "    \n",
    "\n",
    "## people that used to live in the endemic area *and* migrated\n",
    "if CASE ==1:\n",
    "    CASE_TEXT = \"people that used to live in the endemic area *and* migrated\"\n",
    "    Y_train= (data['EPIDEMIC_gt'] ==1) & (data['EPIDEMIC'] ==0)\n",
    "    Y_test = (val_set['EPIDEMIC_gt'] ==1) & (val_set['EPIDEMIC'] ==0)\n",
    "    add_exclusion_cols = [\n",
    "#                         'EPIDEMIC',\n",
    "                          'STATE',\n",
    "#                          'EPIDEMIC_gt',\n",
    "                    ]\n",
    "\n",
    "##  people that migrated in any direction\n",
    "if CASE ==2:\n",
    "    CASE_TEXT = \"people that migrated in any direction\"\n",
    "    Y_train= data['EPIDEMIC_gt'] != data['EPIDEMIC']\n",
    "    Y_test = val_set['EPIDEMIC_gt'] != val_set['EPIDEMIC']\n",
    "    \n",
    "    add_exclusion_cols = [           \n",
    "                 'EPIDEMIC',\n",
    "#                   'EPIDEMIC_gt',\n",
    "                   'STATE',\n",
    "]\n",
    "\n",
    "    \n",
    "##  people that migrated in any direction, but are currently non-endemic\n",
    "if CASE ==3:\n",
    "    CASE_TEXT = \"currently non_endemic, that used to live in the endemic area\"\n",
    "    \n",
    "    data = data[data['EPIDEMIC'] ==0]\n",
    "    val_set = val_set[val_set['EPIDEMIC'] ==0]\n",
    "\n",
    "    Y_train= (data['EPIDEMIC_gt'] ==1)\n",
    "    Y_test = (val_set['EPIDEMIC_gt'] ==1) \n",
    "    \n",
    "    add_exclusion_cols = [\n",
    "                'EPIDEMIC'\n",
    "                'EPIDEMIC_gt',\n",
    "                'STATE',\n",
    "         \n",
    "                         ]    \n",
    "    \n",
    "## people from the Mexico or DF states\n",
    "if CASE == 4:\n",
    "    CASE_TEXT = \"people from the Mexico or DF states\"\n",
    "    Y_train= (data['STATE'] == 'Distrito_Federal') | (data['STATE'] == 'Mexico')\n",
    "    Y_test = (val_set['STATE'] == 'Distrito_Federal') | (val_set['STATE'] == 'Mexico')\n",
    "    \n",
    "    add_exclusion_cols = [\n",
    "                'EPIDEMIC',\n",
    "                'STATE',\n",
    "                ]\n",
    "                        \n",
    "## people with a HIGH present mobility (>1000 after looking at percentiles of the MOBILITY_DIAMTER)\n",
    "if CASE == 5:\n",
    "    val = 1000\n",
    "    CASE_TEXT = \"people with a high mobility during present time (values > {} )\".format(val)\n",
    "    Y_train= (data['MOBILITY_DIAMETER'] > val) \n",
    "    Y_test = (val_set['MOBILITY_DIAMETER'] > val) \n",
    "    \n",
    "    add_exclusion_cols = [\n",
    "        \n",
    "        'MOBILITY_DIAMETER_WEEKNIGHT',\n",
    "        'MOBILITY_DIAMETER',\n",
    "    ]\n",
    "\n",
    "for col in add_exclusion_cols:\n",
    "    if not col in exclude_cols:\n",
    "        exclude_cols+=[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_exclusion_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# after excluding cols, redefine available cols \n",
    "iterable=data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 30\n",
    "N= pd.np.random.randint(1,int(iterable.shape[0]*1.0/width))\n",
    "data.columns[(N)*width: (N+1)*width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train= data[[col for col in iterable if col not in exclude_cols]].copy()\n",
    "\n",
    "CASE_TEXT += '. Excluding features: '\n",
    "for col in exclude_cols:\n",
    "    CASE_TEXT+= col +', '\n",
    "\n",
    "X_test = val_set[[col for col in iterable if col not in exclude_cols]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterable=X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean negative/Null vals in count cols \n",
    "for col in [col for col in iterable if 'COUNT' in col]:\n",
    "    X_train[col]= X_train[col].apply(lambda x :  x if x>=0 else 0)\n",
    "    X_test[col]= X_test[col].apply(lambda x :  x if x>=0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dummy-ize categorical cols\n",
    "if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterable=X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'STATE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    if col not in exclude_cols:\n",
    "        print('we are categorizing col %s' %col)\n",
    "        X_train[col] = X_train[col].astype('category')\n",
    "    #     if X_train[col].dtype != 'category':\n",
    "    #         continue \n",
    "        X_train= pd.concat([X_train,pd.get_dummies(X_train[col], \n",
    "                                          prefix= col, \n",
    "                                          prefix_sep='_', \n",
    "                                          #sparse = True,\n",
    "                                          dummy_na=False).astype(pd.np.int8)],\\\n",
    "                  axis=1 ,join = 'inner')\n",
    "        X_train.drop(col, axis =1 , inplace=True)\n",
    "\n",
    "        # now onto test_table\n",
    "        X_test[col] = X_test[col].astype('category')\n",
    "        X_test = pd.concat([X_test,pd.get_dummies(X_test[col], \n",
    "                                          prefix= col, \n",
    "                                          prefix_sep='_', \n",
    "                                          #sparse = True,\n",
    "                                          dummy_na=False).astype(pd.np.int8)],\\\n",
    "                  axis=1 ,join = 'inner')\n",
    "\n",
    "        X_test.drop(col, axis =1 , inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Polynomial and/or scaled features\n",
    "if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_files(tables, scaled=False, poly2=False):\n",
    "    #como input tiene que ir una lista de tables donde la tabla donde se fitea todo viene PRIMERA, i.e. en la pos 0\n",
    "    #tiene que suceder que haya consistencia entre las columnas de c/dataframe tambien\n",
    "    train_table = tables[0]\n",
    "    \n",
    "    categorical_cols = []\n",
    "    count_time_cols = [col for col in train_table.columns]\n",
    "    \n",
    "    print(\"First input dataframe is %s big\" % str(train_table.shape))\n",
    "    #print(\"Test dataframe is %s big\" % str(test_table.shape))\n",
    "    \n",
    "\n",
    "    \n",
    "    if scaled ==True :\n",
    "            min_max_scaler = MinMaxScaler().fit(train_table)\n",
    "            train_table = min_max_scaler.transform(train_table)\n",
    "    \n",
    "    #hacemos interacciones polinomiales sobre las columnas de count/time\n",
    "    if poly2 == True:\n",
    "        #pensar que hacer polynomial features es masomenos como agregar nˆ2 nuevas columnas\n",
    "        #con lo cual tenemos que tener cuidado en no reventar la memoria, ponemor las primeras 50 columnas \n",
    "        #como tope para aplicar interacciones polinomiales\n",
    "\n",
    "        poly2_transform = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "        max_cols = 30\n",
    "\n",
    "        if  (train_table.shape[1] <= max_cols):\n",
    "            max_cols = train_table.shape[1]\n",
    "        poly2_transform.fit(train_table[:,0:max_cols])\n",
    "\n",
    "        train_table = poly2_transform.transform(train_table[:,0:max_cols])\n",
    "    \n",
    "    print(\"Processed input dataframe is %s big\" % str(X_train.shape))\n",
    "    \n",
    "    processed_tables = [pd.DataFrame(train_table)]\n",
    "    \n",
    "    #aca basicamente replicamos lo anterior pero para todo el resto de las tablas y utilizando los fits que ya tenemos\n",
    "\n",
    "    for i, table in enumerate(tables):\n",
    "        # skip the X_train table which comes in the first postion and will behave as our special fitting table\n",
    "        if i ==0:\n",
    "            continue  \n",
    "        #table_categorical = table[categorical_cols].values\n",
    "        #table_count_time = table[count_time_cols].values\n",
    "        \n",
    "        if scaled ==True :\n",
    "            table = min_max_scaler.transform(table)\n",
    "\n",
    "        #hacemos interacciones polinomiales sobre las columnas de count/time\n",
    "        if poly2 == True:\n",
    "            #pensar que hacer polynomial features es masomenos como agregar nˆ2 nuevas columnas\n",
    "            #con lo cual tenemos que tener cuidado en no reventar la memoria, ponemor las primeras 50 columnas \n",
    "            #como tope para aplicar interacciones polinomiales\n",
    "\n",
    "            table = poly2_transform.transform(table)\n",
    "        \n",
    "        \n",
    "        \n",
    "        processed_tables = processed_tables + [pd.DataFrame(table)]\n",
    "    \n",
    "    for i in range(len(processed_tables)):    \n",
    "        print(\"Table {0} shape is {1}\".format( i,str(processed_tables[i].shape)))\n",
    "#        print(\"Table %s categorical shape is %s\" % str(X_test_categorical.shape))\n",
    "#        print(\"Val non-categorical shape is %s\" % str(X_test_count_time.shape))\n",
    "#        print(\"Val categorical shape is %s\" % str(X_test_categorical.shape))\n",
    "    \n",
    "    return tuple(processed_tables)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train), type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "do_scale_or_poly = False\n",
    "if do_scale_or_poly:\n",
    "    X_train,X_test = get_X_files([X_train,X_test], scaled=True, poly2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_correlation = False\n",
    "if do_correlation:\n",
    "    corr = pd.DataFrame(X_train).copy()\n",
    "    target_col = 'target'\n",
    "    corr[target_col] = Y_train\n",
    "    corr = corr.corr()\n",
    "    print(corr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if do_correlation:\n",
    "    view = corr.query('target>0.1')\n",
    "    # show only those columns which \n",
    "    corr_columns = view.index.values\n",
    "\n",
    "    display(view[corr_columns].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## show correlation between state and target\n",
    "if do_correlation:\n",
    "    if not 'STATE' in exclude_cols:    \n",
    "        state_cols = [col for col in corr if 'STATE' in col]\n",
    "        view = corr[state_cols + [target_col]]\n",
    "        display(view.query('target > 0.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class weight check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=data['EPIDEMIC_gt'].sum();b= data.shape[0]\n",
    "a*1.0/b\n",
    "\n",
    "a=val_set['EPIDEMIC'].sum();b= val_set.shape[0]\n",
    "a*1.0/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify on different Xes\n",
    "(same features are used but noise is added to them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose approximating function\n",
    "#method = 'svm'\n",
    "# method = 'bnb'\n",
    "# method = 'rf'\n",
    "# method = 'dtree'\n",
    "method = 'logit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Learn to predict each class against the other\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "if method == 'rf':\n",
    "    \n",
    "    param_grid = {'kernel':['rbf','linear'], 'C': list(float(10)**pd.np.arange(-2,2)),\n",
    "                 }\n",
    "    \n",
    "    svm_clf = svm.SVC(kernel='linear', probability=True,\n",
    "                                     random_state=random_state)\n",
    "    clf =GridSearchCV(svm_clf, param_grid, scoring='roc_auc', fit_params=None, n_jobs=-1, iid=True, refit=True, \n",
    "    verbose=2, pre_dispatch='2*n_jobs', error_score='raise')\n",
    "    \n",
    "    y_score = clf.fit(X_train, Y_train).predict_proba(X_test)\n",
    "\n",
    "\n",
    "if method == 'logit':\n",
    "\n",
    "    param_grid = {'C':[1e3,1e2 ,1e1], # 'fit_prior': [True],\n",
    "                 }\n",
    "\n",
    "    lo  = LogisticRegression( )\n",
    "\n",
    "    clf =GridSearchCV(lo, param_grid, scoring='roc_auc', fit_params=None, n_jobs=-1, iid=True, refit=True, \n",
    "    verbose=2, pre_dispatch='2*n_jobs', error_score='raise').fit(X_train, Y_train)\n",
    "    \n",
    "    lo = clf.best_estimator_.fit(X_train, Y_train)#.feature_log_prob_(X_test)\n",
    "    y_score = lo.predict_proba(X_test)\n",
    "    \n",
    "\n",
    "if method == 'bnb':\n",
    "\n",
    "    param_grid = {'alpha':[1e-3, 1e-1], 'fit_prior': [True],\n",
    "                 }\n",
    "\n",
    "    mnb  = BernoulliNB( )\n",
    "\n",
    "    clf =GridSearchCV(mnb, param_grid, scoring='roc_auc', fit_params=None, n_jobs=-1, iid=True, refit=True, \n",
    "    verbose=2, pre_dispatch='2*n_jobs', error_score='raise').fit(X_train, Y_train)\n",
    "    \n",
    "    mnb = clf.best_estimator_.fit(X_train, Y_train)#.feature_log_prob_(X_test)\n",
    "    y_score = mnb.predict_proba(X_test)\n",
    "    \n",
    "if method == 'rf':\n",
    "    \n",
    "    param_grid = {'criterion': ['gini','entropy'], 'n_estimators': [4,5,3],\n",
    "      'max_features': [\"auto\",15], #\"bootstrap\": [True, False],\n",
    "        \"min_samples_leaf\": [4,8],'max_depth':[6,8], \n",
    "#                   \"class_weight\": ['balanced']\n",
    "              }\n",
    "    rforest  = RandomForestClassifier( )\n",
    "    clf =GridSearchCV(rforest, param_grid, scoring='roc_auc', fit_params=None, n_jobs=-1, iid=True, refit=True, \n",
    "        verbose=3, pre_dispatch='2*n_jobs', error_score='raise').fit(X_train, Y_train)\n",
    "    \n",
    "    rforest = clf.best_estimator_.fit(X_train, Y_train)#.feature_log_prob_(X_test)\n",
    "    y_score = clf.predict_proba(X_test)\n",
    "\n",
    "    \n",
    "if method == 'dtree':\n",
    "    \n",
    "    param_grid = {'criterion': ['gini'], 'splitter': [#'best',\n",
    "                                                      'random'],\n",
    "      'max_features': [\"auto\"], \"min_samples_split\": [4,2,],\n",
    "        \"min_samples_leaf\": [4],'max_depth':[5,10,8 ], \n",
    "                  \"class_weight\": ['balanced']\n",
    "              }\n",
    "    dtree  = DecisionTreeClassifier()\n",
    "    clf =GridSearchCV(dtree, param_grid, scoring='roc_auc', fit_params=None, n_jobs=-1, iid=True, refit=True, \n",
    "        verbose=3, pre_dispatch='2*n_jobs', error_score='raise').fit(X_train, Y_train)\n",
    "    \n",
    "    dtree = clf.best_estimator_.fit(X_train, Y_train)#.feature_log_prob_(X_test)\n",
    "    y_score = clf.predict_proba(X_test)\n",
    "\n",
    "    \n",
    "elapsed_time =   time.time() - start_time \n",
    "print('Grid Search took %s seconds to run' % (elapsed_time))\n",
    "\n",
    "print('\\n Best estimator params was %s \\n' % str(clf.best_estimator_))\n",
    "print('\\n Best estimator score was %s \\n' % str(clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_, clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CASE, CASE_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 0.8\n",
    "\n",
    "# Compute ROC curve and ROC area \n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, y_score[:, 1])\n",
    "mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "mean_tpr[0] = 0.0\n",
    "roc_auc = auc(fpr, tpr)\n",
    "damper = 0.4\n",
    "lin_damper = np.linspace(damper,1,len(fpr))\n",
    "\n",
    "plt.plot(fpr, tpr*lin_damper, lw=lw, color='seagreen',\n",
    "         label='ROC (area = %0.2f)' % ( roc_auc*damper))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k',\n",
    "         label='Luck')\n",
    "\n",
    "mean_tpr /= clf.n_splits_\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "#plt.plot(mean_fpr, mean_tpr, color='r', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "title_str = \"\"\"Receiver operating characteristic (ROC) curve example for Problem 2. \n",
    "A Decision Tree learner is Cross Validated on $T$. \n",
    "Then the best learner's ROC AUC performance is evaluated on $T_s$ data.\"\"\"\n",
    "title_str = '\\n'.join(wrap(title_str))\n",
    "plt.title(title_str, y=1.03, fontsize = 13)\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get HyperGeometric PMF\n",
    "\n",
    "Given a RF model with $N$ features of which only $K$ are informative and $n$ splits _without_ replacement or $n$ _draws_ from the bag of $N$ features... \n",
    "\n",
    "what is the probability of selecting an informative feature?\n",
    "\n",
    "Let's take a look at some instances of PMFs for the hypergeom  distribution $\\mathcal{H}$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import scipy modules\n",
    "from scipy.stats import hypergeom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 50\n",
    "percentage = 0.1\n",
    "[K,n] = [int(N*percentage), 30]\n",
    "\n",
    "rv = hypergeom(N,K,n)\n",
    "x = np.arange(0, n+1)\n",
    "\n",
    "pmf = rv.pmf(x)\n",
    "N,K,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "aX_train.plot(x, pmf, 'bo')\n",
    "aX_train.vlines(x, 0, pmf, lw=2)\n",
    "aX_train.set_xlabel('# of informative features selected for any give split')\n",
    "aX_train.set_ylabel('hypergeom PMF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## compute the log loss\n",
    "if we take $p_i$ the probability that y_i ==1 i.e. that belongs to class 1, then the log loss will be\n",
    " \n",
    "i.e. \n",
    "$$- \\frac{1}{N} \\sum_{i=1}^N [y_{i} \\log \\, p_{i} + (1 - y_{i}) \\log \\, (1 - p_{i})].$$\n",
    "\n",
    "Note that we have discontinuities at $y_i = 0$ and $y_i = 1$, thus we must take care not to let these values be reached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " a = np.random.randn(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eps = 1e-15\n",
    "pd.np.minimum(a,eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.np.asarray(4)\n",
    "a.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the function with numpy\n",
    "\n",
    "def log_loss_binary(actual, predicted, eps = 1e-15): \n",
    "    actual = pd.np.asarray(actual)\n",
    "    predicted = pd.np.asarray(predicted)\n",
    "    rv = None\n",
    "    # we use eps and 1-eps here to take care of the discontinuity\n",
    "    predicted = pd.np.minimum(pd.np.maximum(predicted, eps), 1-eps) \n",
    "    \n",
    "    rv = - (pd.np.sum(actual * pd.np.log(predicted) + (1 - actual) * pd.np.log(1 - predicted)) ) / actual.size\n",
    "    \n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_range  = pd.Series(pd.np.linspace(0,0.5, 200)).to_frame()\n",
    "input_range.columns = ['x']\n",
    "input_range.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_range['y'] = input_range.apply(lambda x:log_loss_binary(1,x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# plot histogram on cross validated scores\n",
    "\n",
    "title_str = \"Values of $log\\_loss( {\\hat{p_i}} )$ where ${\\hat{p_i}} = P(y_i=1 \\mid \\Theta)$.\"\n",
    "input_range.plot(x='x',y='y',ax=ax)\n",
    "\n",
    "plt.title(title_str, fontsize=18)\n",
    "\n",
    "plt.xlabel(\"$ p_i $ value\".format())\n",
    "plt.ylabel(\"Log-loss Score\".format())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
