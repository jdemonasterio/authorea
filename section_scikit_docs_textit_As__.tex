
\section{scikit-docs}
	
\textit{As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.}

	\textit{each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. [Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.] }
	
	
	Hyper-parameters for tree models are in general based on:
	Max depth. Number of features sampled at each decision node. Bootsrtapped construciton of trees. The split measurement criteria. Leaf size or total nubmer of sample per leaf (note that this is a measure of tree depth).. 
	
	Random Forests' tuning parameteres are the same that those in tree learners. The only difference is in the number of in the total number of tree learners used to build the forest.
	
	There is a tradeoff in bias-variance when varying the different hyperparameters. 

