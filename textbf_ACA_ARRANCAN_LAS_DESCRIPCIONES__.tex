\textbf{ACA ARRANCAN LAS DESCRIPCIONES TECNICAS Y TEORICAS DE CADA CLASIFICADOR}

Notar que todo lo que esta en italica es tipo highlighter de textos de otros autores :)
%NOTAR QUE TO LO QUE ESTA EN ITALICA ES TIPO HIGHLIGHTER DE OTROS TEXTOS DE OTROS AUTORES}



\section{Classifier : Random Forests}


%\section{Hastie Tibshiranie Friedman}

\section{Leo Breiman}
\textit{
Random forests are a combination of tree predictors
such that each tree depends on the values of a random
vector sampled independently and with the same
distribution for all trees in the forest.


The generalization error for forests converges a.s. to a limit
as the number of trees in the forest becomes large

The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the
forest and the correlation between them. Using a random selection of features to split each node yields
error rates that are more with respect to noise.

Internal estimates monitor error, strength, and correlation and these are used to show
the response to increasing the number of features used in the splitting. Internal estimates are also used to
measure variable importance.
d}



Random subspace: each tree is grown using a random selection of features.

Given $K$ number of trees, let $\Theta_k$ encode the parameters for the $k$-th tree and $h(\textbf{x},\Theta_k)$ the corresponding classifier. Let $N$ be the number of samples in the training set. The creation of a random forests involves an iterative procedure where at each $k$ step $\Theta_k$ is drawn from the same distribution but independently of the previous parameters $\Theta_1, \ ..., \ \Theta_{k-1}$ created at previous steps. 

Bagging: to grow each tree a random selection (without replacement) on the training set is used. 

Random split selection: choose a random split among the $K$ best splits. $\Theta$ will be encoded by a vector of randomly drawn integers from 1 to $M$ which is part of the model's hyperparameters.

Let $\{h_k(\textbf{x})\}_{i=1}^K$  \footnote{There is an abuse of notation by noting trees as $h_k(\textbf{x}$ and not $h_k(\textbf{x}, \Theta_k)$ } be a set of classifying trees and let $I$ denote the indicator function.  Define the margin function as

\begin{equation} \label{eq:rf-marginFun}

 mg(\textbf{x},\textbf{y}) = \frac{1}{K}  \sum_{k=1}^K I(h_k(\textbf{x}) = \textbf{y})  
 - max_{j\neq \textbf{y}}(\frac{1}{K} \sum_{k=1}^K I(h_k(\textbf{x}) = j) ) 

\end{equation}

The function measures, in average, how much do the trees vote for the correct class in comparison to all other classes and for this classifier, the generalization error will be given as 
$ P_{\textbf{x}, \textbf{y} }(mg(\textbf{x}, \textbf{y}) < 0) $.


\subsubsection{Proof}
$$ \lim_{K\to\infty} \frac{1}{K} \sum_{k=1}^K I(h_k(\textbf{x}) = j) \ =   \ P_\Theta(h(\theta,\textbf{x}) = j) $$
