\chapter{Ensemble Methods and the Naive Bayes Classifier}\label{ch:ensembleMethods}


\section{Classifier: Decision Trees}\label{section:decision_trees}


% The models builds a
Decision trees are models that can be understood as a tree-like graph, where each node in the tree acts as a binary decision rule to which any input data sample can comply or not.
The rules are built as linear (or similar) partitions over feature space and they act on the value of a single feature, for every sample.
%properties of the values for each feature.

At any node, the model creates a binary partition of the data by evaluating a sample $s$ in the rule.
This decision forwards it to one of the possible remaining partitions by checking if $X_i(s) \in U$, where $X_i$ is the feature used in the node's rule and $U$ is a subset of said feature's space.

For numerical features, the input space $X$ will be partitioned into $L$ and $R$ where $L$ takes the form $(-\infty,c]$. The value $c \in \mathbb{R}$ is a number predefined by the rule itself.
In the same way, for categorical features $L$ will be a subset of the possible categories of that feature.

By iterating this process over more and more nodes, a tree defines a partition of feature space in multiple regions $A_1,\ldots,A_K$.
Each region will have an associated value $c_k$ in such a way that the tree's predicted output $\hat{y}$, for a sample $x$, is $c_k$, when the sample belongs to $A_k$.

Here $c_k$ will be one of the possible values taken by the target variable $y$ in the training set.
And by the way trees were built, each $A_k$ is a hyper-rectangle in feature space.

\subsection{Decision Trees Formulation}\label{subsection:decision_trees_formulation}
In short, the learner can be characterized by the following formula:
\begin{equation}
\label{eq:decisionTreeModel}
h(X) = \sum_{k=1}^K c_k I(X \in A_k)
\end{equation}

where $c_k$ is the value that our model estimates for samples in the $A_k$ region.
Both of these will have to be learned by the model in the optimization procedure. %by minimizing its loss function

By the principles given above, the algorithm will need to determine an ``optimal'' way to split a set of samples, that flow through a decision node.
The \textit{goodness of split} will be measured by specific tree measures called \textit{node impurity measures}.
%them according, in order to optimize a loss metric.

%at each node
Most variations for this machine learning model build rules (tree nodes) in a sequential, greedy, fashion, where node impurity measures are locally optimized at each node to decide on which is the best splitting value.
The reason for doing this is because the construction of optimal binary decision trees is NP-Complete~\cite{decisionTreesNP}.
Doing otherwise would result in an algorithm whose computational complexity is infeasible.

At any splitting node we have to find the \textit{best} feature $X^p$ and value split $t$ for which to partition the data in
$$A_L = \{x \in \mathcal{T} \ / \ x^p \leq t \} $$
and
$$A_R = \{x \in \mathcal{T}\ / \ x^p> t \} $$.
Let $N_l$ and $N_r$ be $|A_L|$ and $|A_R|$
 respectively.
 Then, to quantify the \textit{best} feature for this split the algorithm minimizes:

%\frac{1}{N_{left}}
%\frac{1}{N_{right}}

\begin{equation}
\label{eq:decisionTreeGreedyOptimization}
%\begin{split}
\min_{p,t} \big[ \min_{c_L }  \frac{1}{N_l}\sum_{x \in A_L(p,t) } L(y,c_L)    \ +  \min_{c_R}  \frac{1}{N_r}\sum_{x \in A_R(p,t) } L(y,c_R) \big]
%\end{split}
\end{equation}

where $y$ is the target associated to our sample $x$ and $L(\cdot)$ is the loss function we have used to measure the quality of our split.
Note that this can be done efficiently for a wide range of loss functions since the minimization can be done for each feature independently.

A tree is then grown in an iterative way from the top down\footnote{In this context the \textit{top} of a tree refers to the root of the tree.}, estimating the appropriate parameters at each rule split.
All of the training set's samples would start at the top (the root node) and then travel down through the trees branches, in accordance to their fulfillment or not of each node's rule.
A branch of the tree would stop growing once all samples at a node belong to the same target class.

Finally, we would have that the tree's leafs are the partition subsets over the input data and once a learner is fit, predicting targets for new samples is straightforward: the prediction of their target class will be the value given after traveling the sample down to its corresponding leaf node.

To illustrate this method, an instance is show in \cref{fg:rf-treeFigure}.
This classification tree example is built for the two class problem of gender prediction using data from CDRs:
%[.{\textit{Woman}}]
\smallskip
\begin{figure}[h]
\Tree[.{ $Calling\_Volume \leq 23$ } [.{$Province \in \{ San Luis, Chubut \} $} [.{$Time\_Weekend \geq 16$} [.{\textit{M}} ] [.{\textit{F}} ] ]
[.{$Calls\_Weekdays \leq 48$}
[.{ $Time\_Weekday \geq 17$} [.{\textit{M}} ] [.{\textit{F}} ]] [.{\textit{F}} ] ] ]
[.{$Calls\_Mondays \geq 2$} [.{$Province \in \{ Chubut, Cordoba \} $} [.{\textit{M}} ] [.{\textit{F}} ] ]
[.{\textit{M}} ]]]
\caption{Classification tree example built for a toy gender prediction problem, using CDR available data.}
\label{fg:rf-treeFigure}
\end{figure}

\smallskip


%[.{\textit{M}} ] [.{\textit{F}} ]

\subsection{Impurity Measures}\label{subsection:decision_trees_impurity_measures}


The most used metrics to build each rule are the \textit{Gini impurity measure} and the \textit{entropy} or \textit{information gain} criterion.
The former minimizes the  misclassification error in the output sets resulting from the partition.
It optimizes the accuracy of the model as the resulting value of tagging all resulting samples with the majority label in that partition.

The latter measure optimizes for information entropy, which is analogous to minimizing \textit{Kullback-Leibler divergence} of the resulting sets with respect to the original set previous to the split.

In addition, we have the misclassification measure.
Following is listed formulation of the three most used impurity measures for classification trees:

\begin{itemize}
	\item Gini index: $ \displaystyle \sum_{k\neq k'} \hat{p}_{jk} \hat{p}_{jk'}  = \sum_{k=1}^{K} \hat{p}_{jk} (1 - \hat{p}_{jk}) $
	\item Cross-entropy: $ \displaystyle \sum_{k=1}^{K} -\log(\hat{p}_{jk})\hat{p}_{jk} $
	\item Misclassification error: $ \displaystyle \frac{1}{N_j} \sum_{x \in R_j} I(y\neq c_j) = 1 - c_j $
\end{itemize}

In some cases, a reweighted Gini index can be performant.
In this version a loss matrix is used to multiply each summands of the measure.
By this, the matrix reassigns weights to different cases of misclassification and this become specifically practical when we have that a sample from class $k$ incorrectly assigned to class $k'$ is more valuable than other misclassifications.

Let $L \in \mathbb R_{\ge 0}^{K \times K}$ where $L_{(k,k')}$ is the cost of misclassifying a class $k$ sample into $k'$.
Naturally we will have $L$ as a null diagonal matrix.
As such, the Gini index's summands will take the reweighed form $L_{kk'} \hat{p}_{jk} \hat{p}_{jk'}$.

For the binary (two class) case these measures can be expressed in simpler terms.
If we consider $p$ to be the probability of success, then we have

\begin{itemize}
\item Misclassification binary error: $1 - max(p, 1-p)$ \label{it:decisionTreeCostFunctions}
\item Gini binary index: $ 2p(1-p) $
\item Binary cross-entropy: $ -\log(p)p - \log(1- p)(1-p) $
\end{itemize}



\subsection{Hyperparameters}\label{subsection:decision_trees_hyperparameters}

For the decision tree model, the process of iteratively partitioning the samples in splits continues until a predefined tuning parameter stops the optimization or when the node is pure i.e.\ there is only a single target class for all samples at the node.

The hyper-parameters for this model include the length of the tree, the splitting rule threshold and the node impurity measures.
From the descriptions previously given, we can list these directly:

\begin{itemize}
\item Max depth of the tree, or the allowed levels of splits.
\item The criteria or measure used to select the best split feature at each node.
\item The leaf size or the total number of minimum samples allowed per leaf.
Note that this is a related to limit on branch depth.
\item Number of features selected to decide on the best split feature at each node.
\end{itemize}


Intuitively, it is natural to find that trees of longer depth will overfit the data since more complex interactions among variables will be captured by refining the partition on input space.
A trivial example is to allow a tree to grow fully in depth to later assign to each sample in the training set its own self-contained region.
This yields a model with virtually zero bias yet with a very high prediction error.
In \cref{figure:dtree_overfit_problem_2} we give an example of an overfit decision tree on the CDR dataset for \cref{target2} where the overly-complex model produces poor generalization error.
%Recall that this was described in \cref{figure:dtree_overfit_problem_2}.
Here the training scores and CV scores were compared on models with increasing tree depth which at some point started to overfit the training set by increasing the gap of the CV and training errors.

At the same time, having a tree which is too shallow in depth will result in most cases in a biased algorithm.
This is because it results in an overly simple model incapable of correctly assigning labels.
We must then consider that the depth of a tree is a measure of the model's complexity and as such one of the most important hyperparameters of our model.

Another drawback of the decision tree model is the high variance instability.
Authors point out that two very similar datasets can grow two very different resulting trees.
This is due to the hierarchical nature of the splits, where errors randomly made in the first splits will be carried onwards.
This is because once a samples has been directed through a lower branch, it will continue down through this one without reconsideration of the past errors.

For the reasons described, in this model it is important to control the depth of the trees built.
To do this, the most common method to do this grows a very large tree $T_0$ that will continue until it reaches a depth limit threshold that is very unrestrictive.
Then the tree will be pruned by removing branches and nodes to lower the model's complexity whilst at the same time trying not to compromise much of its accuracy.
More details on this can be seen in \cref{appx:sec:tree_pruning}.
For a broad characterization of decision trees and their construction in classification or regression problems, please refer to~\cite{breiman-cart84}.

\subsection{Experiment}\label{subsection:decision_trees_experiment}

We present here a run of the a decision tree over $\mathcal{T}$.
In this case the \cref{target2} was used as an example.
The tree was built using a standard configuration with a low depth of 5 and a Gini splitting criteria.
We allowed the algorithm to select any feature $X^j$ at each split decision. 
Also, we configured the minimum split threshold to be of $20$ samples.
This meant that no further splits were created when less than this number of samples was at that node.
Under this setup, the algorithm ran in approximately 15 seconds.
\cref{fig:decision_tree_actual_problem} shows a partial representation of the actual decision tree grown on \cref{target2}.
This allows us to see how the algorithm selects better features at low-level nodes, versus the deeper splits.

\bigskip

\begin{sidewaysfigure}[H]
	\centering
	\makebox[\textwidth]{\includegraphics[width=1.2\paperwidth]{figures/decision_tree/full_tree_map.png}}
	\caption{ Example of a final decision tree.
		This model was made with a cross validated procedure on \cref{target3} and the resulting tree has been pruned to fit the image.
		For every node, the samples are split into sets given by the implicit indicator function defined in the condition.}
	\label{fig:decision_tree_actual_problem}
\end{sidewaysfigure}



%\begin{figure*}[tb]
%\centering
% \makebox[\textwidth]{\includegraphics[width=.9\paperwidth]{figures/decision_tree/full_tree_map.png}}
%\caption{ Example of a final decision tree. This model was made with a cross validated procedure on \cref{target3} and the resulting tree has been pruned to fit the image. For every node, the samples are split into sets given by the implicit indicator function defined in the condition.}
%\label{fig:decision_tree_actual_problem}
%\end{figure*}

\smallskip


From \cref{fig:decision_tree_actual_problem} we can approximate which features are most important to the algorithm when deciding a node split, given by the Gini splitting criteria.
Given that in \cref{target2} we are looking for people that migrated in any direction, the choice of the tree\'s root feature is not surprising.
The mobility diameter gives an idea of a user's influence area when using his mobile phone and as such, this attribute might indicative of past migrations.
Subsequently, the usage volume of a user\'s home antenna appears as the second most important features, both during the weeknight and during the whole day.
Yet the error score out by this tree yields a poor performance.
Evaluated with the $Accuracy$ metric, we see $43.2\%$ result with this model which shows very poor prediction error over all of the samples.
With this score, noting which features were selected near the root leaves of the tree would not be very informative to the overall problem.

We decide to iterate once again on this model, but now slightly changing the hyperparameter which controls the balance of $\mathcal{T}$.
The reason for this is that with the current task, the positive and negative classes have a disproportionate balance of samples and this is reflected in the overall model\'s poor performance.
For this reason, we modified the setup of the loss function of the tree to weigh more samples from the under-represented class over the samples from the abundant group.
For each sample, it\'s loss weight was adjusted by it\'s class representation ratio.
With this, the new run scored a $57.9\%$ $Accuracy$ error score which is more than a $15\%$ increase over the previous score.
Still, this is considered a very poor model for supervised learning.

The figure for this model is shown below.

\bigskip

\begin{sidewaysfigure}[H]
	\centering
	\makebox[\textwidth]{\includegraphics[width=1.2\paperwidth]{figures/decision_tree/full_tree_map2.png}}
	\caption{ Second example of a decision tree.
		This model was made with a cross validated procedure on \cref{target3} from an artificially balanced dataset.
		The resulting tree has been pruned to fit the image.}
	\label{fig:decision_tree_actual_problem2}
\end{sidewaysfigure}


\smallskip

From \cref{fig:decision_tree_actual_problem2} the importance of the mobility diameter is more clear.
Once again the learner uses it as the first split in the data, even with balanced samples.
Recall that by the stochastic nature of the algorithm building the tree, two runs with the same hyperparameter settings and data, need not output the same trees.
In this case however, it is notable that the final tree still uses the mobility as the first split in the data, even with balanced samples.
And the same can be said of the feature representing the count of calls made from the home antenna.

This tree also performs poorly in its prediction of which users have migrated in the past.
At some point, it is reasonable to assume that balancing classes leads to a better predictive outcome.
Yet there are still various misclassifications in the predicted target classes.
If we let the tree grow deeper though, we might increase the performance of the training error as shown in \cref{figure:dtree_overfit_problem_2}.
But again, this won't help our generalization error.
So to tackle most of the disadvantages described by this technique, \textit{Random Forests} are what follow as a natural extension to this classifier.


\section{ Random Forests}\label{section:random_forests}

Random Forests are estimators that extend from constructing a group of single decision trees and then combining them to produce a single output decision.
This grouping of lots of classifieres is what is known as an \textit{ensemble}.
The objective in this construction is to focus on decision tree\'s low bias whilst controlling their overfit as much as possible.
For this, a forest of single trees will be constructed in such a way that correlations among each of the individual models is limited as much as possible.
And this is because each individual model will learn on a random subset of features or samples and then, an average of the single outputs will be used as the forest's target output to preserve single estimator's interdependence as much as possible.
There  are other different techniques that intent to construct uncorrelated trees which are combined at the output, yet the main idea is common to all.

\subsection{ Random Forests Formulation}\label{subsection:random_forests_formulation}

Let $K$ be the number of trees in the ensemble and let $\Theta_k$ encode the parameters for the $k$-th tree.
As we have mentioned before, there are various variants to the model and these variants will define the type of encoding for the $\Theta_k, k \in {1,\ldots,K}$  parameters.
For the following part, we will not specify the overall type of ensemble constructed since the  proofs are general to all of them.

To start, we define
$$h(\textbf{x},\Theta_k)$$ will be the corresponding individual classifier and we let $N$ be the number of samples in the training set $\mathcal{T}$.
The creation of a random forest involves an iterative procedure where at the $k$-th step, the parameter $\Theta_k$ is fit from the same distribution as $\Theta_j, \ j<k$, yet it is built in a way that is independent of the previous parameters $\{\Theta_1, \ \ldots, \ \Theta_{k-1} \}$. %$\Theta$ will be encoded by a vector of randomly drawn integers from 1 to $M$ which is part of the model's hyperparameters.


Let $\{ h_k(\textbf{x}) \}_{i=1}^K$\footnote{There is an abuse of notation by noting trees as $h_k(\textbf{x})$ and not $h(\textbf{x}, \Theta_k)$ } be a set of classifying trees and let $I(\cdot)$ denote the indicator function.
Define the margin function as

\begin{equation}
\label{eq:rf-marginFun}
mg(\textbf{x},\textbf{y}) = \frac{1}{K}  \sum_{k=1}^K I(h_k(\textbf{x}) = \textbf{y})
- \max_{j\neq \textbf{y}}\left(\frac{1}{K} \sum_{k=1}^K I(h_k(\textbf{x}) = j) \right)
\end{equation}


The margin function measures, in average, how much do the trees vote for the correct class in comparison to all other classes.
It is the training error of the model when using the misclassification loss.
Here the generalization error is denoted as $PE*$ and is equal to

\begin{equation}
P_{\textbf{x}, \textbf{y} }(mg(\textbf{x},\textbf{y}) <0)
\end{equation}

 It can be shown that, for $K$ sufficiently large, the generalization error under the misclassification loss converges to

\begin{equation}
 P_{\textbf{x}, \textbf{y} } ( P_{\Theta} (h(\textbf{x}, \Theta) = \textbf{y}) - \max_{j \neq \textbf{y}} P_{\Theta} (h(\textbf{x}, \Theta) = j) < 0)
 \end{equation}

almost surely for all sequences of parameters $\Theta_1,\Theta_2, \ldots, \Theta_k,\ldots$

This proof can be found in \cref{appx:sec:rforest_margin_function_convergence}.


\subsection{ Experimental comparison to Decision Trees}\label{subsection:random_forests_comparison_trees}

From what is formulated in \cref{subsection:random_forests_formulation} and in order to compare how this model improves over the Decision Tree model, we ran two experiment setups on a Random Forest learner for the simple task in \cref{target2}, using an $F1$ score.
This was a cross validation procedure of 10 folds for each configuration, where each experiment optimized the score on a different hyperparameter.

For the first, we evaluate the forest's scores on a maximum depth variation for the trees.
And on the second one, we try to illustrate the point of \cref{subsection:random_forests_formulation} by considering forests of increasing size.
As a default configuration, we set bothe the trees' max depth and the number of trees to 10.

Also, we pre-configure a balanced weighting of positive and negative samples in the loss function, where the each sample is weighted by the reciprocal of the class's ratio to the whole dataset.
Finally, the impurity measure  we used was the is the Gini index for the splitting criteria at each tree's node.

Cross validation procedures ran for 1412.8 and 2506.1 seconds respectively. In both of them we excluded any user attributes that informed their home state and current endemic condition, in the time period $T_1$.

The experiment's scores outcomes across their hyperparameter values were graphed in \cref{fig:random_forest_validation_curves_depth_trees,fig:random_forest_validation_curves_num_trees}.
For both, the cross-validated and training set scores were compared. In this way, for each hyperparameter value, the $1-ROC AUC$ score is shown.
In this way, having a lower score in the graphic means a better model was reached.


\begin{figure}[h!]
	\begin{center}
	 \includegraphics[width=1\linewidth]{figures/random-forest/validation_curve_forest_depth_series_f1}
		\caption{ Validation curve on the tree-depth hyperparameter values for the Random Forest  learner.
			The mean CV scores for the $1 - ROC AUC$ score is shown for a CV run experiment on \cref{target2}}
		\label{fig:random_forest_validation_curves_depth_trees}

	\end{center}
\end{figure}

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1\linewidth]{figures/random-forest/validation_curve_forest_num_trees_series_f1}
			\caption{ Validation curve on the number of trees hyperparameter values, for the Random Forest  learner.
				The mean CV scores for the $1 - ROC AUC$ score is shown for a CV run experiment on \cref{target2}}
		\label{fig:random_forest_validation_curves_num_trees}
	\end{center}
\end{figure}

Recall how this relates to \cref{figure:dtree_overfit_problem_2}, in which a tree's training error always improves as a result of an increase in the tree's depth.
Yet the tree's prediction error at one point suffers and deteriorates.
From what we can see in \cref{fig:random_forest_validation_curves_depth_trees}, the cross validated error is always decreasing for increasing tree depth.
A very slight deceleration in the decrease of the cross validated error can be seen for tree depth bigger than 9.
Yet the scores keeps improving and don't worsen.
This is different to \cref{figure:dtree_overfit_problem_2} where if we let the tree grow deeper, at one point we only increase the performance of the training error.
Here, we are possibly improving the generalization error, as indicated by the CV estimation.

 On the other hand, it is interesting to note that there is an insignificant decrease in the training and CV scores when the number of trees is increased.
For this case, there is virtually no change in model error by improving bias or variance.
 An explanation for this might that we are setting a default tree depth of 10 levels already when running this experiment. And a tree configuration of this type might then be capturing all the possible model complexity to this dataset.
 Another plausible explanation for this is that, for this task, the forest's performance rapidly converges to the generalization error with a minimum of 10 trees and that adding more trees doesn't certainly improve on the predictions.


At their optimal configuration, both setups reached CV score averages of at least 0.75, with a maximum of 0.81 for the forest
with trees of 16 levels.
The optimal number of estimators for the CV and the training sets were not the same though, were 139 was the best configuration for the training score whilst 133 was the best average for the CV set.
The relative difference in these best scores was of 3.3\%.

At the same time, the relative difference of the best CV and training scores, for the maximum tree depth experiment was of 9.5\%.
This percentage difference signals that the max depth hyperparameter is significantly more sensible to over-fitting than the number of trees parameter.


\subsection{Predictive error bounds}

Random Forests are built upon a bag of weaker classifier, of which each individual estimator has a different prediction error.
To build an estimate of the generalization error on the ensemble classifier, these individual scores and the relationship between them must be measured.
In this sense, the \textit{strength} and \textit{correlation}of a Random Forest must be analyzed to arrive on an estimate of the generalization error.

%\begin{lemma}
%Given two line segments whose lengths are $a$ and $b$ respectively there is a
%real number $r$ such that $b=ra$.
%\end{lemma}

\begin{theorem}
There exists an upper bound for the generalization error.
\end{theorem}


\begin{proof}
	Define $\hat{\jmath}( \textbf{x},\textbf{y})$ as $\argmax_{j \neq \textbf{y}} P_{\Theta}(h(\textbf{x}) = j)$ and let the margin function for a random forest (not a group of classifiers) be defined as

	\[\label{eq:rf-marginFunRf}
	mr(\textbf{x},\textbf{y}) = P_{\Theta}(h(\textbf{x}) = \textbf{y}) - P_{\Theta}(h(\textbf{x}) = \hat{\jmath})
	\\
	= \Expect_{\Theta} \left[ I(h(\textbf{x},\Theta ) = y ) - I( h( \textbf{x},\Theta ) = \hat{\jmath} ) \right]
	\]


	%\[%\]

	%\]

	Here the margin function is described as the expectation taken over another function which is called the \textbf{raw margin function}\label{eq:rf-rawMarginFun}.
    Intuitively, the raw margin function takes each sample to be $1$ or $-1$ according to whether the ensemble classifier can correctly classify or not the sample's label, given $\Theta$.

	With these definitions, it is straight to see that

        	%\[%\]
            \begin{equation}
            {mr( \textbf{x},\textbf{y} )}^2 = \Expect_{\Theta, \Theta'} \left[ rmg( \Theta,\textbf{x},\textbf{y} ) \ rmg(\Theta',\textbf{x},\textbf{y} ) \right]
            \end{equation}


	This in turn implies that
            \begin{equation}\label{eq:rf-marginFunVar}
	            \begin{split}
	            var(mr) & = \Expect_{\Theta, \Theta'}
	            \left[
	            cov_{\textbf{x},\textbf{y}}
	            (rmg(\Theta,\textbf{x},\textbf{y} )rmg(\Theta',\textbf{x},\textbf{y} ))
	            \right] \\
	            & = \Expect_{\Theta, \Theta'}
	            \left[
	            \rho(\Theta, \Theta')\sigma(\Theta)\sigma(\Theta')
	            \right]
	            \end{split}
            \end{equation}

	where $ \rho(\Theta, \Theta')$ is the correlation between $rmg(\Theta,\textbf{x},\textbf{y})$ and $rmg(\Theta',\textbf{x},\textbf{y})$, and $\sigma(\Theta)$ is the standard deviation of $rmg(\Theta,\textbf{x},\textbf{y})$.
    In both cases, $\Theta$ and $\Theta'$ are given.% to be fixed.

	Equation \cref{eq:rf-marginFunVar} in turn implies that

	\begin{equation}\label{eq:rf-varianceBound}
            \begin{split}
            var(mr) & = \overline{\rho} {(\Expect_{\Theta}\left[ \sigma(\Theta)\right] )}^2 \\
            & \leq \overline{\rho} \Expect_{\Theta} \left[ var(\Theta) \right]
            \end{split}
            \end{equation}

	where we have conveniently defined $\overline{\rho}$ as

	\begin{equation}\label{eq:rf-meanCorrelation}
            \frac{\Expect_{\Theta, \Theta'} \left[ \rho(\Theta, \Theta') \sigma(\Theta) \sigma(\Theta')\right]}
            {\Expect_{\Theta, \Theta'} \left[ \sigma(\Theta) \sigma(\Theta')\right]}
            \end{equation}

	Note that this is the mean value of the correlation.

	Let the strength of the set of weak classifiers in the forest be defined as

            \begin{equation}\label{eq:rf-strength}
            s = \Expect_{\textbf{x},\textbf{y}} \left[ mr(\textbf{x},\textbf{y} ) \right]
            \end{equation}

	Assuming that $s \geq 0$ we have that the prediction error is bounded by
            \begin{equation}\label{eq:rf-predictiveErrorBound1}
            PE^* \leq var(mr)/s^2
            \end{equation}
	by Chebyshev's inequality.
	On the other we also have that

            \begin{equation}\label{eq:rf-expectedVarBound}
            \begin{split}
            \Expect_{\Theta} \left[ var(\Theta) \right] & \leq \Expect_{\Theta} {\left[ \Expect_{\textbf{x},\textbf{y}}\left[ rmg(\Theta,\textbf{x},\textbf{y})  \right] \right]}^2 -s^2 \\
            & \leq 1-s^2
            \end{split}
            \end{equation}



	We can use \cref{eq:rf-varianceBound}, \cref{eq:rf-predictiveErrorBound1} and \cref{eq:rf-expectedVarBound} to establish the upper bound for the prediction error we are looking for

            \begin{equation}\label{eq:rf-PEBound}
            PE^* \leq \overline{\rho}\frac{(1-s^2)}{s^2}
            \end{equation}

\end{proof}


This bound on the generalization error shows the importance of the strength of each individual weak classifier in the forest and the correlation interdependence among them.
The author of the algorithm~\cite{breiman-randomforests} remarks here that this bound may not be strong.
He also puts special importance on the ratio between the correlation and the strength $\frac{\overline{\rho}}{s^2}$ where this should be as small as possible to build a strong classifier.


\subsection{Binary Class}\label{subsection:random_forests_binary_class}

In the context of a binary class problem, where the target variable can only take two values, there are simplifications to the formula \cref{eq:rf-PEBound}.
In this case, the margin function takes the form of $2 P_{\Theta}(h(\textbf{x}) = \textbf{y}) -1$ and similarly the raw margin function results in $2 I(h(\textbf{x}, \Theta) = \textbf{y}) -1$.


The bounds prediction error bounds derived in \cref{eq:rf-predictiveErrorBound1} assume that $s >0$ which in this case results in
\begin{equation}
\Expect_{\textbf{x},\textbf{y}} \left[ P_{\Theta}(h(\textbf{x}) = \textbf{y}) \right] > \frac{1}{2}
\end{equation}


Also, the correlation between $I(h(\textbf{x}, \Theta) = \textbf{y})$ and \ $I(h(\textbf{x}, \Theta') = \textbf{y})$, denoted $\overline{\rho}$ will take the form

\begin{equation}
 \overline{\rho} = \Expect_{\Theta,\Theta'} \left[ \rho \left( h(\cdot{},\Theta) ,h(\cdot{},\Theta') \right)  \right]
 \end{equation}



%


\subsection{Other Notes on Random Forests}\label{subsection:random_forests_other_notes}

One benefit of building Random Forest classifiers is that the algorithm easily increases the prediction error of a group of estimators by randomly building each of these in a way that decreases the variance of the overall model whilst trading a small loss in bias.

The model is also robust to the introduction of a number of noisy features.
If the ratio of informative to non-informative features is not extreme, selecting $m$ features at random at each split will mean that in most cases, splits will be made on those informative features.
Note that in any given tree, the probability of drawing at least one informative feature in a split is still very high.
This is because it follows a hyper geometric distribution $\mathcal{H}(P,j,l)$ with $l$ draws from a total population of $P$ features and only $j$ informative ones.

The depth of growth for each tree is another important tuning parameter.
We must choose it correctly by assessing the model's performance across different values for $m$.
A deep tree will tend to overfit the data by partitioning input space to fit the training data.
This effect will counter the overall reduction in variance of the forest and thus increase the generalization error of our algorithm.

%Therefore controlling the maximum allowed growth for the base learners will be important to improve the performance of the model.

In addition, the algorithm benefits from a heuristic to measure variable importance, where a special modification in the way forests are built allows this to happen.
The idea for this is that at each split we can measure the gain of using a certain variable for the split versus not using it.
Given a candidate feature $X_j$ to be analyzed and for every node in a tree where a split is to be done, we compare the improvement in the split performance, as measured by some loss function, with and without $X_j$.
These results are recorded and averaged across all trees and all the split scenarios to have a score for the feature.
With this, the features with highest scores can be thought to be the most informative variables of the model.

\subsection{Experiments}\label{subsection:random_forests_experiments}

In this subsection, we explore on the task described on \cref{target4} by fitting random forest learners.
Here, our cross validation procedure explored multiple combinations of hyperparameters that we predefined Let $\alpha_i \forall i \in {1,\ldots,j}$ be the list of our $j$ hyperparameters.

Denote all possible combinations by $A_1 \times A_2 \ldots \times  A_j$, where each hyperparameter $\alpha_i$ will take all possible values in the predefined sets $A_i$.
In practice, we must be careful with the size of these combinations since the full cross validation procedure is costly for the Random Forest algorithm.
The learner has $j \geq 10$ and for each $A_i$ it is normal to have more than 10 possible values.
Thus the search will span a space of more than ten thousand combinations.
Moreover, each combination has to be cross validated on the number of folds we define, creating a very large amount of iterations during this search.
These fits will take computer resources such as memory and CPU time.
So we have to limit ourselves to search over a limited amount of values.

Also, if we were to use commodity hardware with standard tools for this procedure, the experiment would take weeks and, given our dataset size, it would not fit into a standard computer's memory of less than 12GB of memory.

For these reasons, we take on the task with a specialized server with a 16 core CPU and 72 gigabytes of available RAM.\@
The system runs a UNIX based OS and our algorithms were scripted in~\cite{python3.5}.
We used the specialized machine learning package~\cite{graphlab} to handle the parallelization and the distribution of system resources.
As a side note, we can't stress enough how convenient this package is for our purposes.
It handles most of the complex task of maxing out the system resources to perform as fast as possible in the cross validation procedure and in the construction of the main $X$ dataset.
Also, it has an \textit{out-of-the-box} memory management for large datasets.
With this, we can natively handle data that is larger than the amount of memory used by the server, requiring minimal user knowledge to tune.
\cite{graphlab} has allowed us to conveniently run these long experiments without risk of failure due to lack of memory and without any prior expert domain knowledge in multi-threading or parallel computing.

For this case, we only considered migrant people that are non-endemic at $T_1$, but have been during $T_0$.
In the general procedure, we cross validated the same possible hyperparameter configurations, where these spanned the following items:

\begin{itemize}\label{list:random_forest_grid_search_params}

  \item Every tree's maximum depth --- [max\_depth]
  \item If the loss function will balance the weights of the positive and negative classes --- [balanced]
  \item The maximum number of trees to grow in each fit--- [max\_iterations]
  \item The splitting criterion at each node which can be either by the Gini Index or the Entropy condition--- [split\_criteria]
  \item The sample percentage of features which are available to use at each split --- [max\_features]
  \item The minimum number of samples required to split a node --- [min\_split]
  \item The scoring function used to evaluate the learner in the CV evaluation --- [CV score]
  \item The minimum reduction in the loss function required to split a node  --- [min\_loss]
%\caption{Available hyperparameters for the Random Forest classifier, with their corresponding code names.}
\end{itemize}


The full experiment ran two different tasks, where each was characterized by the attributes used for our $X$ dataset.
In the first one, the procedure ran with all available features and in the second one we separated the features that had a high correlation, bigger than $0.3$, with the target variable.
This means we removed all of the features that counted the user\'s vulnerable neighbors for a given month, segmented by the direction of the call.\footnote{For a complete description on the definition of these features, please refer to \cref{tab:data_example}.}
Recall that all of these features were central in the construction of the risk maps in \cref{section:riskmaps}.
The color of each antenna's circle represented the amount of vulnerable users, for that given month, when looking at the incoming or outgoing calls.
Then, these feature's correlation to the target variable are indicative, up-to a certain point, of the predictive power of them.

For this experimentation, we recorded the outcome of the cross validation procedures in terms of different scoring metrics, the run-time, the hyperparameters chosen and the top ten best features as given by the algorithm.
The summary of these results is given in \cref{tab:random_forest_big_experiment_results}.

\begin{table}[!htb]
\caption{Table of best results comparing two 5-fold full grid cross validation procedures on a Random Forest Classifier fit for \cref{target4}.
	The scores, best hyperparameter values and run-time are shown for both experiments.}
\label{tab:random_forest_big_experiment_results}
\centering
\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill} }  l l l l l }
%{|p{2cm}|p{2cm}|p{1.5cm}|p{1cm}|p{1.5cm}}
\toprule
Result & Experiment 1 & Experiment 2 \\
\midrule
CV $F1$ score           & 0.273  &  0.196 \\
CV $Accuracy$ score     & 0.880 & 0.903   \\
CV $Precision$ score    & 0.175 &  0.123 \\
CV $Recall$ score       & 0.618 &  0.480 \\
CV $Logloss$ score     &  0.337 & 0.391 \\
CV $ROC AUC$ score    &  0.843 & 0.848 \\
max\_depth     & 12 &  12 \\
max\_iterations          & 150 &  200 \\
balanced        & True & True  \\
split\_criteria          & Gini &  Gini \\
min\_loss  & 10 &  1 \\
Running time (s)        & 2030 &  2064 \\
 max\_features (\%) & 50 & 80  \\

\bottomrule
\end{tabular*}
\end{table}


From the result's table, it's relevant to note that both optimal learners chose the Gini splitting criterion as best, with tree's of a relatively high depth with 12 levels.
A similar situation happened with the number of trees grown; best configurations had more than 100 trees with the second experiment having an optimal number of 200.

Bigger differences appear in the choice of the minimum loss reduction and the features sampled per split.
There are no clear indications as to which configuration is better.
And, whilst having a higher level of randomization in the growth of the trees hints to a less overfitting learner, this difference is not that notable in the CV scorings.

For most of the metrics used to evaluate the cross validation procedure, the scores look very similar.
It is notable that the second experiment is slightly better in the accuracy, precision, Logloss and ROC AUC scores, with a maximum difference of 5\% for all of them.
Yet at the same time the second experiment's performance drastically drops for the recall and precision.
The same occurs accordingly for the $F1$ measure.
We see the learner is losing performance in correctly tagging the user's who have migrated from the endemic region.
This high relative difference hints to how much of an impact is characterized by removing the features for the second experiment.
We can suspect that there's enough missing information in those features to reduce the predictive power of the learner.

In both experiments, its important to note though that the elected hyperparameters are not optimal in any strict sense.
At most, they can hint to the best possible configuration.
Given the large combinatorial nature of this process, there is no guarantee of optimality in this search.
To a greater degree were only covering as much space as our time and our computational systems allow.


We also recorded the model's selection of best features, using the explanation outlined in \cref{subsection:random_forests_other_notes}.
This outcome is presented in \cref{tab:random_forest_big_experiment_best_features}, where the top 10 features for each experiment are shown.

\begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}llll@{}}
            \toprule
            & \multicolumn{3}{l}{Top Features}                                                                   \\ \midrule
            \multicolumn{1}{l|}{\multirow{4}{*}{First Experiment}}  & Call Count Antenna\_0    &   Mobility Diameter   & Mobility Diameter Weeknight \\
            \multicolumn{1}{l|}{}                                   & Call Count Weeknight Antenna\_0 & TimeWeekDay Out Month\_09       & CallsWeekDay In Vuln Month\_08 \\
            \multicolumn{1}{l|}{}                                   & TimeWeekDay Out Month\_08       & State Hidalgo          & CallsWeekDay In Month\_08      \\
            \multicolumn{1}{l|}{}                                   &                                 & TimeWeekDay Out Month\_12       &      \\ \midrule
            \multicolumn{1}{l|}{\multirow{4}{*}{Second Experiment}} & Mobility Diameter               & Call Count Antenna\_0           & Mobility Diameter Weeknight    \\
            \multicolumn{1}{l|}{}                                   & Call Count Antenna\_1           & Call Count Weeknight Antenna\_0 & TimeWeekDay Out Month\_12      \\
            \multicolumn{1}{l|}{}                                   & TimeWeekDay In Vuln Month\_09   & TimeWeekDay Out Vuln Month\_08  & TimeWeekDay Out Month\_09      \\
            \multicolumn{1}{l|}{}                                   &                                 & TimeWeekDay In Vuln Month\_08   &                                \\ \midrule
        \end{tabular}%
    }
    \caption{A representation of the top features that resulted from the Random Forest experiment on \cref{target4}. }\label{tab:random_forest_big_experiment_best_features}
\end{table}


The top features list shows us the importance of mobility diameter in both cases.
Where for weeknights and for the whole week, the user's mobility is  relevant to detect his past endemic condition.
As expected, in both cases we see that there are a number of features logging vulnerable interactions of users living in non-endemic regions.
This is in line with what we hypothesized in the construction of the risk maps in \cref{section:riskmaps}.
The direction of the call and the calls' duration or number of occurrences are both important predictors of this model.
Given that this best feature methodology is an heuristic, there doesn't seem to be any clear indication over which type of CDR interaction has better predictive power.
Yet there is a preference for interactions logged in the earlier months, August and September,
This might be the case where the algorithm is picking up on the most recent migrations, given that our time period of analysis $T_1$ starts in August.

The analysis here presented is relevant in answering the question of long-term migrations.
High scores for these experiments show that there is value in CDR data to predict long-term migrations between two regions.
Also, top features are in line with the assumptions on which the risk maps were constructed.



\section{Boosting Models}\label{section:gradient_boosting}

Boosting methods are similar to additive methods such as in Random Forests because they combine the predictions of weak learners to output the combined model's prediction.
The full model is grown sequentially from base estimators such as decision trees, but the difference is that each new iteration tries to reduce the overall bias of the combined estimator.
This provides greater predictive power when the base model's accuracy is weak.
But care must be taken to control the increase in variance.

\subsection{Ada Boost}
%~\cite{schapire-adaBoost}

In the Ada Boost variation of ensembles, each iteration builds a new weak learner which is set to improve on the samples misclassified by the previous ensemble of weak learners.
The new learner will not be uncorrelated and this is an important distinction of this model.
Weights are used by the algorithm to rank the samples by misclassification importance: a sample with higher misclassification rate will receive a stronger weight.
The name of the algorithm is derived from the term \textit{adaptive boosting}, where sample weights are updated at each iteration.

Tuning parameters in this algorithm are a superset of those used in the base learners.
As an addition, the number of steps \textit{boosting} the ensemble is added, among other new hyperparameters.

This chained construction of weak learners has implications on the computational complexity of the optimization.
Base learners are not constructed independently and as such, the parallelization of this algorithm becomes limited.
At the same time, the sequential optimization of learners improving on the one before marks a \textit{greedy} minimization approach of the general loss function.

These properties underline a substantial difference to Random Forests where base learners are built as uncorrelated as possible and where optimization can be performed globally, which allowed for a significant runtime improvement by parallelizing the algorithm.

\subsection{Formulation}\label{subsection:adaboost_formulation}

Let
\begin{equation}\label{eq:adaBoostTrainingError}
\overline{err} = \frac{1}{N} \sum_{i=1}^{N} I(y_i \neq \hat{y_i})
\end{equation}

denote the training set's misclassification error.
As usual, $N$ is the amount of samples in our dataset, $y$ is our target variable and $\hat{y}$ is our model's prediction for the target, given the samples.
We also take
\begin{equation}
\Expect_{X \ Y} [ I(Y \neq \hat{Y}(X)) ]
\end{equation}

to be the expected error rate of the model on the true, unknown distribution of the data.

Let $m$ index the iteration number in the Ada Boost algorithm.
Set $w^{(m)}_i$ to be the $i$-th sample's weight at this iteration.
We will initialize $w$ to be equiprobable at $w^{(0)}_i = \frac{1}{N} \forall i$.

Let $h(x,\theta)$ denote a weak learner.
With this notation, we assume the loss function to have a domain in the input feature space and in the parameters defining the learner.
Naturally these will depend on the problem structure and on the base learner.

Then Ada Boost's model takes the following form:
\begin{equation}\label{eq:adaBoostModel}
\hat{y}(x) = \sum_{m=1}^{M} \gamma_m h(x,\theta_m)
\end{equation}

where $M$ is the model's hyperparameter indicating the amount of weak learners and thus the amount of iterations.
Here, each $\theta_m$ will encode the base learner's parameters and $\gamma_m$ will denote the weight of that weak learner in the overall model.

The algorithm's iteration will build $\hat{y}$ starting from $\hat{y_i}^{(0)}= 0 \forall i$ and at each stage we will minimize a function that tries to correct the performance of the last model.
At step $m$ we will search for $(\gamma_{m}, \theta_{m})$ where

\begin{equation}\label{eq:adaBoostIteration}
\begin{split}
(\gamma_{m}, \theta_{m}) = \underset{\gamma, \theta}{\mathrm{argmin}} \sum_{i=1}^{N} & L\big( y_i,  \hat{y}^{m}(x_i) + \gamma h(x_i,\theta) \big) \\
= \underset{\gamma, \theta}{\mathrm{argmin}} \sum_{i=1}^{N} & L\big( y_i,  \sum_{j=1}^{m} \gamma_j h(x_i,\theta_j) + \gamma h(x_i,\theta) \big)
\end{split}
\end{equation}

The greedy nature of the algorithm becomes explicit in the procedure above, where we have fixed all the previous optimized values for $\gamma_j$ and $\theta_j$.

Ada Boost was first derived in~\cite{schapire-adaBoost} and it was introduced with a specific minimizing function.
The general version here presented allows the use of a broad range of base learners which need not to be from the same algorithmic family.
In the first version introduced, the loss function used was the exponential loss which is $L(y,z) = e^{-yz}$ and the target variable took the values $1$ or $-1$.

This particular case yields a similar equation as in \cref{eq:adaBoostIteration}, but where

\begin{equation}\label{eq:sadaBoostExponentialIteration}
\begin{split}
(\gamma_{m}, \theta_{m}) = \underset{\gamma, \theta}{\mathrm{argmin}} \sum_{i=1}^{N} & \exp\big( -y_i (\hat{y}^{m}(x_i) + \gamma h(x_i,\theta) )\big) \\
= \underset{\gamma, \theta}{\mathrm{argmin}} \sum_{i=1}^{N} &
\exp\big( -y_i \hat{y}^{m}(x_i)\big) \exp\big(- \gamma h(x_i,\theta)y_i \big)
\end{split}
\end{equation}


Given that we are only minimizing $\gamma$ and $\theta$, we can group $e^{-y_i \hat{y}^{m}(x_i)}$ into a single value $w_i^{(m)}$ which we will set to the weight of each sample.
This weight strongly depends on the past steps of the algorithm.
The equation now becomes

%We can also take the $\gamma$ factor out of the sum, since it is fixed for all samples.
*
\begin{equation}\label{eq:adaBoostExponentialIteration2}
(\gamma_{m}, \theta_{m}) = \underset{\gamma, \theta}{\mathrm{argmin}} \  \sum_{i=1}^{N} w_i^{(m)} \exp \big(-\gamma h(x_i,\theta)y_i \big)
\end{equation}

We can then minimize for $\theta$ first, independently of the value of $\gamma$.
The series in \cref{eq:adaBoostExponentialIteration2} can be decomposed

\begin{equation}\label{eq:adaBoostThetaDecomposition}
\begin{split}
e^{-\gamma} \sum_{i \mid y_i = h(x_i,\theta)} w_i^{(m)} + e^{\gamma} \sum_{i \mid y_i \neq h(x_i,\theta)} w_i^{(m)} & = \\
( e^{\gamma} - e^{-\gamma}) \sum_{i = 1}^{N} w_i^{(m)} I \big( y_i \neq h(x_i,\theta)  \big) + e^{-\gamma} \sum_{i = 1}^{N}  w_i^{(m)} &
\end{split}
\end{equation}


and then the minimizing solution for $h(\cdot, \theta_{m+1})$ will be the one satisfying

\begin{equation}\label{eq:adaBoostThetaMinimization}
\theta_{m} = \underset{ \theta}{\mathrm{argmin}} \sum_{i=1}^{N} w_i^{(m)} I \big( y_i \neq h(x_i,\theta)  \big)
\end{equation}

Let $u = \sum_{i=1}^{N} w_i^{(m)}$ and $v = \sum_{i=1}^{N} w_i^{(m)} I \big( y_i \neq h(x_i,\theta)  \big) $, which are both constant in $\gamma$.
Consider \cref{eq:adaBoostTrainingError} and note that $\frac{u}{v} = \frac{1}{\overline{err}}$.
If we now solve for $\gamma$ in \cref{eq:adaBoostThetaDecomposition}, we can take

\begin{equation}\label{eq:adaBoostBetaMinimization}
f(\gamma) = ( e^{\gamma} - e^{-\gamma}) u + e^{-\gamma}v
\end{equation}

which has a minimum at
\begin{equation}
\gamma_{m} = \frac{1}{2} \log\big( \frac{1 - \overline{err} }{ \overline{err} } \big)
\end{equation}

As seen from the equation above, the minimizing value for $\gamma$ is directly related to the training error of the algorithm for the \textit{whole} dataset.
This weight will be reflected upon all samples in general and then we would expect this rate to decrease at every iteration.
Taking advantage of this closed form, the value is plugged into the next step of the Ada Boost procedure to update sample weights as

\begin{equation}
w_i^{(m+1)} =  w_i^{(m+1)} e^{\gamma_m(-y_i h_m(x_i))} \\
\end{equation}

In this way, we have that the weights are updated for those samples which have a higher misclassification rate.
This is a relevant aspect of the algorithm.
At each step, more importance is given to misclassified samples over correctly classified ones.

%$-y_i h_m(x_i) = 2I \big( y_i = h_m(x_i)  \big) -1$ which means that $\gamma_m(-y_i h_m(x_i))$


The final form of the model is
\begin{equation}
 \hat{y}(x) = sgn\big( \sum_{m=1}^{M} \gamma_m h_m(x) \big)
\end{equation}
  which outputs the most frequent prediction given by all of the weak learners.
  This is because all the correct predictions will be greater than zero and negative values for the incorrect predictions\footnote{This is when we consider the binary class case where $Y$ can take only $1$ or $-1$ values.}.
%This particular property is what gives rise

At first the choice of the exponential loss function can seem arbitrary, but in the context of statistical learning this measure presents an important property where its minimizing function is the log-odds ratio of the two output classes:
\begin{equation}
f^*(X) = \underset{f}{\mathrm{argmin}} \ \Expect_{Y | f(X)}\big[ \exp(-Yf(X)) \big] = \frac{1}{2}\log\big( \frac{ P(Y=1 \mid X) }{ P(Y=-1 \mid X) } \big)
\end{equation}


The use of the exponential loss function $\exp(-Yf(X))$ is also desirable in this context since significantly more weight is put on misclassifications rather than on correct classifications.
This is because the function is not symmetric in $Yf(X)$ and that having a correct classification will mean a factor of only $e^{-1}$, whilst on the other hand a misclassification will mean a factor of $e$.

A drawback of this loss though, is that it is not robust to outliers or to noisy data.
During run-time weights are constantly shifting towards misclassified samples.
Then if samples are mislabeled, this will make the algorithm repetitively focus on classifying incorrectly the data.


\subsection{ Experimental comparison to Random Forests}\label{subsection:boosting_comparison_forest}

In order to compare this learner to Random Forests, we ran the same setup as in \cref{subsection:random_forests_comparison_trees}
with two experiments each varying a different hyperparameter.
The Boosting models were built to classify on task \cref{target2} and the $ROC AUC$ score was used to evaluate their performance.
The cross validation procedure set 10 folds for each hyperparameter
 configuration and both experiments individually tuned on: a) the maximum tree depth and b) the number of base learners.

For both, we vary the tested hyperparameter's values over a predefined range and then graph the change in score across these values.
As a default configuration, 50 was the number of trees, and 6 were the trees' maximum depth and 0.1 was the fixed learning rate.
Additionally, we used the Gini index as the impurity measure in the splitting criteria of each tree's node and the exponential loss function to grow the next estimator in line with the formulation given in \cref{eq:adaBoostThetaDecomposition}.
% .\footnote{Other loss functions are available but these would not yield the AdaBoost formulation.}

Cross validation procedures ran for 82150.8
 and 6076.3 seconds respectively and they both excluded attributes of the user's home state and their current endemic condition at $T_1$.

The experiment's score outcomes across the different hyperparameter values were graphed in \cref{fig:boosting_validation_curves_depth_trees,fig:boosting_validation_curves_num_trees}.
These compare the cross-validated and training set scores, where, for each hyperparameter value, the $1-ROC AUC$ score is graphed as a function of the hyperparameter's value.
In this way, having a lower score in the graphic means a better model was reached.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1\linewidth]{figures/gradient-boosting/validation_curve_boosting_depth_series_f1}
		\caption{ Validation curve for the tree-depth hyperparameter of the Gradient Boosting learner.
		The mean CV scores for the $1 - ROC AUC$ score is shown for a CV run experiment on \cref{target2}}
		\label{fig:boosting_validation_curves_depth_trees}

	\end{center}
\end{figure}

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1\linewidth]{figures/gradient-boosting/validation_curve_boosting_num_trees_series_f1}
			\caption{ Validation curve for the number of trees hyperparameter of the Gradient Boosting learner. The mean CV scores for the $1 - ROC AUC$ score is shown for a CV run experiment on \cref{target2}}
			\label{fig:boosting_validation_curves_num_trees}
	\end{center}
\end{figure}


Recall how these outcomes compare to \cref{figure:dtree_overfit_problem_2} and \cref{fig:random_forest_validation_curves_num_trees},
in which a tree's training error always improves as a result of an increase in the tree's depth.

Here, we see that the estimator is prone to overfitting when using a high $max depth$ value.
This is because the training deviates faster from the CV error when the depth is higher.
This acceleration is clear from the figure where the CV score is invariant while the training error decreases.
This model differs to the Random Forest classifier in that the trees are not uncorrelated, because the $t$-th tree improves on the error of the model at $t$.
This is an effective difference in the models.

For the second experiment, the overfitting difference in the CV score and the train score is smaller, relative to the CV score.

It is interesting to note that, for both cases, there is a very small gain in the CV error when varying the hyperparameters, as evidenced by the range of the scores.
Yet the decrease in CV score is slightly better for the second experiment.
This is different to the Random Forest experiment, where the scores had a stronger improvement in the cross validated score.
The learner seems to better fit the data than the Random Forest model, since lower error rates are achieved, and it converges faster to a stable CV error where adding more trees or depth doesn't improve on the score.

At their optimal configuration, both setups reached CV score averages of at least 0.865, with a maximum of 0.8659 for the learner with trees of 12 levels.
However, the optimal number of estimators for the CV and the training sets were different, were 105 was the best configuration for the training score and 95 was the best average for the CV set. The relative difference in these best scores was of 1.5\%.
It is not surprising that the relative difference of the best CV and training scores, for the maximum tree depth experiment was much higher, with 11.5\%.

The outcomes here expose the drawbacks of this model which is more prone to overfitting and takes longer to fit when compared to the Random Forest algorithm.
Yet it gives better overall performance on the CV error, when the model is correctly calibrated.



\subsection{Gradient Tree Boosting}

As explained before, the boosting methods build an ensemble model learned from other \textit{weaker} learners where decision trees are the most commonly used base models.
This is the same for \textit{gradient tree boosting}.

This type of boosting algorithm was first introduced by Friedman, J., and the full details of the work can be found in Full details of this model can be found in \cref{friedman-gradientBoosting2001}.

Using the same formulation as before, let $Tr$ be a set of tree models and $K$ the number of trees in $Tr$, then trees will be the parameters for this model and at step $m$ the output will be

\begin{equation}
\hat{y}^{(m)}= \sum_t^m \gamma_t h_t(x) , \ h_t \in Tr \ \forall t \in {0,\ldots,K}
\end{equation}

where $\gamma_t$ indexes the weight for each tree $h_t$.

At each step, a new base learner is added to the model through $\gamma_{m+1}$ and $h_{m+1}(\cdot)$:

\begin{equation}
\hat{y}^{(m+1)} =  \hat{y}^{(m)} + \gamma_{m+1} h_{m+1}(x)
\end{equation}

in such a way that the loss function previously is minimized by the next best base learner:

\begin{equation}\label{eq:boosting_iterative_minimization}
h_{m+1}(\cdot) = \underset{h_{m+1},\gamma_{m+1}}{\mathrm{argmin}}  \sum_{i=1}^{n} L ( y_i, \hat{y_i}^{(m)} - \gamma_{m+1} h_{m+1}(x_i) )
\end{equation}


If we treat the tree's weight $\gamma$ as part of the weak learners hyper parameters, we can absorb note each weak learner as $\theta^{m+1}_{q(x)} \forall m in \{1,2,\ldots,K \}$.
This notation follows the one used in $\cref{section:decision_trees}$

where we represent a generic tree in the form of

\begin{equation}
\theta_{q(x)} = \sum_{j=1}^J \theta_j I(x \in R_j)
\end{equation}


%Therefore the model results in,
%
%\begin{equation}
%y = \sum_k f_t(x) , \ f_t \in Tr \ \forall t \in {0,\ldots,K}
%\end{equation}


with $\theta_j \in \mathbb{R} \ \forall j = 1,\ldots,J$ and $ \cup_{j=1}^J R_j$ a feature space partition.

The function
\begin{equation}
q : X \mapsto \{1,\ldots,J\}
\end{equation}

denotes the mapping from samples to regions in the partition.

In summary, this notation sets
$${\{\theta_j, R_j\}}_{j=1}^J$$
as the weak model's parameters and $J$ as a hyper-parameter.
We know that finding the best partition of feature space is a non-trivial optimization problem since finding subset partitions satisfying a global condition is combinatorially hard.

For the full model, the objective function would account for the relationships among the trees and we would have that

\begin{equation}\label{eq:boosting-objfunction}
Obj(\Theta) = \sum_i^n l(y_i,\hat{y}_i) + \sum_t^K R(\theta^m_{q(x)})
\end{equation}
%\footnote{In the formula \cref{eq:boosting-objfunction} }

At the outer level view of the model,$\Theta$ is a parameter encoding all of the base trees' model information.
For each base tree $\theta^m$ is the parameter associated to it and $R{\cdot}$ is a regulating function on the trees's complexity.

This means that

$$\Theta = \bigcup_{m \in {0,\ldots,K}} \theta^m \cup \theta_0$$

The parameter $\Theta$ is not associated to any tree but reserved to characterize the whole ensemble.

If an optimization routine were to collectively fit all the parameters in $\Theta$ to learn this model, we would have a very computational complex model.
In practice this would result in an prohibitive cost.
Instead, we rely on optimization heuristics which rely on first and second order approximations of the loss function at step $m$ to build on the next tree $m+1$.
Smooth loss functions are then fundamental for this procedure to work such as in the adaboost formulation.
The \textit{boosting} optimization heuristic is explained in detail in \cref{appx:sec:boosting_optimization_heuristic}.


As a concluding remark on boosting algorithms, there are two additional heuristics used to improve their generalization performance.
The arguments in favor of these methods are rather experimental and not much theoretical, although their benefits are intuitive.
The authors in~\cite{hastie-elemstatslearn} and~\cite{bishop-patternRecognition} mention them because of their overall contribution to the generalization error.

The first idea to reduce the overall variance of the algorithm is to subsample the data.
This means that at each iteration, only a bootstrapped sample of the dataset will be selected to build the new weak learner.
Samples from $\mathcal{T}$ which are not part of the boostrapped sample are ignored when optimizing for the new learner at \cref{eq:boosting_iterative_minimization}.

The motivation behind this is the same that as in Random Forest, where reducing the overall of available data to fit the new weak learner will most likely reduce the variance of the method.
In practice, the rate of sampling will be supervised by a tuning parameter in the model.

The other heuristic, which was found to be more experimentally important by~\cite{hastie-elemstatslearn}, is to successively apply a \textit{shrinkage} factor $v \in (0,1)$ to the new model.
At step $m$, instead of letting the overall model be

\begin{equation}
\hat{y_i}^{(m)} = \hat{y_i}^{(m-1)} + \theta_m h_m(x_i)
\end{equation}

 we multiply the shrinkage factor $v$ to the new learners before adding them to the overall model.

In the literature this shrinkage factor is also called the \textit{learning rate} of the algorithm.
Note that $v$ is reducing the movement of the algorithm in the direction of optimization provided by $\theta_t$ and $h_t$.
In practice, this results in longer iterations needed to reach the algorithm's \textit{best} prediction rate.
However, when this factor is combined with sub sampling, experiments have shown improvements in the overall generalization accuracy.


\subsection{Experiments}\label{subsection:boosting_experiments}


In this subsection, we explore on the task described in \cref{target4}, using Gradient Boosting classifiers.
The outcomes are compared with the results outlined in \cref{subsection:random_forests_experiments}.
We set two experiments similar to the ones before, where we run two procedures: one with all available features and another without the top target-correlated ones.
In this setup though, we had a minor difference where, as before, it was characterized by the attributes used by our training dataset.

For the third experiment, we \textbf{only} used the best features extracted from the second Random Forest experiment.
These were extracted from the best-fit learner used from that case.
With this third variant, we expected to see how good of a performance can the gradient boosting classifier achieve, using only the top 30 features, as selected by best-fit random forest learner for the previous experiments.
% Even though
Doing this has a hypothetical trade-off between accuracy and procedure run-time, where we can gain speed in our fitting process by compromising some of the predictive power of the algorithm.

Once again, here we performed a cross validation procedure over the available hyperparameters of this classifier.
For the most part, these factors are the same as in the Random Forests classifier.
Still, this model has some additional parameters, which we outline in \cref{list:boosting_grid_search_params}.
For our CV procedure, we will be searching the best learner over a grid combination of these parameters.

\begin{itemize}

  \item The gradient descent's defined step size  --- [step\_size]
  % \item The number of rounds without loss function improvement required to early stop the fit  --- [early\_stopping]
  \item Minimum threshold for the sum of samples' Hessians at a given node.
  If a node's sum is smaller than this threshold then the fitting will stop  --- [min\_leaf\_weight]
\label{list:boosting_grid_search_params}
\end{itemize}


Using the same setup as before, we recorded the outcome of the cross validation procedures for the three experiments.
The results were tabulated in terms of different scoring metrics, the run-time, the hyperparameters chosen and the top ten best features as selected by the best models.

A summary of the metrics and best hyperparameter values is given in \cref{tab:boosting_big_experiment_results}.

\begin{table}[!htb]
\caption{Table of best results comparing three 5-fold full grid cross validation procedures with a Gradient Boosting Classifier for \cref{target4}.  All relevant scores, best hyperparameter values and run-time are shown for both experiments.}
\label{tab:boosting_big_experiment_results}
\centering
\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill} }  l l l l l }
%{|p{2cm}|p{2cm}|p{1.5cm}|p{1cm}|p{1.5cm}}
\toprule
Result & Experiment 1 & Experiment 2 & Experiment 3 \\
\midrule
Running time (s)        & 1172 &  6293 &  204 \\
CV $Accuracy$ score    & 0.889 &  0.843 &  0.8327 \\
CV $ROC AUC$ score     & 0.884 & 0.880  &  0.881 \\
CV $F1$ score           & 0.264  &  0.259 &  0.294 \\
CV $Logloss$ score     & 0.524 &  0.337 &  0.399 \\
CV $Precision$ score    & 0.629 &  0.601 &  0.607 \\
CV $Recall$ score       & 0.167 &  0.165 &  0.194 \\
balanced        & False & False  &  False \\
% early\_stopping  & 5 &  5 & 5 \\
max\_features (\%) & 80 & 75  &  10 \\
max\_depth     & 15 &  15 &  2 \\
max\_iterations     & 250 &  300 &  120 \\
min\_leaf\_weight  & 2 &  5 &  10 \\
min\_loss  & 10 &  1 &  0.01 \\
step\_size  & 0.01 &  0.1 &  1 \\

\bottomrule
\end{tabular*}
\end{table}

The first notable fact from the results is how fast the last procedure is, compared to the two before.
It trained much faster, with almost 3 minutes to complete the full procedure whilst the others had at minimum 15 minutes to do so.
We see that the information from the top 30 features of the RF models was efficiently used in this boosting experiment.
This run-time improvement then should not be surprising as we have limited the amount of available features to use, thus decreasing the necessary computer resources to compute the fit.
% However, this conclusion might seem

The second interesting fact from the outcome is that the model seems to be more overfit, when compared to the experiments of the random forest.
Specially on the two experiments, where very complex models were chosen from the procedure, with a depth of 15 levels and at least 250 trees for both.
This is not the case on the last model though, where a more simple model resulted with a depth of only two levels and 120 trees grown.
The same applies for the max\_features and min\_leaf\_weight hyperparameters, for which lower complexity values resulted form the last CV procedure.
There is difficulty though in establishing which of them had the most weight in an underweight model.

This situation is not that different for the $F1$ score of the experiments.
We have that the last procedure improved on the first two experiments in more than 10\%, using this score and in an almost 20\% increase in the recall score.
A clear conclusion is that the first two models were overfitting the training set.
Then byusing the top random forest features we improved on the $F1$ performance in a strong way.
Finally, when compared with the experiments using the Random Forest, we see that most of the boosting models performed near to the top $F1$ value of 27\% achieved in the Random Forest experiments.
Still, the top Random Forest $F1$ score was attained in the first experiment with Random Forests, where we included the highly correlated features.
This is then a remarkable performance for boosting learners which were not trained on this attributes and still achieved similar or higher scores.


On the other hand, the third model lacked the same strength for the $ROC AUC$ score.
Here, the first model reached a score of almost 90\%, whereas the third model had a poorer performance of 83\%.
Still, the lowest value for all experiments is more than satisfactory for the prediction task.
With these results, and due to the difference in $ROC$ vs. $F1$ scores, there is no clear indication as to which configuration is better across all experiments.


The model's selection of best features, but only for the experiments.
Note that the last selection will have a very similar configuration to the top Random Forest features, since it selects 10 features of the available 30 used to train the model.
This outcome is presented in \cref{tab:boosting_big_experiment_best_features}, where the top 10 features for each experiment are shown.


\begin{table}[ht]
    \centering
    \caption{A representation of the top features that resulted from the Gradient Boosting procedure on two experiments of  \cref{target4}. }
    \label{tab:boosting_big_experiment_best_features}
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}llll@{}}
            \toprule
            & \multicolumn{3}{l}{Top Features}                                                                   \\ \midrule
            \multicolumn{1}{l|}{\multirow{4}{*}{First Experiment}}  & Call Count Antenna\_0 & Mobility Diameter  & Mobility Diameter Weeknight  \\
            \multicolumn{1}{l|}{}                                   &  Call Count Weeknight Antenna\_0 &  Call Count Antenna\_1  & Call Count Antenna\_2  \\
            \multicolumn{1}{l|}{}                                   &   TimeWeekDay In Month\_08  & Call Count Weeknight Antenna\_2 &  TimeWeekDay Out Month\_09  \\
            \multicolumn{1}{l|}{}                                   &  & TimeWeekDay Out Month\_12  &       \\ \midrule
            \multicolumn{1}{l|}{\multirow{4}{*}{Second Experiment}} & Mobility Diameter  & Mobility Diameter Weeknight   &  Call Count Antenna\_0   \\
            \multicolumn{1}{l|}{}                                   &  Call Count Weeknight Antenna\_0 & Call Count Antenna\_0   & TimeWeekDay In Month\_09   \\
            \multicolumn{1}{l|}{}                                   &  TimeWeekDay Out Month\_09  & TimeWeekDay Out Month\_12   &  TimeWeekDay Out Vul Month\_08 \\
            \multicolumn{1}{l|}{}                                   &   &  TimeWeekDay In Month\_10  &              \\ \midrule
           \multicolumn{1}{l|}{\multirow{4}{*}{Third Experiment}} &  TimeWeekDay In Vul Month\_09 & CallsWeekDay In  Month\_09   &  TimeWeekDay Out Vul Month\_12   \\
            \multicolumn{1}{l|}{}                                   &  Mobility Diameter Weeknight  & Call Count Weeknight Antenna\_0   & CallsWeekDay Out  Month\_09    \\
            \multicolumn{1}{l|}{}                                   &  CallsWeekDay In Vul  Month\_08 &  TimeWeekDay In Vul  Month\_08  &  TimeWeekDay Out  Month\_08  \\
            \multicolumn{1}{l|}{}                                   &   & TimeWeekDay In  Month\_12 &   \\
            \midrule
            \end{tabular}%
    }
\end{table}


The top features list shows us the importance of mobility diameter in both cases.
Both weeknight and whole week user's mobility, is deemed relevant to detect his past endemic condition.
This also confirms what was seen in \cref{tab:random_forest_big_experiment_best_features} where these attributes were, in general, performing.
As expected, in both cases we see that there are a number of features logging vulnerable interactions of users living in non-endemic regions.
The phone usage is also selected as important, both in the number and times of calls.
Unluckily, we can't determine its relationship to the target variable in the sense that it might equally affect in both a positive and negative way.
% Phone behavior might differ along
% Where user phone behavior

These results are in line with what we hypothesized in the construction of the risk maps in \cref{section:riskmaps}.
The direction of the call and the calls' duration or number of occurrences are both important predictors of this model.
Given that this best feature methodology is an heuristic, there doesn't seem to be any clear indication over which type of CDR interaction has better predictive power.
Once again, we see there are more features of earlier month interactions, such as August and September.
At the same time, the only other month appearing is December.
This could be indicative of familiar relations as it is a month which coincides with festivities and family reunions.

The analysis here confirms the relevance of CDR information in giving predictive information of long-term migrations.
Once again, experiments results in satisfactory scores across different metrics and for a highly imbalanced class.
Also, top features are in line with the assumptions on which the risk maps were constructed.
Top features are related to the vulnerability and to the users mobility



\section{Benchmark Classifier: Naive Bayes}\label{section:naive_bayes}

The Naive Bayes model encompasses a group of simple and computationally efficient algorithms which are built with a strong statistical assumption of independence among the features.
Even though this belief is in practice wrong, the model still achieves acceptable classification rates for some problems.
In addition, it does not suffer in problems of high-dimensionality, where $p >> n$ i.e.\ there are more attributes than samples in the data.

It is presented mostly for computational benchmark purposes, where in practice the classification rate achieved by this model serves as a baseline for other, more complex, learners.
Furthermore, the algorithm has linear complexity in the number of features and samples $O(d+n)$, so it can be easily extended to \textit{larger} problem implementations.
Also, its maximum-likelihood estimation of the parameters has a closed form solution which is faster to compute over other iterative methods such as techniques using gradient descent or other similar iterative optimization routines.

Let $x = (x_1,\ldots,x_p)$ be any given data sample and $C_k$ be one of $K$ possible output classes of a classification problem.
We take $p(C_k \mid x)$ to be the class posterior probability of this class given the sample.

In \cref{ch:machineLearning}
we have used
\begin{equation}
\label{eq:posteriorProbabilties}
p(C_k| x) = \frac{P(x|C_k)P(C_k)}{P(x)}
\end{equation}


and argued that if our data is given, then our model can only improve the posterior probability by optimizing $P(x|C_k)P(C_k)$ which is just the joint probability of the sample and the class.

Here we can approximate the posterior as

\begin{equation}
\label{eq:posteriorProbabilityDecomposition1}
P(C_k \mid x) \approx p(C_k) * \prod_{j=1}^{p}  P(x_j \mid \bigcap_{k=j+1}^{p} x_k \cap C_k)
\end{equation}


We now impose a strong independence assumption among features, given the target class, to let the conditional probability factors become the probability of each feature. %This assumption is what gives the model its

This yields a posterior probability which depends only on the prior probability and on the individual likelihood of each feature.

\begin{equation}\label{eq:posteriorProbabilityDecomposition2}
P(C_k \mid x) \approx p(C_k) * \prod_{j=1}^{p}  P(x_j | C_k)
\end{equation}
As we have said before, the parameters of the model can only reweigh the likelihood factors, so if we look to maximize the posterior probability, our final estimate of the posterior will take the following form.

\begin{equation}\label{eq:posteriorProbabilityDecomposition3}
P(C_k \mid x) = \frac{1}{Z} p(C_k) * \prod_{j=1}^{p}  P(x_j | C_k)
\end{equation}

where in the equation $Z = p(x)$ is a scaling factor and is fixed to the dataset.

In practice, the model will stem into different algorithms where each variant will have a different probabilistic assumption on the likelihoods $p(x_j \mid C_k)$ of the model and on the priors $p(C_k)$.
It is common to choose among using a non-parametric density estimations from the data or assuming that the data comes from an exponential family distribution such as a Gaussian, Bernoulli or Multinomial distributions.
Different choices will certainly lead to different cross validation scores among problems.
Altogether, these choices can be treated as part our model's hyperparameters and the best one can be selected with our CV procedure.
%Indeed,

Finally, the output class for a given sample will be given by taking the class $k'$ which maximizes the probability $P(C_k' \mid x)$.

\subsection{Experiments}\label{subsection:naive_bayes_experiments}

Here we present a very brief outline of this algorithm's performance on our general problem.
Without going in to depth on the specifics of the experiments, we ran the classifier in a cross validation procedure against all of the four tasks outlined in \cref{long_term}, and using the same $Accuracy$, $F1$ and $ROC AUC$ classifier scores as in other method's experiment runs.
We took advantage of this method's fast fit to be able to evaluate all tasks on this algorithm.
% The speed small time needed to fit this classifier on a large dataset.
The results were composed into \cref{tab:naive_bays_experiment_results} and are shown to highlight two aspects of the method: its lower performance when compared to other previous methods, and the algorithm's fast run-time.


\begin{table}[!htb]
\caption{Table comparing cross validated results
of best results comparing all 8-fold full grid cross validation procedures with a Naive Bayes classifier using a Multinomial prior probability.
Relevant classification scores and run-time are shown}
\label{tab:naive_bays_experiment_results}
\centering
\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill} }  l l l l l }
%{|p{2cm}|p{2cm}|p{1.5cm}|p{1cm}|p{1.5cm}}
\toprule
Measure & Problem 1 & Problem 2 & Problem 3 & Problem 4  \\
\midrule
Running time (s)        & 125    &  121   &  105     &  85     \\
CV $Accuracy$ score     & 0.842  & 0.649  &  0.658   &  0.852   \\
CV $F1$ score           & 0.753  & 0.313  &  0.457   &  0.626   \\
CV $ROC AUC$ score      & 0.827  & 0.617  &  0.635   &  0.768   \\

\bottomrule
\end{tabular*}
\end{table}
