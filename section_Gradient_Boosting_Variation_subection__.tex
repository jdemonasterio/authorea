\section{Gradient Boosting Variation}

\subection{Boosting Trees    }

$\Theta$

The objective function differences minimization objectives. 

Predictive power: how well the algorithm fits the data used for training and, hopefully, the \textit{true} underlying distribution

+ Regularization, favors parsimonious models. This is because all models approximate natural/processes to some degree, so if a simpler model can be used with the same predictive power, then this model should be used.  

\begin{equation} \label{eq:objFunction}
Obj(\Theta) \ = \ L(\Theta) + R(\Theta)
\\
%\sum_{i=0}^{\infty} a_i x^i
\end{equation}

When building a model of ensemble trees, a higher model will be learning on other \textit{weaker} decision trees. If $Tr$ is a set of tree models and $K$ a tree indexer over $Tr$, then trees are the model's parameters and
\[ y = \sum_k f_k(x) , \ f_k \in Tr \ \forall k \in K \]
 
The objective function would account for these relationships in the trees and thus we would have that
 
\[ Obj(\Theta) = \sum_i^n l(y_i,\hat{y_i}))  +  \sum_k R(f_k) \] 
%    

At a higher level we would have that $\Theta$ is a parameter encoding lower trees' parameter information. If $\theta_k$ is the parameter associated for teach tree $f_k \in Tr$ then $\Theta = {}$ where the parameter $\theta_0$ is not associated to any tree and reserved to characterize the tree ensemble. An optimization routine will have to collectively fit all of the parameters in $\Theta$ to learn this model. This is prohibitively costly in practice, so optimization heuristics are used for the fit. 

\subsubsection{Additive Training}

A first take on this optimization problem goes along the way a greedy optimization. Where one tree is fit at a time and new trees are then succesively added to improve on the previous trees errors. 

Let 

aggregating 
%$Obj
\subection{sklearn}


\subection{Hastie Tibshiranie Friedman}

