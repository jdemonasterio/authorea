\section{Gradient Boosting Variation}

\subection{Boosting Trees    }

$\Theta$

The objective function differences minimization objectives. 

Predictive power: how well the algorithm fits the data used for training and, hopefully, the \textit{true} underlying distribution

+ Regularization, favors parsimonious models. This is because all models approximate natural/processes to some degree, so if a simpler model can be used with the same predictive power, then this model should be used.  

\begin{equation} \label{eq:objFunction}
Obj(\Theta) \ = \ L(\Theta) + R(\Theta)
\\
%\sum_{i=0}^{\infty} a_i x^i
\end{equation}

When building a model of ensemble trees, a higher model will be learning on other \textit{weaker} decision trees. If $Tr$ is a set of tree models, then trees are the model's parameters and
\[ y = \sum_k f_k(x) , \ f_k \in Tr \ \forall k \]
 
The objective function would account for these relationships in the trees and thus we would have that
 
\[ Obj(\Theta) = \sum_i^n l(y_i,\hat{y_i}))  +  \sum_k R(f_k) \] 

%    

Here we would have that \Theta is in fact concentrating each of the tree's parameters information. Thus, to learn this model, an optimization routine would have to collectively fit model parameters per tree and fit for interactions among trees. In practice though, this is prohibitively costly, so other heuristics are chosen to fit. 

\subsubsection{Additive Training}

A first take on this optimization problem goes along the way a greedy optimization. Where one tree is fit at a time and new trees are then succesively added to improve on the previous trees errors. 

Let 

aggregating 
%$Obj
\subection{sklearn}


\subection{Hastie Tibshiranie Friedman}

