Posterior probabilities can be directly obtained from values of the data through 
$$p(C_k \mid x) = \frac{p(x \mid C_k)p(C_k)}{p(x)} $$

Our classifiers will partition input space into decision regions $R_k$ for which a class $C_k$ is uniquely attached to samples of that given region. It makes good sense to try and \textit{minmize} the chances of assigning samples to incorrect regions. For example, take a problem with $K$ classes. Given a sample $x_i$, the probability of correct classification is measured by 

\begin{equation}
\begin{split}
=  & \sum_{k=1}^{K} p(x_i \in R_k, C_k )  \\
=  & \sum_{k=1}^{K} \int_{R_k}p(x_i,C_k) dx \\
=  & \sum_{k=1}^{K} \int_{R_k}p(C_k \mid x_i) p(x_i) dx + 
\end{split}
\end{equation}

Observe that the measure is completely characterized by the posterior probabilities. Even when $p(x)$ might not be known or accurately estimated, the algorithm can only decide to improve the posterior probabilities. This is because the factor $p(x)$ is common to both integrals so it makes sense to maximize the posteriors. The goal of the algorithm is to chose the best possible regions $R_k$ for the problem.


\section{Logistic Regression for Classification}

Classification algorithms assign samples to classes. Let $\{C_1,..,C_k\}$ denote the set of possible target classes. We are  interested in maximizing the probability of belonging to a certain class, given the input data:
$$p(C_k| X) = \frac{p(x|C_k)p(C_k)}{p(x)} $$

In this interpretation, $p(C_k)$ is known as the prior probability of belonging to that certain class and $p(C_k|x)$ as the posterior probability, given the data.

\begin{definition}{Logistic Regression}
	Given a choice of parameter $\theta$, 	
\end{definition}

Logistic regression models the posterior probabilities of the classes with a transformation on a linear function of inputs. It also conditions that the posterior probabilities must sum to one. Here the probabilities describing the possible values for the target variable is modeled using a logistic function.

To satisfy this the log-odss ratios of all the classes with respect to a fixed class must all be linear in the inputs. This means that

$$ log\big( \frac{P(C_i|x)}{P(C_j|x)}\big) = \theta_{i0}  + \theta_i^\intercal x  $$ for $i,j \in \{0,1\}, i\neq j$ \label{logit-logOddss}

With these same indices 

$$ P(C_i|x) = \frac{ exp(\theta_{i0}  + \theta_i^\intercal x)}{1 + exp(\theta_{j0}  + \theta_j^\intercal x)}   $$ 

Then the model is specified in terms of the log-odds for each class and is parameterized by $\theta$.

Logistic regression models the target as a Bernoulli random variable when conditioned on the input variables. Formally,

\begin{equation}
\begin{split}
Y_i \mid X_i \  \sim & \operatorname{Bernoulli}(p_i) \\
\mathbb{E}[Y_i \mid X_i ] = & p_i  
\end{split}
\end{equation}
	

The probability function of the target given the features $\Pr(Y_i=y\mid X_i)$ is formally given by 
$$\Pr(Y_i=y \mid X_i = x_i) = p_i^{y} (1-p_i)^{(1-y)}$$\label{logit-probabilityDensity}
where this depends on the class dependence of $y$.

Here the logit function is used to map log odds into conditional probabilties and the model outputs the predicted probabilities of the target variables belonging to the target class.

In this way, we are specifying a model where the target is a linear function of the inputs, corrected by an error term:
$$Y_i = I(\theta_0 + \theta \cdot X_i + \epsilon) \ \forall i$$. \label{logit-indicatorFunction}

Here $\epsilon$ is the error which is distributed with the standard logistic distribution and parameter $p_i$. 

If we take the conditional probability on \ref{logit-indicatorFunction}, given the features we will have that
$$logit(p_i)= ln(\frac{p_i}{1-p_i}) = logit\big( \Expect[Y_i| X_i] \big) = 	\theta_0 + \theta \cdot X_i$$

From this equation it trivially follows that

$$p_i = \sigma(\theta \cdot X_i) = \frac{exp(\theta \cdot X_i) }{1 + exp(\theta \cdot X_i)}$$

where we have used an abuse of notation to absorb $\theta_0$ into $\theta$. Finally, if use this and plug it into the initial conditional probability of $Y_i$ in \ref{logit-probabilityDensity} we will have

$$  \Pr(Y_i=y \mid X_i = x_i) =  p_i^{y} (1-p_i)^{(1-y)} = \frac{exp(y . \theta \cdot X_i) }{1 + exp(\theta \cdot X_i)}$$


\begin{equation} \label{logit-optimization}
\min_{\theta} \gamma\| \theta\|_{j}  + \sum_i=1^N log(e^{-y_i (x_i \cdot \theta + c )} +1) 
\end{equation}

Note that here there are two specific tuning parameters which must be predefined in the optimization procedure. Notably $\gamma$ which measure the weight on the regularization term and$j$ which determines the type of norm to measure the argument of the minimization.

Maximum likelihood is the most common method used to fit the model, with multinomial distributions modeling the features. 


If we take into account that $P(y=1|x,\theta) = 1 - P(y=0|x,\theta)$ , then the estimation of $\theta$ for $N$ samples gives

\[
l(\theta) = \sum_{i=1}^N \big(y_i log(P(y_i|x_i,\theta)) + (1-y_i)log(1 - P(y_i|x_i,\theta)) \big)
\]


\textit{}
\textit{}
\textit{}
\textit{}

\textit{} 
