\subsection{Classification Scoring Metrics}

Metrics are functions defined as a measure of how strong classifiers are with respect to their predictions. They are based on comparing target predictions $\hat{y}$ versus real target values $y$ for each sample or for a group of them. They have their origins in \textit{Type I \& type II} errors common in statistics but have different meanings in this context. 

There are a number of different variations of classification metrics where different problem settings might require different metrics to be used. The problem will define an objective and this will guide the decision of the measure to chose. 

However, all of the metrics arrive from the \textit{contingency table} or \textit{confusion matrix}.

\subsubsection{Contingency Table}

In a binary classification notation, the models' target outcomes $\hat{y}$ can be put into the positive (\textit{T}) or false (\textit{F}) categories whereas actual data is grouped into the true (\textit{T}) or false (\textit{F}) categories. A contingency table would then count the amount of samples that fall into one of the four groups derived from the comparison between the model's expectation and the observed data. 
