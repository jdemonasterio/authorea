\subsection{Classification Scoring Metrics}

Metrics are functions defined as a measure of how strong classifiers are with respect to their predictions. They are based on comparing target predictions $\hat{y}$ versus real target values $y$ for each sample or for a group of them. They have their origins in \textit{Type I & type II} errors common in statistics but have different meanings. 

There are a number of different variations of classification metrics where different problem settings might require different metrics to be used, depending on the objective at hand. However, all of the metrics arrive from the \textit{contingency table} or \textit{confusion matrix}.

\subsection{Contingency Table}

In a binary classification notation, the models' target outcomes $\hat{y} can be put into the positive (\textit{T}) or false (\textit{F}) categories whereas actual data is grouped into the true (\textit{T}) or false (\textit{F}) categories. A contingency table would then count the amunt of samples that fall into one of the four groups derived from the comparison between the model's expectation and the observed data. 


Some metrics have been 

https://en.wikipedia.org/wiki/Receiver_operating_characteristic

First confusion table (TRUE, FALSE vs Positive Negative) with rates and wiki table.

A common methodology would be to estimate the classifiers' prediction error with a metric performed on the test set whilst the classifier's hyperparameters are fitted by crossvalidating on the 

Brief naming:explanation of common metrics.

F1 and F1_weighted.

Roc and Roc_Auc
