In \ref{figure-biasVariance} Hastie et al. fit a regression model on data to illustrate on the interaction between bias and variance. The model increases complexity when using a greater number of features, more variables need to be fit during the optimization procedure. Thus the model increases in complexity (measured in degrees of freedom). 

At first the bias of the model is high both for the training and testing sets. The interpretation is that the model has high bias. Then the prediction error decreases as complexity increases. This behavior is expected because the model learns to better fit the data. At first, the expected prediction error, estimated by averaging over the test set' prediction error's, also decreases when the model's complexity is increased. However when the model starts to overfit the data, the expected prediction error starts to rise. A common heuristic to select the best model is to stop increasing the model's complexity once the $EPE$ stops decreasing.


prediction 
show different curves of prediction error for different models built and tested on samples of the training and testing set respectively in blue and red colors. As the model increases co

An opposite scneario scenario occurs when

rise Typical combinations
The four possible combinations of high 


\textbf{Overfitting}: 
 




\subsection{Convergence of the }
\textbf{Vapnikâ€“Chervonenkis Dimension}
\cite{vapnik-nature2013}
\cite{cherkassky-learning2007}

Related to estimating the true expected prediction error of the underlying distribution with finite samples.

VC dimension true error rate minimization. 
Shattering

Let $\mathcal {A}= \{A_1,A_{2},\dots \}$ be a set family and $T$  another set, then $\mathcal {A}$ shatters $T$ if for every subset $t \subseteq T$ there exists a set $A' \subseteq \mathcal {A} $ such that $ T \cap A' = t$. The VC dimension of $\mathcal {A}$ is the biggest cardinality of a set shattered by $\mathcal {A}$. Note that by definition this means \textit{any} possible set shattered by $\mathcal {A}$


