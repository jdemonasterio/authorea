\section{Boosting Models}\label{section-boosting}
\subsection{AdaBoost}
\cite{schapire-adaBoost}

Boosting methods are similar to additive methods such as in Random Forests because they combine the predictions of weak lerners to output the combined model's prediction. Th full model is grown sequentially from base estimators such as decision trees, but the difference is that each new iteration tries to reduce the overall bias of the combined estimator. This provides greater predictive power when the base model's accuracy is weak. However, care must be taken to control the increase in variance.

In the AdaBoost variation of model ensembling, each iteration builds a new weak learner which is set to improve on the samples misclassified by the weak learner before, rather than building a new uncorrelated learner. Weights are used to order the importance of samples, where a sample with higher misclassification rate will receive a stronger weight. The name of the algorithm is derived from the term \textit{adaptive boosting}, where sample weights are updated at each iteration.

Tuning parameters in this algorithm are the same that as those in the base learners. However, the number of steps that the algorithm will take is a new hyperparameter. 

The chained construction of weak learners has its implications in computational complexity. Base learners cannot be constructed independently and as such, the parallelization of this algorithm is seldomly possible. At the same time, the sequential optimization of learners improving on the one before marks a \textit{greedy} minimization approach of a general loss funciton.
These properties underline a substantial diference to Random Forests where base learners are built as uncorrelated as possible and where optimization can be performed globally, which allows for a significant parallelization of the algorithm. 


Let 
$$\overline{err} = \frac{1}{N} \sum_{i=1}^{N} I(y_i \neq \hat{y_i})$$ \label{equation-adaBoostTrainingError}
denote the training set's misclassification error. As usual, $N$ is the amount of samples in our dataset, $y$ is our target variable and $\hat{y}$ is our model's target prediction, given the samples. We also take $$\Expect_{X \ Y} [ I(Y \neq \hat{Y}(X)) ]$$ 
 to be the expected error rate of the model on the true,  unkown distribution of the data.

Let $m$ index the iteration number in the AdaBoost algorithm. Set $w^{(m)}_i$ to be sample weights which we will initialize equiprobable at $w^{(0)}_i = \frac{1}{N} \forall i$. Let $h(x,\theta)$  denote our model's weak learner with domain in the input feature variables and in the parameters defining the learner. Naturally these will depend on the problem structure and on the learner. Then AdaBoost's model takes the following form:

\begin{equation} \label{equation-adaBoostModel}
\hat{y}(x) = \sum_{m=1}^{M} \gamma_m h(x,\theta_m)
\end{equation}

where $M$ a model's hyperparameter indicating the amount of weak learners and thus the amount of iterations. Here, each $\theta_m$ will encode the base learner's parameters and $\gamma_m$ will denote the weight of that weak learner in the overall model.
The model's iterations will build $\hat{y}$ starting from $\hat{y_i}^{(0)}= 0 \forall i$ and at each stage we will minimize a function that tries to correct the performance of the previous model. At step $m$ we will search for $(\gamma_{m}, \theta_{m})$ where

\begin{equation} \label{equation-adaBoostIteration}
\begin{split}
(\gamma_{m}, \theta_{m}) = \underset{\gamma, \theta}{\mathrm{argmin}}  \sum_{i=1}^{N} & L\big( y_i,   \hat{y}^{m}(x_i) + \gamma h(x_i,\theta) \big) \\
= \underset{\gamma, \theta}{\mathrm{argmin}} \sum_{i=1}^{N}  & L\big( y_i,    \sum_{j=1}^{m} \gamma_j h(x_i,\theta_j) + \gamma h(x_i,\theta) \big) 
 \end{split}
\end{equation}

The greedy nature of the algorithm becomes explicit in the procedure, where we have fixed all of the previous optimized values for $\gamma$ and $\theta$. 

AdaBoost was first derived in \cite{schapire-adaBoost} and it was introduced with specific  minimized function. The general version here presented allows the use of a broad range of base learners which need not to be from the same algorithmic family. In the first version introduced, the loss function was the exponential loss which is $L(y,z) = e^{-yz}$ for the case where the target variable takes the values $1$ or $-1$.

This will yield a similar equation as in \ref{equation-adaBoostIteration}, but one where

\begin{equation} \label{equation-adaBoostExponentialIteration}
\begin{split}
(\gamma_{m}, \theta_{m}) = \underset{\gamma, \theta}{\mathrm{argmin}}  \sum_{i=1}^{N} & exp\big( -y_i  (\hat{y}^{m}(x_i) + \gamma h(x_i,\theta) )\big) \\
= \underset{\gamma, \theta}{\mathrm{argmin}}  \sum_{i=1}^{N} & exp\big( -y_i  \hat{y}^{m}(x_i)\big) exp\big(- \gamma h(x_i,\theta)y_i \big) 
\end{split}
\end{equation}

Given that we are only minimizing over $\gamma$ and $\theta$, we can group $e^{-y_i  \hat{y}^{m}(x_i)}$ into a single value $w_i^{(m)}$ which we will call the weight of each sample, which depends strongly on past steps of the algorithm. We can also take the $\gamma$ factor out of the sum, since it is fixed for all samples. The equation now becomes

\begin{equation} \label{equation-adaBoostExponentialIteration2}
(\gamma_{m}, \theta_{m}) = \underset{\gamma, \theta}{\mathrm{argmin}}  \sum_{i=1}^{N}  w_i^{(m)} exp \big(- \gamma h(x_i,\theta)y_i \big) 
\end{equation}

We can then minimize for $\theta$ first, independently of the value of $\gamma$. The series in \ref{equation-adaBoostExponentialIteration2} can be decomposed 

\begin{equation} \label{equation-adaBoostThetaDecomposition}
\begin{split}
e^{-\gamma} \sum_{i \mid y_i = h(x_i,\theta)} w_i^{(m)}  + e^{\gamma} \sum_{i \mid y_i \neq h(x_i,\theta)} w_i^{(m)} & = \\
( e^{\gamma} - e^{-\gamma}) \sum_{i = 1}^{N} w_i^{(m)} I \big( y_i \neq h(x_i,\theta)   \big)  + e^{-\gamma} \sum_{i = 1}^{N}   w_i^{(m)} &
\end{split}
\end{equation}


and then the minimizing solution $h(\cdot, \theta_{m+1})$ will be the one satisfying

 \begin{equation} \label{equation-adaBoostThetaMinimization}
  \theta_{m} = \underset{ \theta}{\mathrm{argmin}}  \sum_{i=1}^{N}  w_i^{(m)} I \big( y_i \neq h(x_i,\theta)   \big) 
 \end{equation}

Let $u = \sum_{i=1}^{N}  w_i^{(m)}$ and $v = \sum_{i=1}^{N}  w_i^{(m)} I \big( y_i \neq h(x_i,\theta)   \big) $, which are both constant in $\beta$. Consider \ref{equation-adaBoostTrainingError} and note that $\frac{u}{v} = \frac{1}{\overline{err}}$. If we now solve for $\beta$ in \ref{equation-adaBoostThetaDecomposition}, we can take

 \begin{equation} \label{equation-adaBoostBetaMinimization}
f(\beta) = ( e^{\gamma} - e^{-\gamma}) u +  e^{-\gamma}v
\end{equation}

which has a minimum at 
\begin{equation}
\beta_{m} = \frac{1}{2} log\big( \frac{1 - \overline{err} }{ \overline{err} }  \big)
\end{equation}

As seen, the minmizing value for $\beta$ is directly related to the training error of the algorithm in the whole dataset. This weight will be reflected upon all samples in general and then we would expect this rate to decrease in every iteration.  Taking advantage of this closed form, the value is plugged into the next step of the AdaBoost procedure to update sample weights as

\begin{equation}
 w_i^{(m+1)} =   w_i^{(m+1)} e^{\beta_m} e^{(-y_i h_m(x_i))} \\
 \end{equation} 
 
Note that $-y_i h_m(x_i) = 2I \big( y_i = h_m(x_i)   \big) -1$ which is a relevant aspect of the algorithm, where at each step, more importance is given to misclassified samples over correctly classified ones. 

The final form of the model is
$$  \hat{y}(x) = sgn\big(  \sum_{m=1}^{M} \gamma_m h_m(x) \big)$$ which takes on the most frequent output target given by all of the learners. This particular property is what gives rise

At first the choice of the exponential loss function can seem arbitrary, but for the context of statistical learning this measure presents an importan property where its minimizing function is the log-odds ratio of the two output classes:
$$f^*(x) = \underset{f}{\mathrm{argmin}} \  \Expect_{Y | f(x)}\big[ exp(-Yf(x))  \big] = \frac{1}{2}
log\big( \frac{ P(Y=1 \mid x) }{ P(Y=-1 \mid x) }  \big) $$



 A drawback of this function though, is that it is not robus to outliers or to noisy data. During runtime weights are constantly shifting towards misclassified samples and if samples are mislabeled, this will make the algorithm consantly focus on classifying incorrectly the data. 









\subsection{Gradient Tree Boosting}

As explained before, the boosting method builds a high model learnt from other \textit{weaker} learners. For the case of \textit{gradient tree bosting}, decision trees are used as base learners. If $Tr$ is a set of tree models and $K$ the number of trees in $Tr$, then trees wil be the parameters for this model and at step $m$ the output will be

\[
 \hat{y}^{((m)}=  \sum_t^m \gamma_t h_t(x) , \  h_t \in Tr \ \forall t \in {0...K}
\]

where $\gamma_t$ indexes the weight for each tree $h_t$ and $K$ is a hyper-parameter that represent the number of trees. Each new base learner is added to the model 


\[
 \hat{y}^{(m+1)} =   \hat{y}^{(m)}  + \gamma_m h_m(x) 
\]

and the new base model is selected upon minimizing the misclassification rate of the full model at the previous step $m$, where a loss function previously selected is minimized to select the next best base learner:

\[
h_m(\cdot) = \underset{h,\gamma}{\mathrm{argmin}}   \sum_{i=1}^{n} L ( y_i,  \hat{y_i}^{(m-1)} -  \gamma h(x_i)  ) 
\]






 
For the moment  we  include the tree's weight $\gamma$ as part of the weak learners in a single function $f_t(\cdot)$. Therefore the model results in,

\[
y =  \sum_k f_t(x) ,  \ f_t \in Tr  \ \forall t \in {0...K}
\]

We can represent a single tree with the form 

\[
f(x) = \theta_{q(x)} = \sum_{j=1}^J \theta_j I(x \in  R_j)
\]

with $\theta_j \in \mathbb{R} \ \forall j = 1,...,J$ and $ \cup_{j=1}^J R_j$ a partition of feature space. The function $q : X \mapsto \{1,...,J\}$ denotes the mapping from samples to regions. In summary, $\{\theta_j, R_j\}_{j=1}^J$ are the weak model's parameters and $J$ is a hyper-parameter. Note that finding the best partition of feature space is a non-trivial optimization problem since finding subset partitions satisfying a global condition is a combinatorial feat.

For the high model, the objective function would account for the relationships among the trees and we would have that

\[ Obj(\Theta) = \sum_i^n l(y_i,\hat{y}_i))  +  \sum_t R(f_t) \] \label{eq:boositing-objfunction} \footnote{In the formula \ref{eq:boositing-objfunction} }
%    

At this level we would have that $\Theta$ is a parameter encoding all of the base trees' model information. Where $\theta_t$ is the parameter associated toeach tree model $f_t$ and $\Theta =  \bigcup_{t \in {0...K}} \theta_t  \cup \theta_0$.  Here the parameter $\Theta_0$ is not associated to any tree but reserved to characterize the tree assembly. If an optimization routine were to collectively fit all of the parameters in $\Theta$ to learn this model, we would have an very computational complex model. 
In practice this would result in an prohibitive cost so we rely on optimization heuristics  instead. 

\subsubsection{Additive Training}

As usual, the first take on this optimization problem goes using a greedy optimization routine. One tree is fit at a time and new trees are then successively added in later steps to improve on previous trees' errors.

Let $t$ be the step indexer of the algorithm, where $t \in {0,..,K}$, $Obj_t(\Theta)$ be the objective function and $\hat{y}^t$ be the target variable respectively. Then the $i$-eth target's value at each step would iterate in the following way:

\begin{equation} \label{eq:gb-targetSteps}
\begin{split}
\hat{y}_i^0 = & 0 \\
 ... \\ 
\hat{y}_i^t = &\sum_{k=1}^{t} f_k(x_i) = \hat{y}^{t-1}_i +  f_t(x_i)
\end{split}
\end{equation}
%\sum_{i=0}^{\infty} a_i x_i
where each tree is added in such a way that we are minimizing

\begin{equation}
Obj^t(\Theta) =  \sum_i^n L(y_i, \hat{y}^{t-1}_i +  f_t(x_i) ) + c(t) + R(f_t) 
\end{equation}


Note that we have included here a regularization term (see section \ref{subsection-hyperParametersRegularization}) $R$ which most commonly takes the form of a  Tikhonov regularization. On the other hand the term is another complexity tuning parameter controling the length of the overall procedure $c(t)$ which is variable only in $t$. 

If we assume we have enough conditions to approximate the objective function with  second order Taylor approximation around $f_t(x_i)$,we would have

\begin{equation}\label{equation-gradientBoostingTaylor}
Obj^t(\Theta) \approx \sum_i^n {L(y_i, \hat{y}^{t-1}_i) + g_i f_t(x_i) + \frac{1}{2} h_i f_t(x_i)^2 } +  R(f_t) +  c(t)
\end{equation}

Here $g_i$ and $h_i$ are first and second order approximations of the loss function with,

\[
g_i =  \frac{\partial L(y_i, \hat{y}^{t-1}_i)}{\partial \hat{y}^{t-1}_i} \\
h_i =  \frac{\partial^2 L(y_i, \hat{y}^{t-1}_i)}{\partial (\hat{y}^{t-1}_i)^2 }
\]

Still, the equation \ref{equation-gradientBoostingTaylor} can be simplified by taking only the terms that are dependent on $\theta$ and by replacing the output targets of each sample for that tree evaluation.

\begin{equation} \label{eq:gb-objSteps1}
Obj^t(\Theta) \approx  \sum_i^n {g_i \theta_{q(x_i)} + \frac{1}{2} h_i \theta_{q(x_i)}^2 } + \gamma ({t-1}) + \frac{1}{2}\lambda \sum_{j=1}^{t-1} \theta_j^2 \\
\end{equation}

As an example, we have already replaced the regularization terms $c(t)$ and $R(f_t)$ with an $l$2 penalty on the parameter size and on the iteration number. 

\begin{equation} \label{eq:gb-objSteps1}
Obj^t(\Theta) \approx   \sum_{j=1}^{t-1} \left(  \sum_{i \in ({t-1})_j} (g_i )\theta_{j} + \frac{1}{2} \sum_{i \in ({t-1})_j} (h_i + \lambda ) \theta_{j}^2  \right) + \gamma ({t-1})
\end{equation}

Then

\[
Obj^t(\Theta) \approx \sum_i^n {  g_i f_t(x_i) + \frac{1}{2} h_i f_t(x_i)^2 } +  Obj_{t-1}(\Theta) + R(f_t) - R(f_{t-1})
\]

This equation form results in a direct method for a greedy optimization approach.  We will have to search for the tree $f_t$ that minimizes \\
$\sum_i^n {  g_i f_t(x_i) + \frac{1}{2} h_i f_t(x_i)^2 } + R(f_t)$ at the $t$-th step. 

As we have stated before the approach assumes that that $ \forall i \in {1...n}, \forall t \in {1..K}, \exists g_i(\hat{y}^{t}_i), h_i(\hat{y}^{t}_i) $ and that these values are effectively computable. Smooth loss functions play an important role here in providing a feasible method. 

As a last remark on boosting algorithms, there are two additional heuristics used to improve the generalization performance of boosting algorithms. The arguments in favor of these methods are rather experimental and not much theoretical, although their benefits are intuitive . The authors in \cite{hastie-elemstatslearn} and \cite{bishop - patternRecognition} mention them because of their overall contribution to prediction improvement.

The first idea to reduce the overall variance of the algorithm is to subsample the data. This means that at each iteration, only a bootstrapped sample of the dataset will be selected to build the new weak learner. The motivation behind this is the same that as in Random Forest, where reducing the overall of available data to fit the new weak learner will most likely reduce the variance of the method. In practice, the rate of sampling will be controlled by a tuning parameter in the model.

The other heuristic is considerd to be, at least experimentally, more important. This involves successively applying a \textit{shrinkage} factor $v \in  (0,1)$ to the new model.  At step $m$, instead of letting the overall model become $ \hat{y_i}^{(m)} = \hat{y_i}^{(m-1)} +  \gamma_m h_m(x_i) $, we multiply the shrinkage factor $v$ to these values before adding them to the overall model at step $m-1$. This shrinkage factor is also known in the literature as the \textit{learning rate} of the algorithm.  Note that $v$  is reducing the movement of the algorithm in the direction of optimization provided by $\gamma_m$ and $h_m$. In practice this results in longer iterations needed to reach the algorithm's \textit{best} prediction rate. However, and specially combined with subsampling, the use of this factor has been empirically shown overall improvements in generalization accuracy.




\textit{}

\textit{}

\textit{}