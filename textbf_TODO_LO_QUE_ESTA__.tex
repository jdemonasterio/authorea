\textbf{TODO LO QUE ESTA EN ITALICA ES TIPO HIGHLIGHTER}

\section{Bias, Variance, Generalization and Model Complexity}

\subsection{Hastie Tibshiranie Friedman}

\textbf{Gral}
\textit{The generalization performance of a learning method relates to its prediction capability on independent test data. Assessment of this performance is extremely important in practice, since it guides the choice of learning method or model, and gives us a measure of the quality of the ultimately chosen model.}

\textit{Let $k : {1,..,N} &\mapsto {1, .., K}$ be an indexing function are more details be an indexing
function that indicates the partition to which observation i is allocated by the randomization. Denote by}

\textit{}

\textit{}

\textbf{Cross Validation}:  
\textit{simplest and most widely used method... This method directly estimates the expected extra-sample error
$Error = E[L(Y,\hat{f}(X))] $ i.e. the average the average generalization error when the method $\hat{f(X)}$ is applied to an independent test sample from the joint distribution of $X$ and $Y$ . As mentioned earlier, we might hope that cross-validation estimates the conditional error, with the training set. $\mathrm{T}$ held fixed. But cross-validation typically estimates well only the expected prediction error.}

\textit{Ideally, if we had enough data, we would set aside a validation set and use it to assess the performance of our prediction model. Since data are often scarce, this is usually not possible. To finesse the problem, K-fold cross-validation uses part of the available data to fit the model, and a different part to test it. We split the data into K roughly equal-sized parts; }



\textbf{The likelihood principle}: given a generative model for data $d$, given parameters $\theta$, the likelihood is defined as $P (d | \theta)$, and having observed a particular outcome $d_1$ , all inferences and predictions should depend only on the function $P(d_1 | \theta)$ i.e. they depend only on the data at hand, on what actually happened. 

\textbf{Shannon information content of an outcome}: let x be an event/outcome then $h(x)$ is defined to be 
$= log _2(\frac{1}{ P(x)})$
Note that it is measured in bits and that less probable events carry more “information”.
This number is a measure of the information content of a bit. 

\textbf{Entropy}: defined as $H(X) =  E[log _2(\frac{1}{ P(x)})]$  where $X$ is a random variable and by convention $0*log(\frac{1}{0}) = 0$. 
It is clear from the definition that $H(x)\geq 0$ and is equal to 0 only if x is s.t. $P(x)=1$. 

Entropy is maximized when $P(x)~Uniform$ and as such for any given $X$ it goes that $H(X) \leq log(|A(x)|)$

Entropy is additive for two independent random variables i.e. $H(X,Y) = H(X) + H(Y)$

\textbf{Decomposability of entropy}:  if ${p1,p2,..,pn}$


