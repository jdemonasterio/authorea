\textbf{TODO LO QUE ESTA EN ITALICA ES TIPO HIGHLIGHTER}

\section{Supervised vs. Unsupervised and then Classification vs. Regression}

\subsection{}


\section{Bias, Variance, Generalization and Model Complexity}

\subsection{Hastie Tibshiranie Friedman}

\textbf{Gral}
\textit{The generalization performance of a learning method relates to its prediction capability on independent test data. Assessment of this performance is extremely important in practice, since it guides the choice of learning method or model, and gives us a measure of the quality of the ultimately chosen model.}

\textbf{Cross Validation}:  
\textit{simplest and most widely used method... This method directly estimates the expected extra-sample error
$Error = E[L(Y,\hat{f}(X))] $ i.e. the average the average generalization error when the method $\hat{f(X)}$ is applied to an independent test sample from the joint distribution of $X$ and $Y$ . As mentioned earlier, we might hope that cross-validation estimates the conditional error, with the training set. $\mathrm{T}$ held fixed. But cross-validation typically estimates well only the expected prediction error.}

\textit{Ideally, if we had enough data, we would set aside a validation set and use it to assess the performance of our prediction model. Since data are often scarce, this is usually not possible. To finesse the problem, K-fold cross-validation uses part of the available data to fit the model, and a different part to test it. We split the data into K roughly equal-sized parts; }

\textit{Let $k : \{1,..,N\} \mapsto \{1, .., K\}$ be an indexing function are more details be an indexing
function that indicates the partition to which observation i is allocated by the randomization. Denote by}


\subsection{Bengio, GrandValet - No Unbiased Estimator of the Variance of K-Fold Cross-Validation}
\textbf{Cross Validation}: 
\textit{The standard measure of accuracy for trained models is the prediction error (PE), i.e. the expected loss on future examples. Learning algorithms themselves are often compared on their average performance, which estimates expected value of prediction error (EPE) over training sets.
The hold-out technique does not account for the variance with respect to the training set, and may thus be considered inappropriate for the purpose of algorithm comparison [4]. Moreover, it makes an inefficient use of data which forbids its application to small sample sizes. In this situation, one resorts to computer intensive resampling methods such as cross-validation or bootstrap to estimate PE or EPE. We focus here on K-fold cross-validation. While it is known that cross-validation provides an unbiased estimate of EPE, it is also known that its variance may be very large.
Some distribution-free bounds on the deviations of cross-validation are available, but they are specific to locally defined classifiers, such as nearest neighbors.
We focus on the standard K-fold cross-validation procedure, with no overlap between test sets: each example is used once and only once as a test example.
}




\textbf{HyperParameters}:
Como van apareciendo en algunos algoritmos y are different from the "parameters" or coefficients of the learners. They appear as a consequence of numerical, computational and sometimes statistical fine-tuning of algorithms (give an example?). 
La relacion entre cross-validation y la busqueda de hiper parametros. 

\textit{}

\textit{}


\textbf{The likelihood principle}: given a generative model for data $d$, given parameters $\theta$, the likelihood is defined as $P (d | \theta)$, and having observed a particular outcome $d_1$ , all inferences and predictions should depend only on the function $P(d_1 | \theta)$ i.e. they depend only on the data at hand, on what actually happened. 

\textbf{Shannon information content of an outcome}: let x be an event/outcome then $h(x)$ is defined to be 
$= log _2(\frac{1}{ P(x)})$
Note that it is measured in bits and that less probable events carry more “information”.
This number is a measure of the information content of a bit. 

\textbf{Entropy}: defined as $H(X) =  E[log _2(\frac{1}{ P(x)})]$  where $X$ is a random variable and by convention $0*log(\frac{1}{0}) = 0$. 
It is clear from the definition that $H(x)\geq 0$ and is equal to 0 only if x is s.t. $P(x)=1$. 

Entropy is maximized when $P(x)~Uniform$ and as such for any given $X$ it goes that $H(X) \leq log(|A(x)|)$

Entropy is additive for two independent random variables i.e. $H(X,Y) = H(X) + H(Y)$

\textbf{Decomposability of entropy}:  if ${p1,p2,..,pn}$


