# TODO PLAN

## Recorrer predictions pero con SVD para reducir covarianza de variables.

## Machine Learning

* notacion consistente

* terminar logit regression mas explicado


* Hyper-Parameters for Decision Trees (tree depth, node split size, gini and entropy criteiron, etc)

* gradient boosting: agregar un poco mas de como el gain es un scoring. Hastie - (Section 10.13).

	mas data de esta desigualdad: http://authors.library.caltech.edu/11996/1/ABUnc89.pdf
	En tibshiranie, hastie tambien hay


* terminar random forest (variable importance)
	* tuning parameters for forests (num estimators, bagging, num features in each split, etc. )

 
* section Technical Observations (python, jupyter, sklearn, pandas, 
									sframes, graphlab, spark mllib, 
									theano, tensor flow, xgboost)

* terminar Results

* ampliar data description and visualizations

* ampliar introduction