# TODO PLAN

## Recorrer predictions pero con SVD para reducir covarianza de variables.

## Machine Learning

* notacion consistente

* terminar logit regression mas explicado

* gradient boosting: agregar un poco mas de como el gain es un scoring.

	
	mas data de esta desigualdad: http://authors.library.caltech.edu/11996/1/ABUnc89.pdf
	En tibshiranie, hastie tambien hay


* terminar random forest (variable importance)
	* tuning parameters (tree depth, node split size, gini and entropy criteiron, etc)

 
*  terminar Results

* ampliar data description and visualizations

* ampliar introduction