Notar que todo lo que esta en italica es tipo highlighter de textos de otros autores :)
%NOTAR QUE TO LO QUE ESTA EN ITALICA ES TIPO HIGHLIGHTER DE OTROS TEXTOS DE OTROS AUTORES}

\section{Classifier : Decision Tree Learners}

Decision Trees will not be described in detail in this work. Here we present only but a brief overview of this supervised learner used in regression and classification problems.

The models builds a tree in the graph-theory sense where each node has a rule based on set belonging. Each rule defines a linear (or similar) partition of input space and the rules that determine the partitions are built from the sample's features. At each node we would have a decision to include or exclude a sample $s$ from that partition by checking if $X_i(s) \in U$ where $X_i$ might be any given feature of the data and $U$ is a subset of said feature's space.

For numerical features, $U$ will be of the form $(-\infty,c]$ where $c \in \mathbb{R}$ is any given number predefined by each rule. For categorical features, $U$ will be a subset of possible the values of that feature. A tree defines a partition of feature space in disjoint regions $A_1,...,A_K$ such that the sample's predicted output $\hat{y}$ is $c_k$ if the sample belongs to $A_k$. Here $c_k$ is one of the possible values taken by the target variable $y$ in the training set. And by the way trees were built, each $A_k$ is a hyper-rectangles in feature space.

The learner can be characterized by 
\[
h(X) = \sum_{j=1}^J c_k I(X \in A_k)
\]\label{equation-decisionTreeModel}

where $c_k$ is the value that our model estimates forfor samples in the $A_k$ region. Both of these will have to be estimated by the model by minimizing its loss funciton.

The algorithm then needs to determine for the group of input samples that flows into a decision node, what is the best way to split them according, in order to optimize a loss metric. In this context, the criteria used to decide on node splits are called \textit{node impurity measures}. Most variations for this machine learning model build rules at each nodes in a greedy fashion, where node impurity measures are locally optimized at each node to decide what is the best splitting decision. This is done so because not doing so will result in an algorithm whose computational complexity is infeasible since the construction of optimal binary decision trees is NP-Complete \cite{decisionTreesNP}. The parameters for the tree will then be fit sequentially.

At any splitting node, we have to find the \textit{best} feature $X^p$ and split $t$ threshold for which to partition the data in
 $A_{left} = \{x \in \mathcal{T} \  / \ x^p \leq t \} $ and $A_{right} = \{x \in \mathcal{T}\  / \ x^p> t \} $.  To do so we optimize our misclassification loss at the split

\[
min_{p,t} \big[ min_{c_{left} }  \sum_{x \in A_{left}(p,t) } L(y,c_{left})          +   min_{c_{right}}  \sum_{x \in A_{right}(p,t) }  L(y,c_{right}) \big] 
\]\label{equation-decisionTreeGreedyOptimization}

where $y$ is the target associated to our sample $x$. Note that this can be done efficiently for a wide range of loss functions since the minimization can be done for each feature independently. 

A tree is then grown in an iterative way from the top down \footnote{In this context the \textit{top} of a tree refers to the root of the tree.}, estimating the appropriate parameters at each split. All of the training set's samples would start at the top (the root node) and then travel down through the trees branches, in accordance to their fulfillment or not of each node's split rule. The tree's leafs are the subsets that comprise the partition and for new samples, the prediction of their target class will be the output of traveling the sample down to the leaf node. 

To illustrate this method, an instance is show in figure \ref{rf-treeFigure}. This classification tree example is built for the two class problem of gender prediction using data from CDRs:
%[.{\textit{Woman}}]
\smallskip
\begin{figure}[h]\label{rf-treeFigure}
	\Tree[.{ $Calling\_Volume \leq 23$ } [.{$Province \in \{ San Luis, Chubut \} $} [.{$Time\_Weekend \geq 16$} [.{\textit{M}} ] [.{\textit{F}} ]  ]
	[.{$Calls\_Weekdays \leq 48$} 
	[.{ $Time\_Weekday \geq 17$} [.{\textit{M}} ] [.{\textit{F}} ]] [.{\textit{F}} ] ]  ]
	[.{$Calls\_Mondays \geq 2$} [.{$Province \in \{ Chubut, Cordoba \} $}  [.{\textit{M}} ] [.{\textit{F}} ] ]
	[.{\textit{M}}  ]]]
	
\end{figure}

\smallskip

%[.{\textit{M}} ] [.{\textit{F}} ]



The latter optimizes for information entropy, which is analogous to minimizing \textit{Kullback-Liebler divergence} of the resulting sets with respect to the original set previous to the split. This partitioning of the samples will continue iteratively until a predefined optimization or iteration threshold is met. And these thresholds will be given as \textit{tuning parameters} of the model.


In this method hyper-parameters of the model include the length of the tree and the metric used to decide on the \textit{best} split. Parameters are limited to the features selected at each node along with the splitting rule threshold. However, once a learner is fit, predicting targets for new samples is straightforward. 

Hyper-parameters of tree models, also known as tuning parameters, are those parameters need to be set before running the algorithm. From the previous descriptions we can list those directly:

\begin{itemize}
	\item Max depth of the tree, or the number of allowed level of splits.
	\item The criteria or measure used to select the best split feature at each node.
	\item The leaf size or the total number of minimum samples allowed per leaf. Note that this is a limit imposed on  branch depth. 
	\item Number of features selected to decide on the best split feature at each node.
\end{itemize}



For a more complete explanation of a decision tree for classification or regression problems, please refer to \cite{breiman-cart84}.


\textit{}

\textit{}

\textit{}

\textit{}