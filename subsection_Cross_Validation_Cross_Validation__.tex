\subsection{Cross Validation}:  
 
Cross Validation (CV) is one of the most widespread techniques to evaluate the generalization performance of a set of learners. Given a number of possibilities for the tuning (hyper) parameters for a specific model, we would like to decide which selection of these fit the best estimator, when measured by the generalization error. In supervised learning settings, data is generally scarce while prediction error estimates are based on asymptotic or analytical results which are not computable. CV intends to prevent over-fitting by iteratively holding out a random part of the dataset and by measuring the predictive accuracy of the learner fit on data \textit{in-sample} against new values from the \textit{hold-out} part of the dataset. The accuracy measure here is weighted by the loss function, which must be selected beforehand. 
 
 A partition of the samples in a Cross Validation procedure is called a \textit{fold}. CV then partitions data into $K$ random folds, where the number $K$ has to be previously decided.\footnote{ We assume here that a random sample of the initial dataset was already left out in order to test the model's accuracy at the end. We refer to this set as the \textit{testing set}.} Let $\gamma : \{1,..,N\} \mapsto \{1, .., K\}$ be a function mapping samples to folds. Without loss of generality,  let $\alpha$ be an index of the model's hyper-parameters, where each distinct combination of the tuning parameters is identified by this index. Note that the domain of $\alpha$ will vary with the type of approximating function used.
 
 The CV algorithm now iteratively runs over all of the folds, and takes one fold $\gamma^{-1}(\{k\})$ to be the validation set, where $k \in [1,...,K]$ is the indexer of the iteration. $\hat{f}^{-k}$ will denote the fitted estimator on the training set with the $k$-fold hold out and it will be tested against the \textit{out of sample} estimates. This means that for every sample in this $k$-fold, we measure the loss $L(y_i, \hat{f}^{-\gamma(i)}(x_i))$ of the model's prediction against the true target value.
 
 Cross validation intends to estimate the expected \textit{out-of-sample} error $\Expect \left[  L(Y, \hat{f}(X)) \right]$, when the model is tested against \textbf{independent} samples from the true distribution. For this reason, it is fundamental to ensure independence of the training set and the test set. Any transformations that must be done on the input data that jointly uses the input and output samples in the process must be done and \textit{learned} only on the training set. It is very important that we don't introduce information from our test set in the estimator. The model should never \textit{see} the test data until we use it to evaluate our learner. 

\textbf{HyperParameters}:
 Como van apareciendo en algunos algoritmos y are different from the "parameters" or coefficients of the learners. They appear as a consequence of numerical, computational and sometimes statistical fine-tuning of algorithms (give an example?). 
 La relacion entre cross-validation y la busqueda de hiper parametros. 
 

 Let $\mathcal{A} = [\alpha_0, \alpha_1,..., \alpha_l   ]$ be a list of hyperparameter settings and  $\mathcal{K} =[1,..,K]$ a list of folds.  A full K-Fold Cross Validation procedure takes the following form.
 
 \begin{algorithm}%[h]
 	\SetAlgoLined
 	\KwResult{Write here the result }
 	Initialize $\mathcal{A}$ and $\gamma(\cdot)$\;
 	\For{ $\alpha \in  \mathcal{A}$}{
 		\eIf{data transformation}{
 			Perform data transformation on the whole training set \;
 		}{
 		continue\;
 	}
 	\For{ $k \in  \mathcal{K}$}{
 		fit $\hat{f}^{-k}(\cdot, \alpha)$\;
 	}
 	
 	compute $CV(\alpha) = \frac{1}{N} \sum^n_{i=1} L\left( y_i, \hat{f}^{-\gamma(i)}(x_i, \alpha) \right)$\;
 }
 \caption{K-Fold Cross Validation Estimation Procedure}
\end{algorithm}

Note that during the algorithm, each sample's prediction was tested on the model which was fitted without using that sample. 

From the \textbf{CV} procedure it makes sense to choose a final  model $\hat{f}_\alpha$ with the lowest $CV(\alpha)$ value among all of possible hyperparameters. However, following ideas detailed in \textbf{VAPNIK DIMENSION SECTION}, importance is also given to the \textit{complexity} of the approximating function. A common rule of thumb is to favor models with a lower number of hyper parameters or number of features. These will probably estimate the prediction error more accurately. In practice though, a class of approximating functions might have a complexity which is not analytically computable. Thus in some cases, crude heuristic estimates or common sense are used to estimate model complexity, without making use of theoretical arguments.
 
\subsubsection{Choice of $K$ parameter} 

Experimental results show the differences among CV routines used with varying $K$ values. In synthetic and actual dataset , \cite{hastie-elemstatslearn} P. 243, have found that using a \textit{higher} value for $K$, relative to the size of the dataset, means having a bigger training fold since each left-out partition is only $\frac{N}{K}$ in size. The results show models with good bias but high variance which is in accordance with the small size of the validation set. Note that having a higher value for $K$ will effect to a higher computational burden since $K$ estimators need to be fitted.

Having a lower $K$ value means using a smaller training set. Then it is common to find models with lower variance and higher bias. Empirical results also tend to show that the $EPE$ is overestimated. This is because having less available data to fit the model, implies having worse estimates from asymptotic results.
 
One last common choice for $K$ is $N-1$. This scenario is known as \textit{leave-one-out CV} and is a very used format in problems where data arrives sequentially over time. Here, samples of the training set $t_i = (\textbf{x_i},\textbf{y_i})$ are accessible only in an order fashion. For these cases model evaluations are made against the new sample and the training set is updated with every arrival..\footnote{Note that here we must assume the \textit{exchangeability} of samples. This means that the distribution of the training set is not altered by a permutation of samples $F(x_1,...,x_n ) = F(x_\sigma{1},...,x_\sigma{n})$ for any random permutation $\sigma$ of the samples  }
 

A good survey and explanation of the drawbacks of the $K$-Fold CV estimator can be found in \cite{bengio-unbiasedCvEstimator}. The authors prove that there is no distribution-free estimator unbiased estimator of the $K$-Fold cross validation estimator's variance. The theoretical arguments focus on the idea that error correlations between the training and validation sets are not taken into account by the CV procedure. These correlations are known and understood for the general case, but their estimation is not possible. As a consequence of this, comparison between different possible models is is hindered. 

Empirical examples are built from synthetic datasets to show the shortcomings of the cross validation's algorithm which might show high deviations from its central value. Some exceptions appear though, specifically where distribution-free bounds can be found for a class of approximating functions. But these cases are specific only for this class. In general these bounds are not known, so using an unknown deviation of the CV estimator will affect the evaluation of different models.

Consensus \footnote{\cite{hastie-elemstatslearn} P. 260} is that in general CV is a good procedure to estimate the expected prediction error with the training set fixed, but not good for the prediction error, conditional on the training set $\mathcal{T}$ fixed.

